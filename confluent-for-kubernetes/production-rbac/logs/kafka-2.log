+ /mnt/config/kafka/bin/run
===> User: uid=1001 gid=0(root) groups=0(root),1001
===> Load kafka operator scripts from path /mnt/config/kafka/bin
===> Configure log4j
===> Configure disk-usage agent
===> Configure jolokia agent
===> Configure jmx prometheus agent
===> Configure JVM config
===> Launching kafka
[Global flags]
      int AVX3Threshold                            = 4096                              {ARCH diagnostic} {default}
     bool AbortVMOnCompilationFailure              = false                                  {diagnostic} {default}
    ccstr AbortVMOnException                       =                                        {diagnostic} {default}
    ccstr AbortVMOnExceptionMessage                =                                        {diagnostic} {default}
     bool AbortVMOnSafepointTimeout                = false                                  {diagnostic} {default}
     bool AbortVMOnVMOperationTimeout              = false                                  {diagnostic} {default}
     intx AbortVMOnVMOperationTimeoutDelay         = 1000                                   {diagnostic} {default}
      int ActiveProcessorCount                     = -1                                        {product} {default}
    uintx AdaptiveSizeDecrementScaleFactor         = 4                                         {product} {default}
    uintx AdaptiveSizeMajorGCDecayTimeScale        = 10                                        {product} {default}
    uintx AdaptiveSizePolicyCollectionCostMargin   = 50                                        {product} {default}
    uintx AdaptiveSizePolicyInitializingSteps      = 20                                        {product} {default}
    uintx AdaptiveSizePolicyOutputInterval         = 0                                         {product} {default}
    uintx AdaptiveSizePolicyWeight                 = 10                                        {product} {default}
    uintx AdaptiveSizeThroughPutPolicy             = 0                                         {product} {default}
    uintx AdaptiveTimeWeight                       = 25                                        {product} {default}
     bool AggressiveHeap                           = false                                     {product} {default}
     bool AggressiveOpts                           = false                                     {product} {default}
     intx AliasLevel                               = 3                                      {C2 product} {default}
     bool AlignVector                              = false                                  {C2 product} {default}
    ccstr AllocateHeapAt                           =                                           {product} {default}
     intx AllocateInstancePrefetchLines            = 1                                         {product} {default}
     intx AllocatePrefetchDistance                 = 192                                       {product} {default}
     intx AllocatePrefetchInstr                    = 3                                         {product} {default}
     intx AllocatePrefetchLines                    = 4                                         {product} {default}
     intx AllocatePrefetchStepSize                 = 64                                        {product} {default}
     intx AllocatePrefetchStyle                    = 1                                         {product} {default}
     bool AllowJNIEnvProxy                         = false                                     {product} {default}
     bool AllowNonVirtualCalls                     = false                                     {product} {default}
     bool AllowParallelDefineClass                 = false                                     {product} {default}
     bool AllowUserSignalHandlers                  = false                                     {product} {default}
     bool AllowVectorizeOnDemand                   = true                                   {C2 product} {default}
     bool AlwaysActAsServerClassMachine            = false                                     {product} {default}
     bool AlwaysCompileLoopMethods                 = false                                     {product} {default}
     bool AlwaysLockClassLoader                    = false                                     {product} {default}
     bool AlwaysPreTouch                           = false                                     {product} {default}
     bool AlwaysRestoreFPU                         = false                                     {product} {default}
     bool AlwaysTenure                             = false                                     {product} {default}
     intx ArrayCopyLoadStoreMaxElem                = 8                                      {C2 product} {default}
     bool AssertOnSuspendWaitFailure               = false                                     {product} {default}
     bool AssumeMP                                 = true                                      {product} {default}
     intx AutoBoxCacheMax                          = 128                                    {C2 product} {default}
     intx BCEATraceLevel                           = 0                                         {product} {default}
     bool BackgroundCompilation                    = true                                   {pd product} {default}
   size_t BaseFootPrintEstimate                    = 268435456                                 {product} {default}
     intx BiasedLockingBulkRebiasThreshold         = 20                                        {product} {default}
     intx BiasedLockingBulkRevokeThreshold         = 40                                        {product} {default}
     intx BiasedLockingDecayTime                   = 25000                                     {product} {default}
     intx BiasedLockingStartupDelay                = 0                                         {product} {default}
     bool BindCMSThreadToCPU                       = false                                  {diagnostic} {default}
     bool BindGCTaskThreadsToCPUs                  = false                                     {product} {default}
     bool BlockLayoutByFrequency                   = true                                   {C2 product} {default}
     intx BlockLayoutMinDiamondPercentage          = 20                                     {C2 product} {default}
     bool BlockLayoutRotateLoops                   = true                                   {C2 product} {default}
     bool BlockOffsetArrayUseUnallocatedBlock      = false                                  {diagnostic} {default}
     bool BranchOnRegister                         = false                                  {C2 product} {default}
     bool BytecodeVerificationLocal                = false                                     {product} {default}
     bool BytecodeVerificationRemote               = true                                      {product} {default}
     bool C1OptimizeVirtualCallProfiling           = true                                   {C1 product} {default}
     bool C1ProfileBranches                        = true                                   {C1 product} {default}
     bool C1ProfileCalls                           = true                                   {C1 product} {default}
     bool C1ProfileCheckcasts                      = true                                   {C1 product} {default}
     bool C1ProfileInlinedCalls                    = true                                   {C1 product} {default}
     bool C1ProfileVirtualCalls                    = true                                   {C1 product} {default}
     bool C1UpdateMethodData                       = true                                   {C1 product} {default}
     intx CICompilerCount                          = 12                                        {product} {ergonomic}
     bool CICompilerCountPerCPU                    = true                                      {product} {default}
     bool CIPrintCompileQueue                      = false                                  {diagnostic} {default}
     bool CITime                                   = false                                     {product} {default}
     bool CMSAbortSemantics                        = false                                     {product} {default}
    uintx CMSAbortablePrecleanMinWorkPerIteration  = 100                                       {product} {default}
     intx CMSAbortablePrecleanWaitMillis           = 100                                    {manageable} {default}
   size_t CMSBitMapYieldQuantum                    = 10485760                                  {product} {default}
    uintx CMSBootstrapOccupancy                    = 50                                        {product} {default}
     bool CMSClassUnloadingEnabled                 = true                                      {product} {default}
    uintx CMSClassUnloadingMaxInterval             = 0                                         {product} {default}
     bool CMSCleanOnEnter                          = true                                      {product} {default}
   size_t CMSConcMarkMultiple                      = 32                                        {product} {default}
     bool CMSConcurrentMTEnabled                   = true                                      {product} {default}
    uintx CMSCoordinatorYieldSleepCount            = 10                                        {product} {default}
     bool CMSEdenChunksRecordAlways                = true                                      {product} {default}
    uintx CMSExpAvgFactor                          = 50                                        {product} {default}
     bool CMSExtrapolateSweep                      = false                                     {product} {default}
    uintx CMSIncrementalSafetyFactor               = 10                                        {product} {default}
    uintx CMSIndexedFreeListReplenish              = 4                                         {product} {default}
     intx CMSInitiatingOccupancyFraction           = -1                                        {product} {default}
    uintx CMSIsTooFullPercentage                   = 98                                        {product} {default}
   double CMSLargeCoalSurplusPercent               = 0.950000                                  {product} {default}
   double CMSLargeSplitSurplusPercent              = 1.000000                                  {product} {default}
     bool CMSLoopWarn                              = false                                     {product} {default}
    uintx CMSMaxAbortablePrecleanLoops             = 0                                         {product} {default}
     intx CMSMaxAbortablePrecleanTime              = 5000                                      {product} {default}
   size_t CMSOldPLABMax                            = 1024                                      {product} {default}
   size_t CMSOldPLABMin                            = 16                                        {product} {default}
    uintx CMSOldPLABNumRefills                     = 4                                         {product} {default}
    uintx CMSOldPLABReactivityFactor               = 2                                         {product} {default}
     bool CMSOldPLABResizeQuicker                  = false                                     {product} {default}
    uintx CMSOldPLABToleranceFactor                = 4                                         {product} {default}
     bool CMSPLABRecordAlways                      = true                                      {product} {default}
     bool CMSParallelInitialMarkEnabled            = true                                      {product} {default}
     bool CMSParallelRemarkEnabled                 = true                                      {product} {default}
     bool CMSParallelSurvivorRemarkEnabled         = true                                      {product} {default}
    uintx CMSPrecleanDenominator                   = 3                                         {product} {default}
    uintx CMSPrecleanIter                          = 3                                         {product} {default}
    uintx CMSPrecleanNumerator                     = 2                                         {product} {default}
     bool CMSPrecleanRefLists1                     = true                                      {product} {default}
     bool CMSPrecleanRefLists2                     = false                                     {product} {default}
     bool CMSPrecleanSurvivors1                    = false                                     {product} {default}
     bool CMSPrecleanSurvivors2                    = true                                      {product} {default}
    uintx CMSPrecleanThreshold                     = 1000                                      {product} {default}
     bool CMSPrecleaningEnabled                    = true                                      {product} {default}
     bool CMSPrintChunksInDump                     = false                                     {product} {default}
     bool CMSPrintObjectsInDump                    = false                                     {product} {default}
    uintx CMSRemarkVerifyVariant                   = 1                                         {product} {default}
     bool CMSReplenishIntermediate                 = true                                      {product} {default}
   size_t CMSRescanMultiple                        = 32                                        {product} {default}
    uintx CMSSamplingGrain                         = 16384                                     {product} {default}
     bool CMSScavengeBeforeRemark                  = false                                     {product} {default}
    uintx CMSScheduleRemarkEdenPenetration         = 50                                        {product} {default}
   size_t CMSScheduleRemarkEdenSizeThreshold       = 2097152                                   {product} {default}
    uintx CMSScheduleRemarkSamplingRatio           = 5                                         {product} {default}
   double CMSSmallCoalSurplusPercent               = 1.050000                                  {product} {default}
   double CMSSmallSplitSurplusPercent              = 1.100000                                  {product} {default}
     bool CMSSplitIndexedFreeListBlocks            = true                                      {product} {default}
     intx CMSTriggerInterval                       = -1                                     {manageable} {default}
    uintx CMSTriggerRatio                          = 80                                        {product} {default}
     intx CMSWaitDuration                          = 2000                                   {manageable} {default}
    uintx CMSWorkQueueDrainThreshold               = 10                                        {product} {default}
     bool CMSYield                                 = true                                      {product} {default}
    uintx CMSYieldSleepCount                       = 0                                         {product} {default}
   size_t CMSYoungGenPerWorker                     = 67108864                               {pd product} {default}
    uintx CMS_FLSPadding                           = 1                                         {product} {default}
    uintx CMS_FLSWeight                            = 75                                        {product} {default}
    uintx CMS_SweepPadding                         = 1                                         {product} {default}
    uintx CMS_SweepTimerThresholdMillis            = 10                                        {product} {default}
    uintx CMS_SweepWeight                          = 75                                        {product} {default}
    uintx CPUForCMSThread                          = 0                                      {diagnostic} {default}
     bool CalculateClassFingerprint                = false                                     {product} {default}
     bool CheckIntrinsics                          = true                                   {diagnostic} {default}
     bool CheckJNICalls                            = false                                     {product} {default}
     bool ClassUnloading                           = true                                      {product} {default}
     bool ClassUnloadingWithConcurrentMark         = true                                      {product} {default}
     bool ClipInlining                             = true                                      {product} {default}
    uintx CodeCacheExpansionSize                   = 65536                                  {pd product} {default}
    uintx CodeCacheMinBlockLength                  = 6                                   {pd diagnostic} {default}
     bool CompactFields                            = true                                      {product} {default}
     bool CompactStrings                           = true                                   {pd product} {default}
     intx CompilationPolicyChoice                  = 2                                         {product} {default}
ccstrlist CompileCommand                           =                                           {product} {default}
    ccstr CompileCommandFile                       =                                           {product} {default}
ccstrlist CompileOnly                              =                                           {product} {default}
     intx CompileThreshold                         = 10000                                  {pd product} {default}
   double CompileThresholdScaling                  = 1.000000                                  {product} {default}
    ccstr CompilerDirectivesFile                   =                                        {diagnostic} {default}
     bool CompilerDirectivesIgnoreCompileCommands  = false                                  {diagnostic} {default}
      int CompilerDirectivesLimit                  = 50                                     {diagnostic} {default}
     bool CompilerDirectivesPrint                  = false                                  {diagnostic} {default}
     bool CompilerThreadHintNoPreempt              = false                                     {product} {default}
     intx CompilerThreadPriority                   = -1                                        {product} {default}
     intx CompilerThreadStackSize                  = 1024                                   {pd product} {default}
   size_t CompressedClassSpaceSize                 = 1073741824                                {product} {default}
     uint ConcGCThreads                            = 1                                         {product} {command line}
     intx ConditionalMoveLimit                     = 3                                   {C2 pd product} {default}
     intx ContendedPaddingWidth                    = 128                                       {product} {default}
     bool CrashOnOutOfMemoryError                  = false                                     {product} {default}
     bool CreateCoredumpOnCrash                    = true                                      {product} {default}
     bool CriticalJNINatives                       = true                                      {product} {default}
     bool DTraceAllocProbes                        = false                                     {product} {default}
     bool DTraceMethodProbes                       = false                                     {product} {default}
     bool DTraceMonitorProbes                      = false                                     {product} {default}
     bool DebugInlinedCalls                        = true                                {C2 diagnostic} {default}
     bool DebugNonSafepoints                       = false                                  {diagnostic} {default}
     bool Debugging                                = false                                     {product} {default}
     bool DeferInitialCardMark                     = false                                  {diagnostic} {default}
     bool DeoptimizeRandom                         = false                                     {product} {default}
     bool DisableAttachMechanism                   = false                                     {product} {default}
     bool DisableExplicitGC                        = false                                     {product} {default}
ccstrlist DisableIntrinsic                         =                                        {diagnostic} {default}
     bool DisplayVMOutput                          = true                                   {diagnostic} {default}
     bool DisplayVMOutputToStderr                  = false                                     {product} {default}
     bool DisplayVMOutputToStdout                  = false                                     {product} {default}
     bool DoEscapeAnalysis                         = true                                   {C2 product} {default}
     bool DoReserveCopyInSuperWord                 = true                                   {C2 product} {default}
     intx DominatorSearchLimit                     = 1000                                {C2 diagnostic} {default}
     bool DontCompileHugeMethods                   = true                                      {product} {default}
     bool DontYieldALot                            = false                                  {pd product} {default}
    ccstr DumpLoadedClassList                      =                                           {product} {default}
     bool DumpPrivateMappingsInCore                = true                                   {diagnostic} {default}
     bool DumpReplayDataOnError                    = true                                      {product} {default}
     bool DumpSharedMappingsInCore                 = true                                   {diagnostic} {default}
     bool DumpSharedSpaces                         = false                                     {product} {default}
     bool DynamicallyResizeSystemDictionaries      = true                                   {diagnostic} {default}
     bool EagerXrunInit                            = false                                     {product} {default}
     intx EliminateAllocationArraySizeLimit        = 64                                     {C2 product} {default}
     bool EliminateAllocations                     = true                                   {C2 product} {default}
     bool EliminateAutoBox                         = true                                   {C2 product} {default}
     bool EliminateLocks                           = true                                   {C2 product} {default}
     bool EliminateNestedLocks                     = true                                   {C2 product} {default}
     bool EnableContended                          = true                                      {product} {default}
     bool EnableDynamicAgentLoading                = true                                      {product} {default}
     bool EnableThreadSMRExtraValidityChecks       = true                                   {diagnostic} {default}
     bool EnableThreadSMRStatistics                = false                                  {diagnostic} {default}
   size_t ErgoHeapSizeLimit                        = 0                                         {product} {default}
    ccstr ErrorFile                                =                                           {product} {default}
     bool ErrorFileToStderr                        = false                                     {product} {default}
     bool ErrorFileToStdout                        = false                                     {product} {default}
 uint64_t ErrorLogTimeout                          = 120                                       {product} {default}
    ccstr ErrorReportServer                        =                                           {product} {default}
   double EscapeAnalysisTimeout                    = 20.000000                              {C2 product} {default}
     bool EstimateArgEscape                        = true                                      {product} {default}
     bool ExecutingUnitTests                       = false                                     {product} {default}
     bool ExitOnOutOfMemoryError                   = false                                     {product} {default}
     bool ExplicitGCInvokesConcurrent              = true                                      {product} {command line}
     bool ExtendedDTraceProbes                     = false                                     {product} {default}
     bool ExtensiveErrorReports                    = false                                     {product} {default}
    ccstr ExtraSharedClassListFile                 =                                           {product} {default}
     bool FLSAlwaysCoalesceLarge                   = false                                     {product} {default}
    uintx FLSCoalescePolicy                        = 2                                         {product} {default}
   double FLSLargestBlockCoalesceProximity         = 0.990000                                  {product} {default}
     bool FLSVerifyAllHeapReferences               = false                                  {diagnostic} {default}
     bool FLSVerifyIndexTable                      = false                                  {diagnostic} {default}
     bool FLSVerifyLists                           = false                                  {diagnostic} {default}
     bool FailOverToOldVerifier                    = true                                      {product} {default}
     intx FieldsAllocationStyle                    = 1                                         {product} {default}
     bool FilterSpuriousWakeups                    = true                                      {product} {default}
     bool FlightRecorder                           = false                                     {product} {default}
    ccstr FlightRecorderOptions                    =                                           {product} {default}
     bool FoldStableValues                         = true                                   {diagnostic} {default}
     bool ForceDynamicNumberOfGCThreads            = false                                  {diagnostic} {default}
     bool ForceNUMA                                = false                                     {product} {default}
     bool ForceTimeHighResolution                  = false                                     {product} {default}
     bool ForceUnreachable                         = false                                  {diagnostic} {default}
     intx FreqInlineSize                           = 325                                    {pd product} {default}
   double G1ConcMarkStepDurationMillis             = 10.000000                                 {product} {default}
    uintx G1ConcRSHotCardLimit                     = 4                                         {product} {default}
   size_t G1ConcRSLogCacheSize                     = 10                                        {product} {default}
   size_t G1ConcRefinementGreenZone                = 0                                         {product} {default}
   size_t G1ConcRefinementRedZone                  = 0                                         {product} {default}
    uintx G1ConcRefinementServiceIntervalMillis    = 300                                       {product} {default}
     uint G1ConcRefinementThreads                  = 1                                         {product} {ergonomic}
   size_t G1ConcRefinementThresholdStep            = 2                                         {product} {default}
   size_t G1ConcRefinementYellowZone               = 0                                         {product} {default}
    uintx G1ConfidencePercent                      = 50                                        {product} {default}
   size_t G1HeapRegionSize                         = 16777216                                  {product} {command line}
    uintx G1HeapWastePercent                       = 5                                         {product} {default}
    uintx G1MixedGCCountTarget                     = 8                                         {product} {default}
     intx G1RSetRegionEntries                      = 1280                                      {product} {default}
   size_t G1RSetScanBlockSize                      = 64                                        {product} {default}
     intx G1RSetSparseRegionEntries                = 20                                        {product} {default}
     intx G1RSetUpdatingPauseTimePercent           = 10                                        {product} {default}
     uint G1RefProcDrainInterval                   = 1000                                      {product} {default}
    uintx G1ReservePercent                         = 10                                        {product} {default}
    uintx G1SATBBufferEnqueueingThresholdPercent   = 60                                        {product} {default}
   size_t G1SATBBufferSize                         = 1024                                      {product} {default}
     intx G1SummarizeRSetStatsPeriod               = 0                                      {diagnostic} {default}
   size_t G1UpdateBufferSize                       = 256                                       {product} {default}
     bool G1UseAdaptiveConcRefinement              = true                                      {product} {default}
     bool G1UseAdaptiveIHOP                        = true                                      {product} {default}
     bool G1VerifyHeapRegionCodeRoots              = false                                  {diagnostic} {default}
     bool G1VerifyRSetsDuringFullGC                = false                                  {diagnostic} {default}
    uintx GCDrainStackTargetSize                   = 64                                        {product} {ergonomic}
    uintx GCHeapFreeLimit                          = 2                                         {product} {default}
    uintx GCLockerEdenExpansionPercent             = 5                                         {product} {default}
     bool GCLockerInvokesConcurrent                = false                                     {product} {default}
    uintx GCLockerRetryAllocationCount             = 2                                      {diagnostic} {default}
     bool GCParallelVerificationEnabled            = true                                   {diagnostic} {default}
    uintx GCPauseIntervalMillis                    = 21                                        {product} {default}
     uint GCTaskTimeStampEntries                   = 200                                       {product} {default}
    uintx GCTimeLimit                              = 98                                        {product} {default}
    uintx GCTimeRatio                              = 12                                        {product} {default}
     intx GuaranteedSafepointInterval              = 1000                                   {diagnostic} {default}
     uint HandshakeTimeout                         = 0                                      {diagnostic} {default}
   size_t HeapBaseMinAddress                       = 2147483648                             {pd product} {default}
     bool HeapDumpAfterFullGC                      = false                                  {manageable} {default}
     bool HeapDumpBeforeFullGC                     = false                                  {manageable} {default}
     bool HeapDumpOnOutOfMemoryError               = false                                  {manageable} {default}
    ccstr HeapDumpPath                             =                                        {manageable} {default}
    uintx HeapFirstMaximumCompactionCount          = 3                                         {product} {default}
    uintx HeapMaximumCompactionInterval            = 20                                        {product} {default}
    uintx HeapSearchSteps                          = 3                                         {product} {default}
   size_t HeapSizePerGCThread                      = 43620760                                  {product} {default}
     intx HotMethodDetectionLimit                  = 100000                                 {diagnostic} {default}
     bool IdealizeClearArrayNode                   = true                             {C2 pd diagnostic} {default}
     bool IgnoreEmptyClassPaths                    = false                                     {product} {default}
     bool IgnoreUnrecognizedVMOptions              = false                                     {product} {default}
     bool IgnoreUnverifiableClassesDuringDump      = true                                   {diagnostic} {default}
     bool ImplicitNullChecks                       = true                                {pd diagnostic} {default}
    uintx IncreaseFirstTierCompileThresholdAt      = 50                                        {product} {default}
     bool IncrementalInline                        = true                                   {C2 product} {default}
     intx InitArrayShortSize                       = 64                                  {pd diagnostic} {default}
   size_t InitialBootClassLoaderMetaspaceSize      = 4194304                                   {product} {default}
    uintx InitialCodeCacheSize                     = 2555904                                {pd product} {default}
   size_t InitialHeapSize                          = 1056964608                                {product} {ergonomic}
    uintx InitialRAMFraction                       = 64                                        {product} {default}
   double InitialRAMPercentage                     = 1.562500                                  {product} {default}
    uintx InitialSurvivorRatio                     = 8                                         {product} {default}
    uintx InitialTenuringThreshold                 = 7                                         {product} {default}
    uintx InitiatingHeapOccupancyPercent           = 35                                        {product} {command line}
     bool InjectGCWorkerCreationFailure            = false                                  {diagnostic} {default}
     bool Inline                                   = true                                      {product} {default}
     bool InlineArrayCopy                          = true                                   {diagnostic} {default}
     bool InlineClassNatives                       = true                                   {diagnostic} {default}
    ccstr InlineDataFile                           =                                           {product} {default}
     intx InlineFrequencyCount                     = 100                                 {pd diagnostic} {default}
     bool InlineMathNatives                        = true                                   {diagnostic} {default}
     bool InlineNIOCheckIndex                      = true                                {C1 diagnostic} {default}
     bool InlineNatives                            = true                                   {diagnostic} {default}
     bool InlineObjectCopy                         = true                                {C2 diagnostic} {default}
     bool InlineObjectHash                         = true                                   {diagnostic} {default}
     bool InlineReflectionGetCallerClass           = true                                {C2 diagnostic} {default}
     intx InlineSmallCode                          = 2000                                   {pd product} {default}
     bool InlineSynchronizedMethods                = true                                   {C1 product} {default}
     bool InlineThreadNatives                      = true                                   {diagnostic} {default}
     bool InlineUnsafeOps                          = true                                   {diagnostic} {default}
     bool InsertMemBarAfterArraycopy               = true                                   {C2 product} {default}
     intx InteriorEntryAlignment                   = 16                                  {C2 pd product} {default}
     intx InterpreterProfilePercentage             = 33                                        {product} {default}
     bool JavaMonitorsInStackTrace                 = true                                      {product} {default}
     intx JavaPriority10_To_OSPriority             = -1                                        {product} {default}
     intx JavaPriority1_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority2_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority3_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority4_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority5_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority6_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority7_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority8_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority9_To_OSPriority              = -1                                        {product} {default}
     bool LIRFillDelaySlots                        = false                               {C1 pd product} {default}
   size_t LargePageHeapSizeThreshold               = 134217728                                 {product} {default}
   size_t LargePageSizeInBytes                     = 0                                         {product} {default}
     intx LiveNodeCountInliningCutoff              = 40000                                  {C2 product} {default}
     bool LoadExecStackDllInVMThread               = true                                      {product} {default}
     bool LogCompilation                           = false                                  {diagnostic} {default}
     bool LogEvents                                = true                                   {diagnostic} {default}
    uintx LogEventsBufferEntries                   = 20                                     {diagnostic} {default}
    ccstr LogFile                                  =                                        {diagnostic} {default}
     bool LogTouchedMethods                        = false                                  {diagnostic} {default}
     bool LogVMOutput                              = false                                  {diagnostic} {default}
     intx LoopMaxUnroll                            = 16                                     {C2 product} {default}
     intx LoopOptsCount                            = 43                                     {C2 product} {default}
     intx LoopPercentProfileLimit                  = 10                                  {C2 pd product} {default}
    uintx LoopStripMiningIter                      = 1000                                   {C2 product} {default}
    uintx LoopStripMiningIterShortLoop             = 100                                    {C2 product} {default}
     intx LoopUnrollLimit                          = 60                                  {C2 pd product} {default}
     intx LoopUnrollMin                            = 4                                      {C2 product} {default}
     bool LoopUnswitching                          = true                                   {C2 product} {default}
    uintx MallocMaxTestWords                       = 0                                      {diagnostic} {default}
     bool ManagementServer                         = true                                      {product} {command line}
   size_t MarkStackSize                            = 4194304                                   {product} {ergonomic}
   size_t MarkStackSizeMax                         = 16777216                                  {product} {default}
     uint MarkSweepAlwaysCompactCount              = 4                                         {product} {default}
    uintx MarkSweepDeadRatio                       = 5                                         {product} {default}
     intx MaxBCEAEstimateLevel                     = 5                                         {product} {default}
     intx MaxBCEAEstimateSize                      = 150                                       {product} {default}
 uint64_t MaxDirectMemorySize                      = 0                                         {product} {default}
     bool MaxFDLimit                               = true                                      {product} {default}
    uintx MaxGCMinorPauseMillis                    = 18446744073709551615                      {product} {default}
    uintx MaxGCPauseMillis                         = 20                                        {product} {command line}
    uintx MaxHeapFreeRatio                         = 70                                     {manageable} {default}
   size_t MaxHeapSize                              = 16861102080                               {product} {ergonomic}
     intx MaxInlineLevel                           = 15                                        {product} {default}
     intx MaxInlineSize                            = 35                                        {product} {default}
     intx MaxJNILocalCapacity                      = 65536                                     {product} {default}
     intx MaxJavaStackTraceDepth                   = 1024                                      {product} {default}
     intx MaxJumpTableSize                         = 65000                                  {C2 product} {default}
     intx MaxJumpTableSparseness                   = 5                                      {C2 product} {default}
     intx MaxLabelRootDepth                        = 1100                                   {C2 product} {default}
     intx MaxLoopPad                               = 11                                     {C2 product} {default}
   size_t MaxMetaspaceExpansion                    = 5451776                                   {product} {default}
    uintx MaxMetaspaceFreeRatio                    = 80                                        {product} {command line}
   size_t MaxMetaspaceSize                         = 18446744073709547520                      {product} {default}
   size_t MaxNewSize                               = 10116661248                               {product} {ergonomic}
     intx MaxNodeLimit                             = 80000                                  {C2 product} {default}
 uint64_t MaxRAM                                   = 137438953472                           {pd product} {default}
    uintx MaxRAMFraction                           = 4                                         {product} {default}
   double MaxRAMPercentage                         = 25.000000                                 {product} {default}
     intx MaxRecursiveInlineLevel                  = 1                                         {product} {default}
    uintx MaxTenuringThreshold                     = 15                                        {product} {default}
     intx MaxTrivialSize                           = 6                                         {product} {default}
     intx MaxVectorSize                            = 64                                     {C2 product} {default}
   size_t MetaspaceSize                            = 100663296                              {pd product} {command line}
     bool MethodFlushing                           = true                                      {product} {default}
   size_t MinHeapDeltaBytes                        = 16777216                                  {product} {ergonomic}
    uintx MinHeapFreeRatio                         = 40                                     {manageable} {default}
     intx MinInliningThreshold                     = 250                                       {product} {default}
     intx MinJumpTableSize                         = 10                                  {C2 pd product} {default}
   size_t MinMetaspaceExpansion                    = 339968                                    {product} {default}
    uintx MinMetaspaceFreeRatio                    = 50                                        {product} {command line}
     intx MinPassesBeforeFlush                     = 10                                     {diagnostic} {default}
    uintx MinRAMFraction                           = 2                                         {product} {default}
   double MinRAMPercentage                         = 50.000000                                 {product} {default}
    uintx MinSurvivorRatio                         = 3                                         {product} {default}
   size_t MinTLABSize                              = 2048                                      {product} {default}
     intx MonitorBound                             = 0                                         {product} {default}
     bool MonitorInUseLists                        = true                                      {product} {default}
     intx MultiArrayExpandLimit                    = 6                                      {C2 product} {default}
    uintx NUMAChunkResizeWeight                    = 20                                        {product} {default}
   size_t NUMAInterleaveGranularity                = 2097152                                   {product} {default}
    uintx NUMAPageScanRate                         = 256                                       {product} {default}
   size_t NUMASpaceResizeRate                      = 1073741824                                {product} {default}
     bool NUMAStats                                = false                                     {product} {default}
    ccstr NativeMemoryTracking                     = off                                       {product} {default}
     bool NeedsDeoptSuspend                        = false                                  {pd product} {default}
     bool NeverActAsServerClassMachine             = false                                  {pd product} {default}
     bool NeverTenure                              = false                                     {product} {default}
    uintx NewRatio                                 = 2                                         {product} {default}
   size_t NewSize                                  = 1363144                                   {product} {default}
   size_t NewSizeThreadIncrease                    = 5320                                   {pd product} {default}
     intx NmethodSweepActivity                     = 10                                        {product} {default}
     intx NodeLimitFudgeFactor                     = 2000                                   {C2 product} {default}
    uintx NonNMethodCodeHeapSize                   = 7594288                                {pd product} {ergonomic}
    uintx NonProfiledCodeHeapSize                  = 122031976                              {pd product} {ergonomic}
     intx NumberOfLoopInstrToAlign                 = 4                                      {C2 product} {default}
     intx ObjectAlignmentInBytes                   = 8                                    {lp64_product} {default}
   size_t OldPLABSize                              = 1024                                      {product} {default}
    uintx OldPLABWeight                            = 50                                        {product} {default}
   size_t OldSize                                  = 5452592                                   {product} {default}
     bool OmitStackTraceInFastThrow                = true                                      {product} {default}
ccstrlist OnError                                  =                                           {product} {default}
ccstrlist OnOutOfMemoryError                       =                                           {product} {default}
     intx OnStackReplacePercentage                 = 140                                    {pd product} {default}
     bool OptimizeExpensiveOps                     = true                                {C2 diagnostic} {default}
     bool OptimizeFill                             = true                                   {C2 product} {default}
     bool OptimizePtrCompare                       = true                                   {C2 product} {default}
     bool OptimizeStringConcat                     = true                                   {C2 product} {default}
     bool OptoBundling                             = false                               {C2 pd product} {default}
     intx OptoLoopAlignment                        = 16                                     {pd product} {default}
     bool OptoRegScheduling                        = true                                {C2 pd product} {default}
     bool OptoScheduling                           = false                               {C2 pd product} {default}
     bool OverrideVMProperties                     = false                                     {product} {default}
    uintx PLABWeight                               = 75                                        {product} {default}
     bool PSChunkLargeArrays                       = true                                      {product} {default}
      int ParGCArrayScanChunk                      = 50                                        {product} {default}
     intx ParGCCardsPerStrideChunk                 = 256                                    {diagnostic} {default}
    uintx ParGCDesiredObjsFromOverflowList         = 20                                        {product} {default}
    uintx ParGCStridesPerThread                    = 2                                      {diagnostic} {default}
     bool ParGCTrimOverflow                        = true                                      {product} {default}
     bool ParGCUseLocalOverflow                    = false                                     {product} {default}
    uintx ParallelGCBufferWastePct                 = 10                                        {product} {default}
     uint ParallelGCThreads                        = 1                                         {product} {command line}
   size_t ParallelOldDeadWoodLimiterMean           = 50                                        {product} {default}
   size_t ParallelOldDeadWoodLimiterStdDev         = 80                                        {product} {default}
     bool ParallelRefProcBalancingEnabled          = true                                      {product} {default}
     bool ParallelRefProcEnabled                   = false                                     {product} {default}
     bool PartialPeelAtUnsignedTests               = true                                   {C2 product} {default}
     bool PartialPeelLoop                          = true                                   {C2 product} {default}
     intx PartialPeelNewPhiDelta                   = 0                                      {C2 product} {default}
     bool PauseAtExit                              = false                                  {diagnostic} {default}
     bool PauseAtStartup                           = false                                  {diagnostic} {default}
    ccstr PauseAtStartupFile                       =                                        {diagnostic} {default}
    uintx PausePadding                             = 1                                         {product} {default}
     intx PerBytecodeRecompilationCutoff           = 200                                       {product} {default}
     intx PerBytecodeTrapLimit                     = 4                                         {product} {default}
     intx PerMethodRecompilationCutoff             = 400                                       {product} {default}
     intx PerMethodTrapLimit                       = 100                                       {product} {default}
     bool PerfAllowAtExitRegistration              = false                                     {product} {default}
     bool PerfBypassFileSystemCheck                = false                                     {product} {default}
     intx PerfDataMemorySize                       = 32768                                     {product} {default}
     intx PerfDataSamplingInterval                 = 50                                        {product} {default}
    ccstr PerfDataSaveFile                         =                                           {product} {default}
     bool PerfDataSaveToFile                       = false                                     {product} {default}
     bool PerfDisableSharedMem                     = true                                      {product} {default}
     intx PerfMaxStringConstLength                 = 1024                                      {product} {default}
   size_t PreTouchParallelChunkSize                = 1073741824                                {product} {default}
     bool PreferContainerQuotaForCPUCount          = true                                      {product} {default}
     bool PreferInterpreterNativeStubs             = false                                  {pd product} {default}
     intx PrefetchCopyIntervalInBytes              = 576                                       {product} {default}
     intx PrefetchFieldsAhead                      = 1                                         {product} {default}
     intx PrefetchScanIntervalInBytes              = 576                                       {product} {default}
     bool PreserveAllAnnotations                   = false                                     {product} {default}
     bool PreserveFramePointer                     = false                                  {pd product} {default}
   size_t PretenureSizeThreshold                   = 0                                         {product} {default}
     bool PrintAdapterHandlers                     = false                                  {diagnostic} {default}
     bool PrintAssembly                            = false                                  {diagnostic} {default}
    ccstr PrintAssemblyOptions                     =                                        {diagnostic} {default}
     bool PrintBiasedLockingStatistics             = false                                  {diagnostic} {default}
     bool PrintClassHistogram                      = false                                  {manageable} {default}
     bool PrintCodeCache                           = false                                     {product} {default}
     bool PrintCodeCacheOnCompilation              = false                                     {product} {default}
     bool PrintCommandLineFlags                    = false                                     {product} {default}
     bool PrintCompilation                         = false                                     {product} {default}
     bool PrintCompilation2                        = false                                  {diagnostic} {default}
     bool PrintConcurrentLocks                     = false                                  {manageable} {default}
     bool PrintExtendedThreadInfo                  = false                                     {product} {default}
     bool PrintFlagsFinal                          = true                                      {product} {command line}
     bool PrintFlagsInitial                        = false                                     {product} {default}
     bool PrintFlagsRanges                         = false                                     {product} {default}
     bool PrintGC                                  = false                                     {product} {default}
     bool PrintGCDetails                           = false                                     {product} {default}
     bool PrintHeapAtSIGBREAK                      = true                                      {product} {default}
     bool PrintInlining                            = false                                  {diagnostic} {default}
     bool PrintInterpreter                         = false                                  {diagnostic} {default}
     bool PrintIntrinsics                          = false                               {C2 diagnostic} {default}
     bool PrintJNIResolving                        = false                                     {product} {default}
     bool PrintMetaspaceStatisticsAtExit           = false                                  {diagnostic} {default}
     bool PrintMethodData                          = false                                  {diagnostic} {default}
     bool PrintMethodFlushingStatistics            = false                                  {diagnostic} {default}
     bool PrintMethodHandleStubs                   = false                                  {diagnostic} {default}
     bool PrintNMTStatistics                       = false                                  {diagnostic} {default}
     bool PrintNMethods                            = false                                  {diagnostic} {default}
     bool PrintNativeNMethods                      = false                                  {diagnostic} {default}
     bool PrintOptoAssembly                        = false                               {C2 diagnostic} {default}
     bool PrintPreciseBiasedLockingStatistics      = false                               {C2 diagnostic} {default}
     bool PrintPreciseRTMLockingStatistics         = false                               {C2 diagnostic} {default}
     bool PrintSafepointStatistics                 = false                                     {product} {default}
     intx PrintSafepointStatisticsCount            = 300                                       {product} {default}
     intx PrintSafepointStatisticsTimeout          = -1                                        {product} {default}
     bool PrintSharedArchiveAndExit                = false                                     {product} {default}
     bool PrintSharedDictionary                    = false                                     {product} {default}
     bool PrintSignatureHandlers                   = false                                  {diagnostic} {default}
     bool PrintStringTableStatistics               = false                                     {product} {default}
     bool PrintStubCode                            = false                                  {diagnostic} {default}
     bool PrintTieredEvents                        = false                                     {product} {default}
     bool PrintTouchedMethodsAtExit                = false                                  {diagnostic} {default}
     bool PrintVMOptions                           = false                                     {product} {default}
     bool PrintVMQWaitTime                         = false                                     {product} {default}
     bool PrintWarnings                            = true                                      {product} {default}
    uintx ProcessDistributionStride                = 4                                         {product} {default}
     bool ProfileDynamicTypes                      = true                                {C2 diagnostic} {default}
     bool ProfileInterpreter                       = true                                   {pd product} {default}
     bool ProfileIntervals                         = false                                     {product} {default}
     intx ProfileIntervalsTicks                    = 100                                       {product} {default}
     intx ProfileMaturityPercentage                = 20                                        {product} {default}
     bool ProfileVM                                = false                                     {product} {default}
    uintx ProfiledCodeHeapSize                     = 122031976                              {pd product} {ergonomic}
     intx ProfilerNumberOfCompiledMethods          = 25                                     {diagnostic} {default}
     intx ProfilerNumberOfInterpretedMethods       = 25                                     {diagnostic} {default}
     intx ProfilerNumberOfRuntimeStubNodes         = 25                                     {diagnostic} {default}
     intx ProfilerNumberOfStubMethods              = 25                                     {diagnostic} {default}
     bool ProfilerPrintByteCodeStatistics          = false                                     {product} {default}
     bool ProfilerRecordPC                         = false                                     {product} {default}
    uintx PromotedPadding                          = 3                                         {product} {default}
    uintx QueuedAllocationWarningCount             = 0                                         {product} {default}
      int RTMRetryCount                            = 5                                    {ARCH product} {default}
     bool RangeCheckElimination                    = true                                      {product} {default}
     bool ReassociateInvariants                    = true                                   {C2 product} {default}
     bool ReduceBulkZeroing                        = true                                   {C2 product} {default}
     bool ReduceFieldZeroing                       = true                                   {C2 product} {default}
     bool ReduceInitialCardMarks                   = true                                   {C2 product} {default}
     bool ReduceNumberOfCompilerThreads            = true                                   {diagnostic} {default}
     bool ReduceSignalUsage                        = false                                     {product} {default}
     intx RefDiscoveryPolicy                       = 0                                         {product} {default}
     bool RegisterFinalizersAtInit                 = true                                      {product} {default}
     bool RelaxAccessControlCheck                  = false                                     {product} {default}
    ccstr ReplayDataFile                           =                                           {product} {default}
     bool RequireSharedSpaces                      = false                                     {product} {default}
    uintx ReservedCodeCacheSize                    = 251658240                              {pd product} {ergonomic}
     bool ResizeOldPLAB                            = true                                      {product} {default}
     bool ResizePLAB                               = true                                      {product} {default}
     bool ResizeTLAB                               = true                                   {pd product} {default}
     bool RestoreMXCSROnJNICalls                   = false                                     {product} {default}
     bool RestrictContended                        = true                                      {product} {default}
     bool RestrictReservedStack                    = true                                      {product} {default}
     bool RewriteBytecodes                         = true                                   {pd product} {default}
     bool RewriteFrequentPairs                     = true                                   {pd product} {default}
     bool SafepointALot                            = false                                  {diagnostic} {default}
     bool SafepointTimeout                         = false                                     {product} {default}
     intx SafepointTimeoutDelay                    = 10000                                     {product} {default}
     bool ScavengeBeforeFullGC                     = false                                     {product} {default}
     intx ScavengeRootsInCode                      = 2                                      {diagnostic} {default}
     bool SegmentedCodeCache                       = true                                      {product} {ergonomic}
     intx SelfDestructTimer                        = 0                                         {product} {default}
     bool SerializeVMOutput                        = true                                   {diagnostic} {default}
    ccstr SharedArchiveConfigFile                  =                                           {product} {default}
    ccstr SharedArchiveFile                        =                                           {product} {default}
   size_t SharedBaseAddress                        = 34359738368                               {product} {default}
    ccstr SharedClassListFile                      =                                           {product} {default}
    uintx SharedSymbolTableBucketSize              = 4                                         {product} {default}
     bool ShenandoahAllocFailureALot               = false                                  {diagnostic} {default}
     bool ShenandoahCASBarrier                     = true                                   {diagnostic} {default}
     bool ShenandoahCloneBarrier                   = true                                   {diagnostic} {default}
    uintx ShenandoahCodeRootsStyle                 = 2                                      {diagnostic} {default}
     bool ShenandoahDegeneratedGC                  = true                                   {diagnostic} {default}
     bool ShenandoahElasticTLAB                    = true                                   {diagnostic} {default}
    ccstr ShenandoahGCHeuristics                   = adaptive                                  {product} {default}
    ccstr ShenandoahGCMode                         = satb                                      {product} {default}
     bool ShenandoahHumongousMoves                 = true                                   {diagnostic} {default}
     bool ShenandoahIUBarrier                      = false                                  {diagnostic} {default}
     bool ShenandoahLoadRefBarrier                 = true                                   {diagnostic} {default}
     bool ShenandoahLoopOptsAfterExpansion         = true                                   {diagnostic} {default}
     bool ShenandoahOOMDuringEvacALot              = false                                  {diagnostic} {default}
     bool ShenandoahOptimizeStaticFinals           = true                                   {diagnostic} {default}
     bool ShenandoahPreclean                       = true                                   {diagnostic} {default}
     bool ShenandoahSATBBarrier                    = true                                   {diagnostic} {default}
     bool ShenandoahSelfFixing                     = true                                   {diagnostic} {default}
   size_t ShenandoahSoftMaxHeapSize                = 0                                      {manageable} {default}
     bool ShenandoahVerify                         = false                                  {diagnostic} {default}
     intx ShenandoahVerifyLevel                    = 4                                      {diagnostic} {default}
     bool ShowHiddenFrames                         = false                                  {diagnostic} {default}
     bool ShowMessageBoxOnError                    = false                                     {product} {default}
     bool ShowRegistersOnAssert                    = false                                  {diagnostic} {default}
     bool ShrinkHeapInSteps                        = true                                      {product} {default}
     intx SoftRefLRUPolicyMSPerMB                  = 1000                                      {product} {default}
     bool SpecialArraysEquals                      = true                                {C2 diagnostic} {default}
     bool SpecialEncodeISOArray                    = true                                {C2 diagnostic} {default}
     bool SpecialStringCompareTo                   = true                                {C2 diagnostic} {default}
     bool SpecialStringEquals                      = true                                {C2 diagnostic} {default}
     bool SpecialStringIndexOf                     = true                                {C2 diagnostic} {default}
     bool SplitIfBlocks                            = true                                   {C2 product} {default}
     intx StackRedPages                            = 1                                      {pd product} {default}
     intx StackReservedPages                       = 1                                      {pd product} {default}
     intx StackShadowPages                         = 20                                     {pd product} {default}
     bool StackTraceInThrowable                    = true                                      {product} {default}
     intx StackYellowPages                         = 2                                      {pd product} {default}
    uintx StartAggressiveSweepingAt                = 10                                        {product} {default}
     bool StartAttachListener                      = false                                     {product} {default}
    ccstr StartFlightRecording                     =                                           {product} {default}
     bool StressCodeAging                          = false                                  {diagnostic} {default}
     bool StressGCM                                = false                               {C2 diagnostic} {default}
     bool StressLCM                                = false                               {C2 diagnostic} {default}
     bool StressLdcRewrite                         = false                                     {product} {default}
    uintx StringDeduplicationAgeThreshold          = 3                                         {product} {default}
     bool StringDeduplicationRehashALot            = false                                  {diagnostic} {default}
     bool StringDeduplicationResizeALot            = false                                  {diagnostic} {default}
    uintx StringTableSize                          = 65536                                     {product} {default}
     bool SuperWordLoopUnrollAnalysis              = true                                {C2 pd product} {default}
     bool SuperWordReductions                      = true                                   {C2 product} {default}
     bool SuppressFatalErrorMessage                = false                                     {product} {default}
    uintx SurvivorPadding                          = 3                                         {product} {default}
    uintx SurvivorRatio                            = 8                                         {product} {default}
     intx SuspendRetryCount                        = 50                                        {product} {default}
     intx SuspendRetryDelay                        = 5                                         {product} {default}
    uintx TLABAllocationWeight                     = 35                                        {product} {default}
    uintx TLABRefillWasteFraction                  = 64                                        {product} {default}
   size_t TLABSize                                 = 0                                         {product} {default}
     bool TLABStats                                = true                                      {product} {default}
    uintx TLABWasteIncrement                       = 4                                         {product} {default}
    uintx TLABWasteTargetPercent                   = 1                                         {product} {default}
    uintx TargetPLABWastePct                       = 10                                        {product} {default}
    uintx TargetSurvivorRatio                      = 50                                        {product} {default}
    uintx TenuredGenerationSizeIncrement           = 20                                        {product} {default}
    uintx TenuredGenerationSizeSupplement          = 80                                        {product} {default}
    uintx TenuredGenerationSizeSupplementDecay     = 2                                         {product} {default}
     bool ThreadLocalHandshakes                    = true                                   {pd product} {default}
     intx ThreadPriorityPolicy                     = 0                                         {product} {default}
     bool ThreadPriorityVerbose                    = false                                     {product} {default}
     intx ThreadStackSize                          = 1024                                   {pd product} {default}
    uintx ThresholdTolerance                       = 10                                        {product} {default}
     intx Tier0BackedgeNotifyFreqLog               = 10                                        {product} {default}
     intx Tier0InvokeNotifyFreqLog                 = 7                                         {product} {default}
     intx Tier0ProfilingStartPercentage            = 200                                       {product} {default}
     intx Tier23InlineeNotifyFreqLog               = 20                                        {product} {default}
     intx Tier2BackEdgeThreshold                   = 0                                         {product} {default}
     intx Tier2BackedgeNotifyFreqLog               = 14                                        {product} {default}
     intx Tier2CompileThreshold                    = 0                                         {product} {default}
     intx Tier2InvokeNotifyFreqLog                 = 11                                        {product} {default}
     intx Tier3AOTBackEdgeThreshold                = 120000                                    {product} {default}
     intx Tier3AOTCompileThreshold                 = 15000                                     {product} {default}
     intx Tier3AOTInvocationThreshold              = 10000                                     {product} {default}
     intx Tier3AOTMinInvocationThreshold           = 1000                                      {product} {default}
     intx Tier3BackEdgeThreshold                   = 60000                                     {product} {default}
     intx Tier3BackedgeNotifyFreqLog               = 13                                        {product} {default}
     intx Tier3CompileThreshold                    = 2000                                      {product} {default}
     intx Tier3DelayOff                            = 2                                         {product} {default}
     intx Tier3DelayOn                             = 5                                         {product} {default}
     intx Tier3InvocationThreshold                 = 200                                       {product} {default}
     intx Tier3InvokeNotifyFreqLog                 = 10                                        {product} {default}
     intx Tier3LoadFeedback                        = 5                                         {product} {default}
     intx Tier3MinInvocationThreshold              = 100                                       {product} {default}
     intx Tier4BackEdgeThreshold                   = 40000                                     {product} {default}
     intx Tier4CompileThreshold                    = 15000                                     {product} {default}
     intx Tier4InvocationThreshold                 = 5000                                      {product} {default}
     intx Tier4LoadFeedback                        = 3                                         {product} {default}
     intx Tier4MinInvocationThreshold              = 600                                       {product} {default}
     bool TieredCompilation                        = true                                   {pd product} {default}
     intx TieredCompileTaskTimeout                 = 50                                        {product} {default}
     intx TieredRateUpdateMaxTime                  = 25                                        {product} {default}
     intx TieredRateUpdateMinTime                  = 1                                         {product} {default}
     intx TieredStopAtLevel                        = 4                                         {product} {default}
     bool TimeLinearScan                           = false                                  {C1 product} {default}
     bool TraceCompilerThreads                     = false                                  {diagnostic} {default}
    ccstr TraceJVMTI                               =                                           {product} {default}
     bool TraceJVMTIObjectTagging                  = false                                  {diagnostic} {default}
     bool TraceNMethodInstalls                     = false                                  {diagnostic} {default}
     bool TraceSpilling                            = false                               {C2 diagnostic} {default}
     bool TraceSuspendWaitFailures                 = false                                     {product} {default}
     bool TraceTypeProfile                         = false                               {C2 diagnostic} {default}
     intx TrackedInitializationLimit               = 50                                     {C2 product} {default}
     bool TransmitErrorReport                      = false                                     {product} {default}
     bool TrapBasedNullChecks                      = false                                  {pd product} {default}
     bool TrapBasedRangeChecks                     = false                               {C2 pd product} {default}
     intx TypeProfileArgsLimit                     = 2                                         {product} {default}
    uintx TypeProfileLevel                         = 111                                    {pd product} {default}
     intx TypeProfileMajorReceiverPercent          = 90                                     {C2 product} {default}
     intx TypeProfileParmsLimit                    = 2                                         {product} {default}
     intx TypeProfileWidth                         = 2                                         {product} {default}
     intx UnguardOnExecutionViolation              = 0                                         {product} {default}
     bool UnlinkSymbolsALot                        = false                                     {product} {default}
     bool UnlockDiagnosticVMOptions                = true                                   {diagnostic} {command line}
     bool UseAES                                   = true                                      {product} {default}
     bool UseAESCTRIntrinsics                      = true                                   {diagnostic} {default}
     bool UseAESIntrinsics                         = true                                   {diagnostic} {default}
     bool UseAOTStrictLoading                      = false                                  {diagnostic} {default}
     intx UseAVX                                   = 3                                    {ARCH product} {default}
     bool UseAdaptiveGCBoundary                    = false                                     {product} {default}
     bool UseAdaptiveGenerationSizePolicyAtMajorCollection  = true                             {product} {default}
     bool UseAdaptiveGenerationSizePolicyAtMinorCollection  = true                             {product} {default}
     bool UseAdaptiveNUMAChunkSizing               = true                                      {product} {default}
     bool UseAdaptiveSizeDecayMajorGCCost          = true                                      {product} {default}
     bool UseAdaptiveSizePolicy                    = true                                      {product} {default}
     bool UseAdaptiveSizePolicyFootprintGoal       = true                                      {product} {default}
     bool UseAdaptiveSizePolicyWithSystemGC        = false                                     {product} {default}
     bool UseAddressNop                            = true                                 {ARCH product} {default}
     bool UseAdler32Intrinsics                     = false                                  {diagnostic} {default}
     bool UseBASE64Intrinsics                      = true                                      {product} {default}
     bool UseBMI1Instructions                      = true                                 {ARCH product} {default}
     bool UseBMI2Instructions                      = true                                 {ARCH product} {default}
     bool UseBiasedLocking                         = true                                      {product} {default}
     bool UseBimorphicInlining                     = true                                   {C2 product} {default}
      int UseBootstrapCallInfo                     = 1                                      {diagnostic} {default}
     bool UseCLMUL                                 = true                                 {ARCH product} {default}
     bool UseCMSBestFit                            = true                                      {product} {default}
     bool UseCMSInitiatingOccupancyOnly            = false                                     {product} {default}
     bool UseCMoveUnconditionally                  = false                                  {C2 product} {default}
     bool UseCRC32CIntrinsics                      = true                                   {diagnostic} {default}
     bool UseCRC32Intrinsics                       = true                                   {diagnostic} {default}
     bool UseCharacterCompareIntrinsics            = false                               {C2 diagnostic} {default}
     bool UseCodeAging                             = true                                      {product} {default}
     bool UseCodeCacheFlushing                     = true                                      {product} {default}
     bool UseCompiler                              = true                                      {product} {default}
     bool UseCompressedClassPointers               = true                                 {lp64_product} {ergonomic}
     bool UseCompressedOops                        = true                                 {lp64_product} {ergonomic}
     bool UseConcMarkSweepGC                       = false                                     {product} {default}
     bool UseCondCardMark                          = false                                     {product} {default}
     bool UseContainerSupport                      = true                                      {product} {default}
     bool UseCopySignIntrinsic                     = false                                  {diagnostic} {default}
     bool UseCountLeadingZerosInstruction          = true                                 {ARCH product} {default}
     bool UseCountTrailingZerosInstruction         = true                                 {ARCH product} {default}
     bool UseCountedLoopSafepoints                 = true                                   {C2 product} {default}
     bool UseCounterDecay                          = true                                      {product} {default}
     bool UseCpuAllocPath                          = false                                  {diagnostic} {default}
     bool UseDivMod                                = true                                   {C2 product} {default}
     bool UseDynamicNumberOfCompilerThreads        = true                                      {product} {default}
     bool UseDynamicNumberOfGCThreads              = true                                      {product} {default}
     bool UseFMA                                   = true                                      {product} {default}
     bool UseFPUForSpilling                        = true                                   {C2 product} {default}
     bool UseFastJNIAccessors                      = true                                      {product} {default}
     bool UseFastStosb                             = true                                 {ARCH product} {default}
     bool UseG1GC                                  = true                                      {product} {command line}
     bool UseGCOverheadLimit                       = true                                      {product} {default}
     bool UseGCTaskAffinity                        = false                                     {product} {default}
     bool UseGHASHIntrinsics                       = true                                   {diagnostic} {default}
     bool UseHeavyMonitors                         = false                                     {product} {default}
     bool UseHugeTLBFS                             = false                                     {product} {default}
     bool UseImplicitStableValues                  = true                                {C2 diagnostic} {default}
     bool UseIncDec                                = true                              {ARCH diagnostic} {default}
     bool UseInlineCaches                          = true                                      {product} {default}
     bool UseInlineDepthForSpeculativeTypes        = true                                {C2 diagnostic} {default}
     bool UseInterpreter                           = true                                      {product} {default}
     bool UseJumpTables                            = true                                   {C2 product} {default}
     bool UseLWPSynchronization                    = true                                      {product} {default}
     bool UseLargePages                            = false                                  {pd product} {default}
     bool UseLargePagesInMetaspace                 = false                                     {product} {default}
     bool UseLargePagesIndividualAllocation        = false                                  {pd product} {default}
     bool UseLegacyJNINameEscaping                 = false                                     {product} {default}
     bool UseLibmIntrinsic                         = true                              {ARCH diagnostic} {default}
     bool UseLinuxPosixThreadCPUClocks             = true                                      {product} {default}
     bool UseLoopCounter                           = true                                      {product} {default}
     bool UseLoopInvariantCodeMotion               = true                                   {C1 product} {default}
     bool UseLoopPredicate                         = true                                   {C2 product} {default}
     bool UseMathExactIntrinsics                   = true                                {C2 diagnostic} {default}
     bool UseMaximumCompactionOnSystemGC           = true                                      {product} {default}
     bool UseMembar                                = true                                   {pd product} {default}
     bool UseMontgomeryMultiplyIntrinsic           = true                                {C2 diagnostic} {default}
     bool UseMontgomerySquareIntrinsic             = true                                {C2 diagnostic} {default}
     bool UseMulAddIntrinsic                       = true                                {C2 diagnostic} {default}
     bool UseMultiplyToLenIntrinsic                = true                                {C2 diagnostic} {default}
     bool UseNUMA                                  = false                                     {product} {default}
     bool UseNUMAInterleaving                      = false                                     {product} {default}
     bool UseNewCode                               = false                                  {diagnostic} {default}
     bool UseNewCode2                              = false                                  {diagnostic} {default}
     bool UseNewCode3                              = false                                  {diagnostic} {default}
     bool UseNewLongLShift                         = false                                {ARCH product} {default}
     bool UseOSErrorReporting                      = false                                  {pd product} {default}
     bool UseOnStackReplacement                    = true                                   {pd product} {default}
     bool UseOnlyInlinedBimorphic                  = true                                   {C2 product} {default}
     bool UseOprofile                              = false                                     {product} {default}
     bool UseOptoBiasInlining                      = true                                   {C2 product} {default}
     bool UsePSAdaptiveSurvivorSizePolicy          = true                                      {product} {default}
     bool UseParallelGC                            = false                                     {product} {default}
     bool UseParallelOldGC                         = false                                     {product} {default}
     bool UsePerfData                              = true                                      {product} {default}
     bool UsePopCountInstruction                   = true                                      {product} {default}
     bool UseProfiledLoopPredicate                 = true                                   {C2 product} {default}
     bool UseRDPCForConstantTableBase              = false                                  {C2 product} {default}
     bool UseRTMDeopt                              = false                                {ARCH product} {default}
     bool UseRTMLocking                            = false                                {ARCH product} {default}
     bool UseSHA                                   = true                                      {product} {default}
     bool UseSHA1Intrinsics                        = false                                  {diagnostic} {default}
     bool UseSHA256Intrinsics                      = true                                   {diagnostic} {default}
     bool UseSHA512Intrinsics                      = true                                   {diagnostic} {default}
     bool UseSHM                                   = false                                     {product} {default}
     intx UseSSE                                   = 4                                         {product} {default}
     bool UseSSE42Intrinsics                       = true                                 {ARCH product} {default}
     bool UseSemaphoreGCThreadsSynchronization     = true                                   {diagnostic} {default}
     bool UseSerialGC                              = false                                     {product} {default}
     bool UseSharedSpaces                          = false                                     {product} {default}
     bool UseShenandoahGC                          = false                                     {product} {default}
     bool UseSignalChaining                        = true                                      {product} {default}
     bool UseSignumIntrinsic                       = false                                  {diagnostic} {default}
     bool UseSquareToLenIntrinsic                  = true                                {C2 diagnostic} {default}
     bool UseStoreImmI16                           = false                                {ARCH product} {default}
     bool UseStringDeduplication                   = false                                     {product} {default}
     bool UseSubwordForMaxVector                   = true                                   {C2 product} {default}
     bool UseSuperWord                             = true                                   {C2 product} {default}
     bool UseSwitchProfiling                       = true                                   {diagnostic} {default}
     bool UseTLAB                                  = true                                   {pd product} {default}
     bool UseThreadPriorities                      = true                                   {pd product} {default}
     bool UseTransparentHugePages                  = false                                     {product} {default}
     bool UseTypeProfile                           = true                                      {product} {default}
     bool UseTypeSpeculation                       = true                                   {C2 product} {default}
     bool UseUnalignedAccesses                     = true                                   {diagnostic} {default}
     bool UseUnalignedLoadStores                   = true                                 {ARCH product} {default}
     bool UseVectorCmov                            = false                                  {C2 product} {default}
     bool UseVectorizedMismatchIntrinsic           = true                                   {diagnostic} {default}
     bool UseXMMForArrayCopy                       = true                                      {product} {default}
     bool UseXMMForObjInit                         = false                                {ARCH product} {default}
     bool UseXmmI2D                                = false                                {ARCH product} {default}
     bool UseXmmI2F                                = false                                {ARCH product} {default}
     bool UseXmmLoadAndClearUpper                  = true                                 {ARCH product} {default}
     bool UseXmmRegToRegMoveAll                    = true                                 {ARCH product} {default}
     bool VMThreadHintNoPreempt                    = false                                     {product} {default}
     intx VMThreadPriority                         = -1                                        {product} {default}
     intx VMThreadStackSize                        = 1024                                   {pd product} {default}
     intx ValueMapInitialSize                      = 11                                     {C1 product} {default}
     intx ValueMapMaxLoopSize                      = 8                                      {C1 product} {default}
     intx ValueSearchLimit                         = 1000                                   {C2 product} {default}
     bool VerifyAdapterCalls                       = false                                  {diagnostic} {default}
     bool VerifyAfterGC                            = false                                  {diagnostic} {default}
     bool VerifyBeforeExit                         = false                                  {diagnostic} {default}
     bool VerifyBeforeGC                           = false                                  {diagnostic} {default}
     bool VerifyBeforeIteration                    = false                                  {diagnostic} {default}
     bool VerifyDuringGC                           = false                                  {diagnostic} {default}
     bool VerifyDuringStartup                      = false                                  {diagnostic} {default}
     intx VerifyGCLevel                            = 0                                      {diagnostic} {default}
    uintx VerifyGCStartAt                          = 0                                      {diagnostic} {default}
ccstrlist VerifyGCType                             =                                        {diagnostic} {default}
     bool VerifyMergedCPBytecodes                  = true                                      {product} {default}
     bool VerifyMethodHandles                      = false                                  {diagnostic} {default}
     bool VerifyObjectStartArray                   = true                                   {diagnostic} {default}
     bool VerifyRememberedSets                     = false                                  {diagnostic} {default}
     bool VerifySharedSpaces                       = false                                     {product} {default}
     bool VerifyStringTableAtExit                  = false                                  {diagnostic} {default}
ccstrlist VerifySubSet                             =                                        {diagnostic} {default}
     bool WhiteBoxAPI                              = false                                  {diagnostic} {default}
    uintx YoungGenerationSizeIncrement             = 20                                        {product} {default}
    uintx YoungGenerationSizeSupplement            = 80                                        {product} {default}
    uintx YoungGenerationSizeSupplementDecay       = 8                                         {product} {default}
   size_t YoungPLABSize                            = 4096                                      {product} {default}
   double ZAllocationSpikeTolerance                = 2.000000                                  {product} {default}
     uint ZCollectionInterval                      = 0                                         {product} {default}
     bool ZConcurrentJNIWeakGlobalHandles          = true                                   {diagnostic} {default}
     bool ZConcurrentStringTable                   = true                                   {diagnostic} {default}
     bool ZConcurrentVMWeakHandles                 = true                                   {diagnostic} {default}
   double ZFragmentationLimit                      = 25.000000                                 {product} {default}
   size_t ZMarkStacksMax                           = 8589934592                                {product} {default}
     bool ZOptimizeLoadBarriers                    = true                                   {diagnostic} {default}
    ccstr ZPath                                    =                                           {product} {default}
     bool ZProactive                               = true                                   {diagnostic} {default}
     bool ZStallOnOutOfMemory                      = true                                      {product} {default}
     bool ZStatisticsForceTrace                    = false                                  {diagnostic} {default}
     uint ZStatisticsInterval                      = 10                                        {product} {default}
     bool ZSymbolTableUnloading                    = false                                  {diagnostic} {default}
     bool ZUnmapBadViews                           = false                                  {diagnostic} {default}
     bool ZVerifyForwarding                        = false                                  {diagnostic} {default}
     bool ZVerifyMarking                           = false                                  {diagnostic} {default}
     bool ZWeakRoots                               = true                                   {diagnostic} {default}
     bool ZeroTLAB                                 = false                                     {product} {default}
log4j:WARN No such property [fields] in org.apache.log4j.EnhancedPatternLayout.
[INFO] 2023-11-08 14:50:52,503 [main] io.confluent.agent.monitoring.DiskUsage premain - DiskUsage Agent: config : /opt/confluentinc/etc/kafka/disk-usage-agent.properties
[INFO] 2023-11-08 14:50:52,627 [main] io.confluent.agent.monitoring.DiskUsage premain - DiskUsage Agent: Registering object :io.confluent.caas:type=VolumeMetrics, service=kafka, dir=data for class : io.confluent.agent.monitoring.Volume
[INFO] 2023-11-08 14:50:52,630 [main] io.confluent.agent.monitoring.DiskUsage premain - DiskUsage Agent: Ping Volume{store=/mnt/data/data0 (/dev/sdc), total=10464022528, used=24576, available=10447220736, percentUsed=2.3486187968573916E-4, percentAvailable=99.83943276158818, mountpoint='/mnt/data/data0', deviceName='/dev/sdc'}
I> No access restrictor found, access to any MBean is allowed
Jolokia: Agent started with URL http://10.40.0.15:7777/jolokia/
[INFO] 2023-11-08 14:50:53,017 [main] kafka.utils.Log4jControllerRegistration$ <clinit> - Registered kafka:type=kafka.Log4jController MBean
[INFO] 2023-11-08 14:50:53,926 [main] org.apache.zookeeper.common.X509Util <clinit> - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[INFO] 2023-11-08 14:50:54,055 [main] org.apache.kafka.common.utils.LoggingSignalHandler register - Registered signal handlers for TERM, INT, HUP
[INFO] 2023-11-08 14:50:54,059 [main] kafka.server.KafkaServer info - starting
[INFO] 2023-11-08 14:50:54,059 [main] kafka.server.KafkaServer info - FIPS mode enabled: false
[INFO] 2023-11-08 14:50:54,060 [main] kafka.server.KafkaServer info - Connecting to zookeeper on zookeeper.confluent.svc.cluster.local:2182/kafka-confluent
[INFO] 2023-11-08 14:50:54,077 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Initializing a new session to zookeeper.confluent.svc.cluster.local:2182.
[INFO] 2023-11-08 14:50:54,083 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
[INFO] 2023-11-08 14:50:54,083 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:host.name=kafka-2.kafka.confluent.svc.cluster.local
[INFO] 2023-11-08 14:50:54,083 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.version=11.0.14.1
[INFO] 2023-11-08 14:50:54,083 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.vendor=Azul Systems, Inc.
[INFO] 2023-11-08 14:50:54,083 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.home=/usr/lib/jvm/zulu11-ca
[INFO] 2023-11-08 14:50:54,083 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.class.path=/usr/bin/../ce-broker-plugins/build/libs/*:/usr/bin/../ce-broker-plugins/build/dependant-libs/*:/usr/bin/../ce-auth-providers/build/libs/*:/usr/bin/../ce-auth-providers/build/dependant-libs/*:/usr/bin/../ce-rest-server/build/libs/*:/usr/bin/../ce-rest-server/build/dependant-libs/*:/usr/bin/../ce-audit/build/libs/*:/usr/bin/../ce-audit/build/dependant-libs/*:/usr/bin/../share/java/kafka/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/broker-plugins-7.1.0-ce.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/gax-httpjson-0.89.1.jar:/usr/bin/../share/java/kafka/aws-java-sdk-core-1.11.988.jar:/usr/bin/../share/java/kafka/auto-service-annotations-1.0-rc7.jar:/usr/bin/../share/java/kafka/api-common-2.0.2.jar:/usr/bin/../share/java/kafka/zipkin-2.23.2.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/kafka/netty-codec-haproxy-4.1.73.Final.jar:/usr/bin/../share/java/kafka/logredactor-1.0.10.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-ce-logs-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/netty-codec-xml-4.1.73.Final.jar:/usr/bin/../share/java/kafka/auto-value-annotations-1.8.2.jar:/usr/bin/../share/java/kafka/annotations-13.0.jar:/usr/bin/../share/java/kafka/core-1.54.0.0.jar:/usr/bin/../share/java/kafka/gax-2.4.1.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/oauth2-oidc-sdk-9.7.jar:/usr/bin/../share/java/kafka/jackson-dataformat-xml-2.12.3.jar:/usr/bin/../share/java/kafka/google-oauth-client-1.32.1.jar:/usr/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/kafka/reactor-netty-http-1.0.7.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.1.0-ce.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/proto-google-iam-v1-1.1.0.jar:/usr/bin/../share/java/kafka/netty-handler-proxy-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final-linux-aarch_64.jar:/usr/bin/../share/java/kafka/netty-codec-http2-4.1.73.Final.jar:/usr/bin/../share/java/kafka/bc-fips-1.0.2.jar:/usr/bin/../share/java/kafka/stax2-api-4.2.1.jar:/usr/bin/../share/java/kafka/reactor-netty-core-1.0.7.jar:/usr/bin/../share/java/kafka/httpclient-4.5.13.jar:/usr/bin/../share/java/kafka/kafka-streams-7.1.0-ce.jar:/usr/bin/../share/java/kafka/brave-instrumentation-http-5.13.3.jar:/usr/bin/../share/java/kafka/ion-java-1.0.2.jar:/usr/bin/../share/java/kafka/netty-codec-stomp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/kafka-raft-7.1.0-ce.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-transport-sctp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/joda-time-2.9.9.jar:/usr/bin/../share/java/kafka/okio-jvm-3.0.0.jar:/usr/bin/../share/java/kafka/confluent-licensing-new-7.1.0-ce.jar:/usr/bin/../share/java/kafka/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/checker-qual-3.8.0.jar:/usr/bin/../share/java/kafka/azure-core-http-netty-1.10.1.jar:/usr/bin/../share/java/kafka/netty-codec-dns-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-aarch_64.jar:/usr/bin/../share/java/kafka/client-java-14.0.0.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.21.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jbcrypt-0.4.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.1.0-ce.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/kafka/annotations-3.0.1.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/kafka/kafka-shell-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jcip-annotations-1.0.jar:/usr/bin/../share/java/kafka/httpcore-4.4.14.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jna-platform-5.6.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/kafka/internal-rest-server-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/bin/../share/java/kafka/simpleclient_common-0.12.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final.jar:/usr/bin/../share/java/kafka/confluent-audit-7.1.0-ce.jar:/usr/bin/../share/java/kafka/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/netty-tcnative-classes-2.0.46.Final.jar:/usr/bin/../share/java/kafka/database-2.1.4.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/kafka/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/opencensus-contrib-http-util-0.28.0.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-classes-macos-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-json-7.1.0-ce.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/auth-providers-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/bin/../share/java/kafka/nimbus-jose-jwt-9.9.3.jar:/usr/bin/../share/java/kafka/azure-core-1.18.0.jar:/usr/bin/../share/java/kafka/aws-java-sdk-sts-1.11.988.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/google-auth-library-credentials-1.1.0.jar:/usr/bin/../share/java/kafka/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/kafka/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/kafka/msal4j-1.10.1.jar:/usr/bin/../share/java/kafka/rbac-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.73.Final.jar:/usr/bin/../share/java/kafka/javax.json-1.0.4.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/zipkin-reporter-2.16.3.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-transforms-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jose4j-0.7.8.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jmespath-java-1.11.988.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-common-1.5.31.jar:/usr/bin/../share/java/kafka/google-cloud-core-http-2.1.3.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jsr305-3.0.1.jar:/usr/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/netty-codec-redis-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-runtime-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/annotations-15.0.jar:/usr/bin/../share/java/kafka/grpc-context-1.40.1.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp10.jar:/usr/bin/../share/java/kafka/zipkin-reporter-brave-2.16.3.jar:/usr/bin/../share/java/kafka/simpleclient-0.12.0.jar:/usr/bin/../share/java/kafka/kafka-tools-7.1.0-ce.jar:/usr/bin/../share/java/kafka/ce-sbk_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/aws-java-sdk-s3-1.11.988.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/woodstox-core-6.2.4.jar:/usr/bin/../share/java/kafka/netty-codec-memcache-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_otel-0.12.0.jar:/usr/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.46.Final.jar:/usr/bin/../share/java/kafka/proto-google-common-protos-2.5.0.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/google-cloud-storage-2.1.2.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/brave-5.13.3.jar:/usr/bin/../share/java/kafka/azure-storage-internal-avro-12.0.5.jar:/usr/bin/../share/java/kafka/google-auth-library-oauth2-http-1.1.0.jar:/usr/bin/../share/java/kafka/netty-transport-udt-4.1.73.Final.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.1.0-ce.jar:/usr/bin/../share/java/kafka/google-api-client-1.32.1.jar:/usr/bin/../share/java/kafka/auto-common-0.10.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/reactor-core-3.4.6.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/bin/../share/java/kafka/telemetry-client-1.745.0.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/KeePassJava2-2.1.4.jar:/usr/bin/../share/java/kafka/google-cloud-core-2.1.3.jar:/usr/bin/../share/java/kafka/confluent-serializers-new-7.1.0-ce.jar:/usr/bin/../share/java/kafka/auto-service-1.0-rc7.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.32.jar:/usr/bin/../share/java/kafka/netty-codec-mqtt-4.1.73.Final.jar:/usr/bin/../share/java/kafka/okhttp-4.9.1.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-1.5.31.jar:/usr/bin/../share/java/kafka/threetenbp-1.5.1.jar:/usr/bin/../share/java/kafka/minimal-json-0.9.5.jar:/usr/bin/../share/java/kafka/checker-qual-3.5.0.jar:/usr/bin/../share/java/kafka/netty-all-4.1.73.Final.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_otel_agent-0.12.0.jar:/usr/bin/../share/java/kafka/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jackson-dataformat-cbor-2.12.3.jar:/usr/bin/../share/java/kafka/KeePassJava2-simple-2.1.4.jar:/usr/bin/../share/java/kafka/telemetry-api-1.745.0.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/google-http-client-apache-v2-1.40.0.jar:/usr/bin/../share/java/kafka/tink-1.6.0.jar:/usr/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/usr/bin/../share/java/kafka/bctls-fips-1.0.10.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/bin/../share/java/kafka/client-java-api-14.0.0.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/commons-lang3-3.11.jar:/usr/bin/../share/java/kafka/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/kafka/google-http-client-1.40.0.jar:/usr/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/usr/bin/../share/java/kafka/icu4j-61.1.jar:/usr/bin/../share/java/kafka/guava-30.0-jre.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/netty-handler-proxy-4.1.65.Final.jar:/usr/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/kafka/azure-identity-1.3.3.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/snakeyaml-1.29.jar:/usr/bin/../share/java/kafka/commons-compress-1.21.jar:/usr/bin/../share/java/kafka/azure-storage-blob-12.12.0.jar:/usr/bin/../share/java/kafka/commons-collections4-4.4.jar:/usr/bin/../share/java/kafka/azure-storage-common-12.12.0.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/google-http-client-appengine-1.40.0.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/kafka/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/kafka/netty-transport-rxtx-4.1.73.Final.jar:/usr/bin/../share/java/kafka/google-api-services-cloudkms-v1-rev108-1.25.0.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/httpcore-4.4.13.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/kafka/gson-fire-1.8.5.jar:/usr/bin/../share/java/kafka/reactive-streams-1.0.3.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.1.0-ce.jar:/usr/bin/../share/java/kafka/reactor-netty-http-brave-1.0.7.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/kafka/reactor-netty-1.0.7.jar:/usr/bin/../share/java/kafka/jcip-annotations-1.0-1.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/netty-codec-smtp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/simpleclient_httpserver-0.12.0.jar:/usr/bin/../share/java/kafka/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/kafka/ST4-4.3.jar:/usr/bin/../share/java/kafka/kafka-storage-7.1.0-ce.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.22.1.1.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/commons-codec-1.15.jar:/usr/bin/../share/java/kafka/KeePassJava2-dom-2.1.4.jar:/usr/bin/../share/java/kafka/netty-codec-socks-4.1.73.Final.jar:/usr/bin/../share/java/kafka/client-java-proto-14.0.0.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jna-5.6.0.jar:/usr/bin/../share/java/kafka/google-api-services-storage-v1-rev20210127-1.32.1.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/KeePassJava2-jaxb-2.1.4.jar:/usr/bin/../share/java/kafka/KeePassJava2-kdb-2.1.4.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_common-0.12.0.jar:/usr/bin/../share/java/kafka/logging-interceptor-4.9.1.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/kafka/aalto-xml-1.0.0.jar:/usr/bin/../share/java/kafka/KeePassJava2-kdbx-2.1.4.jar:/usr/bin/../share/java/kafka/antlr4-4.9.2.jar:/usr/bin/../share/java/kafka/connect-mirror-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jsr305-3.0.2.jar:/usr/bin/../share/java/kafka/google-http-client-jackson2-1.40.0.jar:/usr/bin/../share/java/kafka/lang-tag-1.5.jar:/usr/bin/../share/java/kafka/connector-datapreview-extension-7.1.0-ce.jar:/usr/bin/../share/java/kafka/opencensus-api-0.28.0.jar:/usr/bin/../share/java/kafka/connect-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/google-http-client-gson-1.40.0.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/tink-gcpkms-1.6.0.jar:/usr/bin/../share/java/kafka/swagger-annotations-1.6.3.jar:/usr/bin/../share/java/kafka/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/kafka/trogdor-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/aws-java-sdk-kms-1.11.988.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/logredactor-metrics-1.0.10.jar:/usr/bin/../share/java/kafka/re2j-1.6.jar:/usr/bin/../share/java/kafka/protobuf-java-3.17.3.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/commons-math3-3.6.1.jar:/usr/bin/../share/java/kafka/asm-9.1.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.1.0-ce.jar:/usr/bin/../share/java/kafka/failureaccess-1.0.1.jar:/usr/bin/../share/java/kafka/accessors-smart-2.4.7.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.1.0-ce.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/content-type-2.1.jar:/usr/bin/../share/java/kafka/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-aarch_64.jar:/usr/bin/../share/java/kafka/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/commons-io-2.11.0.jar:/usr/bin/../share/java/kafka/gson-2.8.6.jar:/usr/bin/../share/java/kafka/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/kafka/jackson-dataformat-properties-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/snakeyaml-1.27.jar:/usr/bin/../share/java/kafka/json-smart-2.4.7.jar:/usr/bin/../share/java/kafka/error_prone_annotations-2.3.4.jar:/usr/bin/../share/java/kafka/msal4j-persistence-extension-1.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/confluent-metadata-service/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-server-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/auto-value-annotations-1.8.1.jar:/usr/bin/../share/java/confluent-metadata-service/netty-handler-proxy-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jcommander-1.72.jar:/usr/bin/../share/java/confluent-metadata-service/jul-to-slf4j-1.7.30.jar:/usr/bin/../share/java/confluent-metadata-service/authorizer-client-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/usr/bin/../share/java/confluent-metadata-service/annotations-3.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/ce-kafka-http-server-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-audit-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-metadata-service/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/confluent-metadata-service/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/usr/bin/../share/java/confluent-metadata-service/auth-providers-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/javax.json-1.0.4.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/bsh-2.0b6.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-metadata-service/javax.servlet-api-4.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-server-common-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-client-1.745.0.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-api-1.745.0.jar:/usr/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.10.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-api-server-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/confluent-metadata-service/icu4j-61.1.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/ST4-4.3.jar:/usr/bin/../share/java/confluent-metadata-service/jose4j-0.7.2.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-common-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/netty-codec-socks-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/antlr4-4.9.2.jar:/usr/bin/../share/java/confluent-metadata-service/bc-fips-1.0.2.1.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-common-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-client-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/confluent-metadata-service/gson-2.8.6.jar:/usr/bin/../share/java/confluent-metadata-service/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-dataformat-properties-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/snakeyaml-1.27.jar:/usr/bin/../share/java/confluent-metadata-service/testng-6.14.3.jar:/usr/bin/../share/java/rest-utils/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-xml-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/rest-utils/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/jersey-server-2.34.jar:/usr/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/rest-utils/guava-30.1.1-jre.jar:/usr/bin/../share/java/rest-utils/jetty-plus-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-webapp-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/http2-common-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/classmate-1.3.4.jar:/usr/bin/../share/java/rest-utils/http2-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/websocket-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/rest-utils/checker-qual-3.8.0.jar:/usr/bin/../share/java/rest-utils/asm-9.2.jar:/usr/bin/../share/java/rest-utils/jetty-jaas-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/asm-analysis-9.2.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/rest-utils/jetty-alpn-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-jmx-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/usr/bin/../share/java/rest-utils/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/lz4-java-1.8.0.jar:/usr/bin/../share/java/rest-utils/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/rest-utils/activation-1.1.1.jar:/usr/bin/../share/java/rest-utils/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/rest-utils/javassist-3.25.0-GA.jar:/usr/bin/../share/java/rest-utils/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/rest-utils/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/rest-utils/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/rest-utils/websocket-api-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/hk2-api-2.6.1.jar:/usr/bin/../share/java/rest-utils/hibernate-validator-6.1.7.Final.jar:/usr/bin/../share/java/rest-utils/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/rest-utils/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/asm-tree-9.2.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/rest-utils/websocket-common-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/asm-commons-9.2.jar:/usr/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/rest-utils/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/rest-utils/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/http2-hpack-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/rest-utils/jetty-annotations-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/rest-utils/j2objc-annotations-1.3.jar:/usr/bin/../share/java/rest-utils/jakarta.el-3.0.3.jar:/usr/bin/../share/java/rest-utils/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/rest-utils/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jackson-databind-2.12.3.jar:/usr/bin/../share/java/rest-utils/websocket-client-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/jetty-jndi-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/hk2-locator-2.6.1.jar:/usr/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/rest-utils/hk2-utils-2.6.1.jar:/usr/bin/../share/java/rest-utils/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/jackson-core-2.12.3.jar:/usr/bin/../share/java/rest-utils/jersey-hk2-2.34.jar:/usr/bin/../share/java/rest-utils/websocket-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/kafka-clients-7.1.0-ccs.jar:/usr/bin/../share/java/rest-utils/jsr305-3.0.2.jar:/usr/bin/../share/java/rest-utils/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jersey-common-2.34.jar:/usr/bin/../share/java/rest-utils/rest-utils-7.1.0.jar:/usr/bin/../share/java/rest-utils/jersey-client-2.34.jar:/usr/bin/../share/java/rest-utils/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/failureaccess-1.0.1.jar:/usr/bin/../share/java/rest-utils/jetty-alpn-java-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/usr/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-common/build-tools-7.1.0.jar:/usr/bin/../share/java/confluent-common/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-common/slf4j-api-1.7.30.jar:/usr/bin/../share/java/confluent-common/common-metrics-7.1.0.jar:/usr/bin/../share/java/confluent-common/common-config-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/ce-kafka-http-server/javax-websocket-server-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-xml-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-server-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/guava-30.1.1-jre.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-plus-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/ce-kafka-http-server/javax-websocket-client-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-webapp-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-common-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/classmate-1.3.4.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/checker-qual-3.8.0.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jaas-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-analysis-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-alpn-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jmx-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/ce-kafka-http-server-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jaxb-api-2.3.0.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/lz4-java-1.8.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/ce-kafka-http-server/activation-1.1.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/ce-kafka-http-server/javassist-3.25.0-GA.jar:/usr/bin/../share/java/ce-kafka-http-server/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/common-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-api-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-api-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/hibernate-validator-6.1.7.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-tree-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-common-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-commons-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.websocket-api-1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-hpack-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-annotations-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/ce-kafka-http-server/j2objc-annotations-1.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.el-3.0.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-databind-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-client-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jndi-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-locator-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-utils-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-core-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-hk2-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jsr305-3.0.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-common-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/rest-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-client-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/failureaccess-1.0.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-alpn-java-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.websocket-client-api-1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/ce-kafka-rest-servlet/ce-kafka-rest-servlet-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/lz4-java-1.8.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/common-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/ce-kafka-rest-extensions-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka-rest-lib/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/logredactor-1.0.10.jar:/usr/bin/../share/java/kafka-rest-lib/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka-rest-lib/annotations-13.0.jar:/usr/bin/../share/java/kafka-rest-lib/guava-30.1.1-jre.jar:/usr/bin/../share/java/kafka-rest-lib/commons-logging-1.2.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-common-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-metadata-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-provider-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/okio-jvm-3.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/wire-schema-jvm-4.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/checker-qual-3.8.0.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-module-parameter-names-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/resilience4j-core-1.7.1.jar:/usr/bin/../share/java/kafka-rest-lib/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka-rest-lib/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-schema-provider-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-guava-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/common-utils-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/wire-runtime-jvm-4.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/org.everit.json.schema-1.12.2.jar:/usr/bin/../share/java/kafka-rest-lib/handy-uri-templates-2.1.8.jar:/usr/bin/../share/java/kafka-rest-lib/kotlinx-coroutines-core-1.3.7.jar:/usr/bin/../share/java/kafka-rest-lib/jose4j-0.7.8.jar:/usr/bin/../share/java/kafka-rest-lib/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka-rest-lib/proto-google-common-protos-2.5.1.jar:/usr/bin/../share/java/kafka-rest-lib/scala-reflect-2.13.5.jar:/usr/bin/../share/java/kafka-rest-lib/kafka_2.13-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/confluent-log4j-1.2.17-cp10.jar:/usr/bin/../share/java/kafka-rest-lib/resilience4j-ratelimiter-1.7.1.jar:/usr/bin/../share/java/kafka-rest-lib/commons-validator-1.6.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-joda-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/commons-cli-1.4.jar:/usr/bin/../share/java/kafka-rest-lib/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-raft-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka-rest-lib/spotbugs-annotations-4.3.0.jar:/usr/bin/../share/java/kafka-rest-lib/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/kafka-rest-lib/minimal-json-0.9.5.jar:/usr/bin/../share/java/kafka-rest-lib/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka-rest-lib/scala-library-2.13.5.jar:/usr/bin/../share/java/kafka-rest-lib/paranamer-2.8.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-server-common-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-schema-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/commons-collections-3.2.2.jar:/usr/bin/../share/java/kafka-rest-lib/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka-rest-lib/j2objc-annotations-1.3.jar:/usr/bin/../share/java/kafka-rest-lib/joda-time-2.10.8.jar:/usr/bin/../share/java/kafka-rest-lib/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/bin/../share/java/kafka-rest-lib/vavr-match-0.10.2.jar:/usr/bin/../share/java/kafka-rest-lib/commons-compress-1.21.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-script-runtime-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-avro-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka-rest-lib/common-config-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka-rest-lib/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/kafka-rest-lib/commons-digester-1.8.1.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-storage-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/vavr-0.10.2.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-storage-api-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-types-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-clients-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/jsr305-3.0.2.jar:/usr/bin/../share/java/kafka-rest-lib/json-20201115.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-schema-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-rest-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/logredactor-metrics-1.0.10.jar:/usr/bin/../share/java/kafka-rest-lib/re2j-1.6.jar:/usr/bin/../share/java/kafka-rest-lib/protobuf-java-3.17.3.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/kafka-rest-lib/avro-1.11.0.jar:/usr/bin/../share/java/kafka-rest-lib/classgraph-4.8.21.jar:/usr/bin/../share/java/kafka-rest-lib/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/kafka-rest-lib/auto-value-annotations-1.7.2.jar:/usr/bin/../share/java/kafka-rest-lib/failureaccess-1.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-common-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/gson-2.8.6.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-jvm-1.4.21.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-ce-logs-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-server-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/bcprov-jdk15on-1.68.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-socks-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/broker-plugins-7.1.0-ce-test.jar:/usr/bin/../share/java/confluent-security/kafka-rest/guava-30.1.1-jre.jar:/usr/bin/../share/java/confluent-security/kafka-rest/maven-artifact-3.8.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-licensing-new-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/checker-qual-3.8.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jbcrypt-0.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-log4j-appender-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/annotations-3.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-unix-common-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jaxb-api-2.3.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-tcnative-classes-2.0.46.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/activation-1.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javassist-3.25.0-GA.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-json-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/plexus-utils-3.2.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-handler-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.json-1.0.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-api-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-transforms-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jose4j-0.7.8.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-kqueue-4.1.65.Final-osx-x86_64.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-runtime-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/bcpkix-jdk15on-1.68.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-tools-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/reflections-0.9.12.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.servlet-api-4.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/metrics-core-2.2.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-client-1.745.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-serializers-new-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-api-1.745.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/icu4j-61.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-handler-proxy-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/commons-compress-1.21.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-databind-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-locator-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-security-plugins-common-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/ST4-4.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-utils-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-epoll-4.1.65.Final-linux-x86_64.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-hk2-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr4-4.9.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-common-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-client-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/avro-1.11.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/confluent-security/kafka-rest/commons-lang3-3.12.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-kafka-rest-security-plugin-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/argparse4j-0.7.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/gson-2.8.6.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/snakeyaml-1.27.jar:/usr/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-security/schema-validator/annotations-13.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/guava-30.1.1-jre.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-logging-1.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-common-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-protobuf-provider-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/okio-jvm-3.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/wire-schema-jvm-4.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/checker-qual-3.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-module-parameter-names-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-json-schema-provider-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-guava-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/wire-runtime-jvm-4.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/org.everit.json.schema-1.12.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/handy-uri-templates-2.1.8.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlinx-coroutines-core-1.3.7.jar:/usr/bin/../share/java/confluent-security/schema-validator/proto-google-common-protos-2.5.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-validator-1.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/scala-library-2.13.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-schema-serializer-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-collections-3.2.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/joda-time-2.10.8.jar:/usr/bin/../share/java/confluent-security/schema-validator/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-compress-1.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-script-runtime-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-digester-1.8.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-protobuf-types-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/json-20201115.jar:/usr/bin/../share/java/confluent-security/schema-validator/re2j-1.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/confluent-security/schema-validator/avro-1.11.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/classgraph-4.8.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/confluent-security/schema-validator/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-common-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/gson-2.8.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-jvm-1.4.21.jar:/usr/bin/../support-metrics-client/build/dependant-libs-2.13.6/*:/usr/bin/../support-metrics-client/build/libs/*:/usr/bin/../share/java/confluent-telemetry/confluent-metrics-7.1.0-ce.jar:/usr/share/java/support-metrics-client/*
[INFO] 2023-11-08 14:50:54,086 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.io.tmpdir=/tmp
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.compiler=<NA>
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.name=Linux
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.arch=amd64
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.version=5.15.109+
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:user.name=?
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:user.home=?
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:user.dir=/home/appuser
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.memory.free=1750MB
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.memory.max=16080MB
[INFO] 2023-11-08 14:50:54,087 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.memory.total=1840MB
[INFO] 2023-11-08 14:50:54,091 [main] org.apache.zookeeper.ZooKeeper <init> - Initiating client connection, connectString=zookeeper.confluent.svc.cluster.local:2182 sessionTimeout=22500 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7bca6fac
[INFO] 2023-11-08 14:50:54,164 [main] org.apache.zookeeper.ClientCnxnSocket initProperties - jute.maxbuffer value is 4194304 Bytes
[INFO] 2023-11-08 14:50:54,172 [main] org.apache.zookeeper.ClientCnxn initRequestTimeout - zookeeper.request.timeout value is 0. feature enabled=false
[INFO] 2023-11-08 14:50:54,174 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Waiting until connected.
[INFO] 2023-11-08 14:50:54,209 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:50:54,211 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:50:54,243 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.1.8:2182.
[INFO] 2023-11-08 14:50:54,243 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:50:54,352 [nioEventLoopGroup-2-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:50:54,458 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0x535e94e7]
[WARN] 2023-11-08 14:50:54,467 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - future isn't success.
io.netty.channel.AbstractChannel$AnnotatedConnectException: Connection refused: zookeeper.confluent.svc.cluster.local/10.40.1.8:2182
Caused by: java.net.ConnectException: Connection refused
	at java.base/sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at java.base/sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:777)
	at io.netty.channel.socket.nio.NioSocketChannel.doFinishConnect(NioSocketChannel.java:330)
	at io.netty.channel.nio.AbstractNioChannel$AbstractNioUnsafe.finishConnect(AbstractNioChannel.java:334)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:710)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:658)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:584)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:496)
	at io.netty.util.concurrent.SingleThreadEventExecutor$4.run(SingleThreadEventExecutor.java:986)
	at io.netty.util.internal.ThreadExecutorMap$2.run(ThreadExecutorMap.java:74)
	at io.netty.util.concurrent.FastThreadLocalRunnable.run(FastThreadLocalRunnable.java:30)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:50:55,470 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:50:55,471 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:50:55,471 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.2.12:2182.
[INFO] 2023-11-08 14:50:55,471 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:50:55,472 [nioEventLoopGroup-2-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:50:55,473 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0x0c95433f]
[INFO] 2023-11-08 14:50:55,475 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.15:48472, server: zookeeper.confluent.svc.cluster.local/10.40.2.12:2182
[INFO] 2023-11-08 14:50:55,536 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0x0c95433f, L:/10.40.0.15:48472 - R:zookeeper.confluent.svc.cluster.local/10.40.2.12:2182]
[INFO] 2023-11-08 14:50:55,663 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty channelInactive - channel is disconnected: [id: 0x0c95433f, L:/10.40.0.15:48472 ! R:zookeeper.confluent.svc.cluster.local/10.40.2.12:2182]
[INFO] 2023-11-08 14:50:55,663 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[WARN] 2023-11-08 14:50:55,663 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn run - Session 0x0 for sever zookeeper.confluent.svc.cluster.local/10.40.2.12:2182, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
EndOfStreamException: channel for sessionid 0x0 is lost
	at org.apache.zookeeper.ClientCnxnSocketNetty.doTransport(ClientCnxnSocketNetty.java:285)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
[INFO] 2023-11-08 14:50:57,284 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:50:57,284 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:50:57,285 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.1.10:2182.
[INFO] 2023-11-08 14:50:57,285 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:50:57,286 [nioEventLoopGroup-2-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:50:57,287 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0xe4f5b9f2]
[INFO] 2023-11-08 14:50:57,288 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.15:60278, server: zookeeper.confluent.svc.cluster.local/10.40.1.10:2182
[INFO] 2023-11-08 14:50:57,291 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0xe4f5b9f2, L:/10.40.0.15:60278 - R:zookeeper.confluent.svc.cluster.local/10.40.1.10:2182]
[INFO] 2023-11-08 14:50:57,483 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty channelInactive - channel is disconnected: [id: 0xe4f5b9f2, L:/10.40.0.15:60278 ! R:zookeeper.confluent.svc.cluster.local/10.40.1.10:2182]
[INFO] 2023-11-08 14:50:57,483 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[WARN] 2023-11-08 14:50:57,483 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn run - Session 0x0 for sever zookeeper.confluent.svc.cluster.local/10.40.1.10:2182, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
EndOfStreamException: channel for sessionid 0x0 is lost
	at org.apache.zookeeper.ClientCnxnSocketNetty.doTransport(ClientCnxnSocketNetty.java:285)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
[INFO] 2023-11-08 14:50:58,772 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:50:58,772 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:50:58,773 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.1.8:2182.
[INFO] 2023-11-08 14:50:58,773 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:50:58,774 [nioEventLoopGroup-2-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:50:58,775 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0xca70ce4c]
[INFO] 2023-11-08 14:50:58,776 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.15:42776, server: zookeeper.confluent.svc.cluster.local/10.40.1.8:2182
[INFO] 2023-11-08 14:50:58,778 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0xca70ce4c, L:/10.40.0.15:42776 - R:zookeeper.confluent.svc.cluster.local/10.40.1.8:2182]
[INFO] 2023-11-08 14:50:58,906 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxn onConnected - Session establishment complete on server zookeeper.confluent.svc.cluster.local/10.40.1.8:2182, session id = 0x12f96c0001, negotiated timeout = 22500
[INFO] 2023-11-08 14:50:58,911 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Connected.
[INFO] 2023-11-08 14:50:58,937 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Closing.
[INFO] 2023-11-08 14:50:59,203 [main] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[INFO] 2023-11-08 14:50:59,205 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty channelInactive - channel is disconnected: [id: 0xca70ce4c, L:/10.40.0.15:42776 ! R:zookeeper.confluent.svc.cluster.local/10.40.1.8:2182]
[INFO] 2023-11-08 14:50:59,205 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[INFO] 2023-11-08 14:50:59,206 [main] org.apache.zookeeper.ZooKeeper close - Session: 0x12f96c0001 closed
[INFO] 2023-11-08 14:50:59,206 [main-EventThread] org.apache.zookeeper.ClientCnxn run - EventThread shut down for session: 0x12f96c0001
[INFO] 2023-11-08 14:50:59,207 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Closed.
[INFO] 2023-11-08 14:50:59,208 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Initializing a new session to zookeeper.confluent.svc.cluster.local:2182/kafka-confluent.
[INFO] 2023-11-08 14:50:59,209 [main] org.apache.zookeeper.ZooKeeper <init> - Initiating client connection, connectString=zookeeper.confluent.svc.cluster.local:2182/kafka-confluent sessionTimeout=22500 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@29a1505c
[INFO] 2023-11-08 14:50:59,209 [main] org.apache.zookeeper.ClientCnxnSocket initProperties - jute.maxbuffer value is 4194304 Bytes
[INFO] 2023-11-08 14:50:59,209 [main] org.apache.zookeeper.ClientCnxn initRequestTimeout - zookeeper.request.timeout value is 0. feature enabled=false
[INFO] 2023-11-08 14:50:59,210 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Waiting until connected.
[INFO] 2023-11-08 14:50:59,210 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:50:59,210 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:50:59,210 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.2.12:2182.
[INFO] 2023-11-08 14:50:59,210 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:50:59,211 [nioEventLoopGroup-3-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:50:59,212 [nioEventLoopGroup-3-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0xfb39e5ac]
[INFO] 2023-11-08 14:50:59,214 [nioEventLoopGroup-3-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.15:48488, server: zookeeper.confluent.svc.cluster.local/10.40.2.12:2182
[INFO] 2023-11-08 14:50:59,237 [nioEventLoopGroup-3-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0xfb39e5ac, L:/10.40.0.15:48488 - R:zookeeper.confluent.svc.cluster.local/10.40.2.12:2182]
[INFO] 2023-11-08 14:50:59,422 [nioEventLoopGroup-3-1] org.apache.zookeeper.ClientCnxn onConnected - Session establishment complete on server zookeeper.confluent.svc.cluster.local/10.40.2.12:2182, session id = 0x2000013a55b0000, negotiated timeout = 22500
[INFO] 2023-11-08 14:50:59,423 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Connected.
[INFO] 2023-11-08 14:51:03,816 [main] kafka.server.KafkaServer info - Cluster ID = 9PWH12e6ROOpezLRDGV6Ag
[WARN] 2023-11-08 14:51:03,820 [main] kafka.server.BrokerMetadataCheckpoint warn - No meta.properties file under dir /mnt/data/data0/logs/meta.properties
[INFO] 2023-11-08 14:51:03,878 [main] kafka.server.KafkaConfig logAll - KafkaConfig values: 
	advertised.listeners = EXTERNAL://rb2.my.domain:9092,INTERNAL://kafka-2.kafka.confluent.svc.cluster.local:9071,REPLICATION://kafka-2.kafka.confluent.svc.cluster.local:9072,TOKEN://kafka-2.kafka.confluent.svc.cluster.local:9073
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 2
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = 2
	broker.session.timeout.ms = 9000
	broker.session.uuid = GLYD9RvdRwqiJGkc5Hmbsw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 120
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = true
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.reporting.enable = false
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1200000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = REPLICATION
	inter.broker.protocol.version = 2.6
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = EXTERNAL:SASL_SSL,INTERNAL:SASL_SSL,REPLICATION:SASL_SSL,TOKEN:SASL_SSL
	listeners = EXTERNAL://:9092,INTERNAL://:9071,REPLICATION://:9072,TOKEN://:9073
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /mnt/data/data0/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.6
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	node.id = 2
	num.io.threads = 8
	num.network.threads = 4
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 15000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 3
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.coordinator.enable = false
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 45000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [PLAIN]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = org.apache.zookeeper.ClientCnxnSocketNetty
	zookeeper.connect = zookeeper.confluent.svc.cluster.local:2182/kafka-confluent
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 22500
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = true
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	zookeeper.ssl.truststore.password = [hidden]
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[INFO] 2023-11-08 14:51:03,895 [main] kafka.server.KafkaConfig logAll - KafkaConfig values: 
	advertised.listeners = EXTERNAL://rb2.my.domain:9092,INTERNAL://kafka-2.kafka.confluent.svc.cluster.local:9071,REPLICATION://kafka-2.kafka.confluent.svc.cluster.local:9072,TOKEN://kafka-2.kafka.confluent.svc.cluster.local:9073
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 2
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = 2
	broker.session.timeout.ms = 9000
	broker.session.uuid = GLYD9RvdRwqiJGkc5Hmbsw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 120
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = true
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.reporting.enable = false
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1200000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = REPLICATION
	inter.broker.protocol.version = 2.6
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = EXTERNAL:SASL_SSL,INTERNAL:SASL_SSL,REPLICATION:SASL_SSL,TOKEN:SASL_SSL
	listeners = EXTERNAL://:9092,INTERNAL://:9071,REPLICATION://:9072,TOKEN://:9073
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /mnt/data/data0/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.6
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	node.id = 2
	num.io.threads = 8
	num.network.threads = 4
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 15000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 3
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.coordinator.enable = false
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 45000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [PLAIN]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = org.apache.zookeeper.ClientCnxnSocketNetty
	zookeeper.connect = zookeeper.confluent.svc.cluster.local:2182/kafka-confluent
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 22500
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = true
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	zookeeper.ssl.truststore.password = [hidden]
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[INFO] 2023-11-08 14:51:03,953 [main] kafka.log.LogManager info - Log directory /mnt/data/data0/logs not found, creating it.
[INFO] 2023-11-08 14:51:04,009 [main] kafka.log.LogManager info - Loading logs from log dirs ArraySeq(/mnt/data/data0/logs)
[INFO] 2023-11-08 14:51:04,014 [main] kafka.log.LogManager info - Attempting recovery for all logs in /mnt/data/data0/logs since no clean shutdown file was found
[INFO] 2023-11-08 14:51:04,019 [main] kafka.log.LogManager info - Loaded 0 logs in 9ms.
[INFO] 2023-11-08 14:51:04,021 [main] kafka.log.LogManager info - Starting log cleanup with a period of 300000 ms.
[INFO] 2023-11-08 14:51:04,023 [main] kafka.log.LogManager info - Starting log flusher with a default period of 9223372036854775807 ms.
[INFO] 2023-11-08 14:51:04,042 [main] kafka.log.LogCleaner info - Starting the log cleaner
[INFO] 2023-11-08 14:51:04,150 [kafka-log-cleaner-thread-0] kafka.log.LogCleaner info - [kafka-log-cleaner-thread-0]: Starting
[INFO] 2023-11-08 14:51:04,163 [main] kafka.server.KafkaServer info - [KafkaServer id=2] Creating metadataCache (multi-tenant: false)
[INFO] 2023-11-08 14:51:04,173 [main] io.confluent.security.audit.AuditLogConfig logAll - AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 

[INFO] 2023-11-08 14:51:04,174 [main] io.confluent.security.audit.AuditLogConfig logAll - AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 

[INFO] 2023-11-08 14:51:04,202 [main] io.confluent.crn.CrnAuthorityConfig logAll - CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP

[INFO] 2023-11-08 14:51:04,203 [main] io.confluent.crn.CrnAuthorityConfig logAll - CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP

[INFO] 2023-11-08 14:51:04,204 [main] io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig logAll - MultiTenantAuditLogConfig values: 
	confluent.security.event.logger.client.ip.enable = false
	confluent.security.event.logger.multitenant.enable = false

[INFO] 2023-11-08 14:51:04,226 [ThrottledChannelReaper-Produce] kafka.server.ClientQuotaManager$ThrottledChannelReaper info - [ThrottledChannelReaper-Produce]: Starting
[INFO] 2023-11-08 14:51:04,230 [main] kafka.server.DiskUsageBasedThrottlingConfig$ apply - Empty logDirs received! Disk based throttling won't be activated!
[INFO] 2023-11-08 14:51:04,231 [ThrottledChannelReaper-Fetch] kafka.server.ClientQuotaManager$ThrottledChannelReaper info - [ThrottledChannelReaper-Fetch]: Starting
[INFO] 2023-11-08 14:51:04,232 [main] kafka.server.DiskUsageBasedThrottlingConfig$ apply - Empty logDirs received! Disk based throttling won't be activated!
[INFO] 2023-11-08 14:51:04,235 [ThrottledChannelReaper-Request] kafka.server.ClientQuotaManager$ThrottledChannelReaper info - [ThrottledChannelReaper-Request]: Starting
[INFO] 2023-11-08 14:51:04,236 [main] kafka.server.DiskUsageBasedThrottlingConfig$ apply - Empty logDirs received! Disk based throttling won't be activated!
[INFO] 2023-11-08 14:51:04,237 [ThrottledChannelReaper-ControllerMutation] kafka.server.ClientQuotaManager$ThrottledChannelReaper info - [ThrottledChannelReaper-ControllerMutation]: Starting
[INFO] 2023-11-08 14:51:04,262 [ExpirationReaper-2-ClusterLink] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-2-ClusterLink]: Starting
[INFO] 2023-11-08 14:51:04,273 [main] kafka.server.link.ClusterLinkManager info - Enforce create cluster link policy.
[INFO] 2023-11-08 14:51:04,448 [main] io.confluent.kafka.server.plugins.policy.ClusterLinkPolicyConfig logAll - ClusterLinkPolicyConfig values: 
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.link.policy.acl.sync.ms.max = 300000
	confluent.plugins.link.policy.acl.sync.ms.min = 1000
	confluent.plugins.link.policy.availability.check.ms.max = 60000
	confluent.plugins.link.policy.availability.check.ms.min = 3000
	confluent.plugins.link.policy.consumer.offset.sync.ms.max = 300000
	confluent.plugins.link.policy.consumer.offset.sync.ms.min = 1000
	confluent.plugins.link.policy.replica.socket.receive.buffer.bytes.max = 1048576
	confluent.plugins.link.policy.replica.socket.receive.buffer.bytes.min = 32768
	confluent.plugins.link.policy.sasl.mechanism.allowed = [PLAIN, SCRAM-SHA-256, SCRAM-SHA-512]
	confluent.plugins.link.policy.topic.config.sync.ms.max = 300000
	confluent.plugins.link.policy.topic.config.sync.ms.min = 1000

[INFO] 2023-11-08 14:51:04,448 [main] io.confluent.kafka.server.plugins.policy.CreateClusterLinkPolicy reconfigure - Setting maximum number of destination links to 5 and maximum number of source links to 5
[INFO] 2023-11-08 14:51:04,503 [main] org.apache.kafka.common.security.authenticator.AbstractLogin login - Successfully logged in.
[INFO] 2023-11-08 14:51:04,539 [BrokerToControllerChannelManager broker=2 name=forwarding] kafka.server.BrokerToControllerRequestThread info - [BrokerToControllerChannelManager broker=2 name=forwarding]: Starting
[INFO] 2023-11-08 14:51:04,688 [main] kafka.network.ConnectionQuotas info - Updated connection-tokens max connection creation rate to 2147483647
[INFO] 2023-11-08 14:51:04,699 [main] kafka.network.Acceptor info - Awaiting socket connections on 0.0.0.0:9092.
[INFO] 2023-11-08 14:51:04,722 [main] org.apache.kafka.common.security.authenticator.AbstractLogin login - Successfully logged in.
[INFO] 2023-11-08 14:51:04,757 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=2] Created data-plane acceptor and processors for endpoint : ListenerName(EXTERNAL)
[INFO] 2023-11-08 14:51:04,757 [main] kafka.network.ConnectionQuotas info - Updated connection-tokens max connection creation rate to 2147483647
[INFO] 2023-11-08 14:51:04,758 [main] kafka.network.Acceptor info - Awaiting socket connections on 0.0.0.0:9071.
[INFO] 2023-11-08 14:51:04,760 [main] org.apache.kafka.common.security.authenticator.AbstractLogin login - Successfully logged in.
[INFO] 2023-11-08 14:51:04,777 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=2] Created data-plane acceptor and processors for endpoint : ListenerName(INTERNAL)
[INFO] 2023-11-08 14:51:04,778 [main] kafka.network.ConnectionQuotas info - Updated connection-tokens max connection creation rate to 2147483647
[INFO] 2023-11-08 14:51:04,778 [main] kafka.network.Acceptor info - Awaiting socket connections on 0.0.0.0:9072.
[INFO] 2023-11-08 14:51:05,106 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=2] Created data-plane acceptor and processors for endpoint : ListenerName(REPLICATION)
[INFO] 2023-11-08 14:51:05,109 [main] kafka.network.ConnectionQuotas info - Updated connection-tokens max connection creation rate to 2147483647
[INFO] 2023-11-08 14:51:05,110 [main] kafka.network.Acceptor info - Awaiting socket connections on 0.0.0.0:9073.
[INFO] 2023-11-08 14:51:05,115 [main] io.confluent.kafka.clients.plugins.auth.jwt.JwtAuthenticatorConfig logAll - JwtAuthenticatorConfig values: 
	allowUnsafeURL = false
	audience = 
	issuer = Confluent
	jkuDomainWhiteList = []
	jwksLocation = /mnt/secrets/mds-token/mdsPublicKey.pem
	verificationKeyRefreshInterval = 3600000
	verificationKeyResolver = pemfile

[INFO] 2023-11-08 14:51:05,285 [main] org.apache.kafka.common.security.authenticator.AbstractLogin login - Successfully logged in.
[INFO] 2023-11-08 14:51:05,292 [main] io.confluent.kafka.clients.plugins.auth.jwt.JwtAuthenticatorConfig logAll - JwtAuthenticatorConfig values: 
	allowUnsafeURL = false
	audience = 
	issuer = Confluent
	jkuDomainWhiteList = []
	jwksLocation = /mnt/secrets/mds-token/mdsPublicKey.pem
	verificationKeyRefreshInterval = 3600000
	verificationKeyResolver = pemfile

[INFO] 2023-11-08 14:51:05,298 [main] io.confluent.kafka.clients.plugins.auth.jwt.JwtAuthenticatorConfig logAll - JwtAuthenticatorConfig values: 
	allowUnsafeURL = false
	audience = 
	issuer = Confluent
	jkuDomainWhiteList = []
	jwksLocation = /mnt/secrets/mds-token/mdsPublicKey.pem
	verificationKeyRefreshInterval = 3600000
	verificationKeyResolver = pemfile

[INFO] 2023-11-08 14:51:05,305 [main] io.confluent.kafka.clients.plugins.auth.jwt.JwtAuthenticatorConfig logAll - JwtAuthenticatorConfig values: 
	allowUnsafeURL = false
	audience = 
	issuer = Confluent
	jkuDomainWhiteList = []
	jwksLocation = /mnt/secrets/mds-token/mdsPublicKey.pem
	verificationKeyRefreshInterval = 3600000
	verificationKeyResolver = pemfile

[INFO] 2023-11-08 14:51:05,310 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=2] Created data-plane acceptor and processors for endpoint : ListenerName(TOKEN)
[INFO] 2023-11-08 14:51:05,353 [ExpirationReaper-2-Produce] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-2-Produce]: Starting
[INFO] 2023-11-08 14:51:05,354 [ExpirationReaper-2-Fetch] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-2-Fetch]: Starting
[INFO] 2023-11-08 14:51:05,355 [ExpirationReaper-2-DeleteRecords] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-2-DeleteRecords]: Starting
[INFO] 2023-11-08 14:51:05,356 [ExpirationReaper-2-ElectLeader] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-2-ElectLeader]: Starting
[INFO] 2023-11-08 14:51:05,357 [ExpirationReaper-2-ListOffsets] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-2-ListOffsets]: Starting
[INFO] 2023-11-08 14:51:05,375 [LogDirFailureHandler] kafka.server.ReplicaManager$LogDirFailureHandler info - [LogDirFailureHandler]: Starting
[INFO] 2023-11-08 14:51:05,393 [main] kafka.zk.KafkaZkClient info - Creating /brokers/ids/2 (is it secure? false)
[INFO] 2023-11-08 14:51:05,413 [main] kafka.zk.KafkaZkClient info - Stat of the created znode at /brokers/ids/2 is: 4294967373,4294967373,1699455065405,1699455065405,1,0,0,144115272454438912,449,0,4294967373

[INFO] 2023-11-08 14:51:05,414 [main] kafka.zk.KafkaZkClient info - Registered broker 2 at path /brokers/ids/2 with addresses: EXTERNAL://rb2.my.domain:9092,INTERNAL://kafka-2.kafka.confluent.svc.cluster.local:9071,REPLICATION://kafka-2.kafka.confluent.svc.cluster.local:9072,TOKEN://kafka-2.kafka.confluent.svc.cluster.local:9073, czxid (broker epoch): 4294967373
[INFO] 2023-11-08 14:51:05,698 [main] kafka.controller.DataBalanceManager apply - DataBalancer: attempting startup with io.confluent.databalancer.KafkaDataBalanceManager
[INFO] 2023-11-08 14:51:05,715 [main] io.confluent.databalancer.KafkaDataBalanceManager enableDatabalancerMetric - Registering metric ActiveBalancerCount
[INFO] 2023-11-08 14:51:05,719 [controller-event-thread] kafka.controller.ControllerEventManager$ControllerEventThread info - [ControllerEventThread controllerId=2] Starting
[INFO] 2023-11-08 14:51:05,738 [ExpirationReaper-2-topic] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-2-topic]: Starting
[INFO] 2023-11-08 14:51:05,749 [ExpirationReaper-2-Heartbeat] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-2-Heartbeat]: Starting
[INFO] 2023-11-08 14:51:05,750 [ExpirationReaper-2-Rebalance] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-2-Rebalance]: Starting
[INFO] 2023-11-08 14:51:05,781 [main] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Starting up.
[INFO] 2023-11-08 14:51:05,789 [main] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Startup complete.
[INFO] 2023-11-08 14:51:05,809 [main] kafka.coordinator.transaction.ZkProducerIdManager info - [ZK ProducerId Manager 2]: Acquired new producerId block ProducerIdsBlock{brokerId=2, producerIdStart=2000, producerIdLen=1000} by writing to Zk with path version 3
[INFO] 2023-11-08 14:51:05,836 [main] kafka.coordinator.transaction.TransactionCoordinator info - [TransactionCoordinator id=2] Starting up.
[INFO] 2023-11-08 14:51:05,842 [TxnMarkerSenderThread-2] kafka.coordinator.transaction.TransactionMarkerChannelManager info - [Transaction Marker Channel Manager 2]: Starting
[INFO] 2023-11-08 14:51:05,842 [main] kafka.coordinator.transaction.TransactionCoordinator info - [TransactionCoordinator id=2] Startup complete.
[INFO] 2023-11-08 14:51:05,853 [main] io.confluent.security.authorizer.ConfluentAuthorizerConfig logAll - ConfluentAuthorizerConfig values: 
	allow.everyone.if.no.acl.found = false
	broker.users = 
	confluent.authorizer.access.rule.providers = [ZK_ACL, CONFLUENT]
	confluent.authorizer.acl.migration.batch.size = 1000
	confluent.authorizer.init.timeout.ms = 600000
	confluent.authorizer.migrate.acls.from.zk = false
	super.users = User:operator;User:kafka

[INFO] 2023-11-08 14:51:05,898 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Initializing a new session to zookeeper.confluent.svc.cluster.local:2182.
[INFO] 2023-11-08 14:51:05,898 [main] org.apache.zookeeper.ZooKeeper <init> - Initiating client connection, connectString=zookeeper.confluent.svc.cluster.local:2182 sessionTimeout=22500 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@3030836d
[INFO] 2023-11-08 14:51:05,898 [main] org.apache.zookeeper.ClientCnxnSocket initProperties - jute.maxbuffer value is 4194304 Bytes
[INFO] 2023-11-08 14:51:05,899 [main] org.apache.zookeeper.ClientCnxn initRequestTimeout - zookeeper.request.timeout value is 0. feature enabled=false
[INFO] 2023-11-08 14:51:05,900 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Waiting until connected.
[INFO] 2023-11-08 14:51:05,900 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:51:05,900 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:51:05,901 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.1.10:2182.
[INFO] 2023-11-08 14:51:05,901 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:51:05,902 [nioEventLoopGroup-4-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:51:05,903 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0xad891025]
[INFO] 2023-11-08 14:51:05,905 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.15:58916, server: zookeeper.confluent.svc.cluster.local/10.40.1.10:2182
[INFO] 2023-11-08 14:51:05,907 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0xad891025, L:/10.40.0.15:58916 - R:zookeeper.confluent.svc.cluster.local/10.40.1.10:2182]
[INFO] 2023-11-08 14:51:05,986 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxn onConnected - Session establishment complete on server zookeeper.confluent.svc.cluster.local/10.40.1.10:2182, session id = 0x100001300e00004, negotiated timeout = 22500
[INFO] 2023-11-08 14:51:05,986 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Connected.
[INFO] 2023-11-08 14:51:05,996 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Closing.
[INFO] 2023-11-08 14:51:06,001 [main] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[INFO] 2023-11-08 14:51:06,002 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxnSocketNetty channelInactive - channel is disconnected: [id: 0xad891025, L:/10.40.0.15:58916 ! R:zookeeper.confluent.svc.cluster.local/10.40.1.10:2182]
[INFO] 2023-11-08 14:51:06,002 [main-EventThread] org.apache.zookeeper.ClientCnxn run - EventThread shut down for session: 0x100001300e00004
[INFO] 2023-11-08 14:51:06,002 [main] org.apache.zookeeper.ZooKeeper close - Session: 0x100001300e00004 closed
[INFO] 2023-11-08 14:51:06,002 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[INFO] 2023-11-08 14:51:06,003 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Closed.
[INFO] 2023-11-08 14:51:06,004 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Initializing a new session to zookeeper.confluent.svc.cluster.local:2182/kafka-confluent.
[INFO] 2023-11-08 14:51:06,004 [main] org.apache.zookeeper.ZooKeeper <init> - Initiating client connection, connectString=zookeeper.confluent.svc.cluster.local:2182/kafka-confluent sessionTimeout=22500 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@4c24f3a2
[INFO] 2023-11-08 14:51:06,005 [main] org.apache.zookeeper.ClientCnxnSocket initProperties - jute.maxbuffer value is 4194304 Bytes
[INFO] 2023-11-08 14:51:06,005 [main] org.apache.zookeeper.ClientCnxn initRequestTimeout - zookeeper.request.timeout value is 0. feature enabled=false
[INFO] 2023-11-08 14:51:06,006 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Waiting until connected.
[INFO] 2023-11-08 14:51:06,006 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:51:06,006 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:51:06,006 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.1.8:2182.
[INFO] 2023-11-08 14:51:06,006 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:51:06,007 [nioEventLoopGroup-5-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:51:06,008 [nioEventLoopGroup-5-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0xac6c8958]
[INFO] 2023-11-08 14:51:06,009 [nioEventLoopGroup-5-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.15:55382, server: zookeeper.confluent.svc.cluster.local/10.40.1.8:2182
[INFO] 2023-11-08 14:51:06,030 [nioEventLoopGroup-5-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0xac6c8958, L:/10.40.0.15:55382 - R:zookeeper.confluent.svc.cluster.local/10.40.1.8:2182]
[INFO] 2023-11-08 14:51:06,077 [nioEventLoopGroup-5-1] org.apache.zookeeper.ClientCnxn onConnected - Session establishment complete on server zookeeper.confluent.svc.cluster.local/10.40.1.8:2182, session id = 0x12f96c0004, negotiated timeout = 22500
[INFO] 2023-11-08 14:51:06,077 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Connected.
[INFO] 2023-11-08 14:51:06,178 [/kafka-acl-changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread info - [/kafka-acl-changes-event-process-thread]: Starting
[INFO] 2023-11-08 14:51:06,178 [/kafka-acl-extended-changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread info - [/kafka-acl-extended-changes-event-process-thread]: Starting
[INFO] 2023-11-08 14:51:06,212 [main] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-2.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[INFO] 2023-11-08 14:51:06,219 [main] kafka.server.KafkaConfig logAll - KafkaConfig values: 
	advertised.listeners = EXTERNAL://rb2.my.domain:9092,INTERNAL://kafka-2.kafka.confluent.svc.cluster.local:9071,REPLICATION://kafka-2.kafka.confluent.svc.cluster.local:9072,TOKEN://kafka-2.kafka.confluent.svc.cluster.local:9073
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 2
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = 2
	broker.session.timeout.ms = 9000
	broker.session.uuid = GLYD9RvdRwqiJGkc5Hmbsw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 120
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = true
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.reporting.enable = false
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1200000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = REPLICATION
	inter.broker.protocol.version = 2.6
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = EXTERNAL:SASL_SSL,INTERNAL:SASL_SSL,REPLICATION:SASL_SSL,TOKEN:SASL_SSL
	listeners = EXTERNAL://:9092,INTERNAL://:9071,REPLICATION://:9072,TOKEN://:9073
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /mnt/data/data0/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.6
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	node.id = 2
	num.io.threads = 8
	num.network.threads = 4
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 15000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 3
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.coordinator.enable = false
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 45000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [PLAIN]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = org.apache.zookeeper.ClientCnxnSocketNetty
	zookeeper.connect = zookeeper.confluent.svc.cluster.local:2182/kafka-confluent
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 22500
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = true
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	zookeeper.ssl.truststore.password = [hidden]
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[INFO] 2023-11-08 14:51:06,229 [main] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-2.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[INFO] 2023-11-08 14:51:06,425 [main] io.confluent.security.store.kafka.KafkaStoreConfig logAll - KafkaStoreConfig values: 
	confluent.metadata.refresh.timeout.ms = 60000
	confluent.metadata.retry.timeout.ms = 86400000
	confluent.metadata.topic.create.timeout.ms = 600000
	confluent.metadata.topic.replication.factor = 3

[INFO] 2023-11-08 14:51:06,448 [main] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-auth-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.security.store.kafka.clients.JsonSerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.security.store.kafka.clients.JsonSerde

[WARN] 2023-11-08 14:51:06,490 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,491 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,491 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'auto.create.topics.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,491 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,491 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,492 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,492 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,492 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,492 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,492 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,493 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,493 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'num.recovery.threads.per.data.dir' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,493 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,493 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,494 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,494 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.server.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,494 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.mechanism.inter.broker.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,494 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'default.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,494 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,494 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'broker.session.uuid' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,495 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,495 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,495 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,495 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,495 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,496 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,496 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.operator.managed' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,496 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'transaction.state.log.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,497 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'num.io.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,497 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,498 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'delete.topic.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,498 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,498 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.clientCnxnSocket' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,498 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,498 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,499 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.ssl.client.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,499 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.login.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,499 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'offsets.commit.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,499 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,500 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,500 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'authorizer.class.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,500 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,500 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,501 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,501 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,501 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,501 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,501 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'offsets.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,502 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'transaction.state.log.min.isr' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,502 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,502 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,502 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'socket.request.max.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,502 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,502 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,505 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,505 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,505 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'offsets.retention.minutes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,505 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'inter.broker.protocol.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,505 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,505 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,505 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.security.protocol.map' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,505 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'log.retention.hours' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,506 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,508 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'num.partitions' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,508 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,508 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,508 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,508 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'inter.broker.listener.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,508 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,508 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.balancer.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'node.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,509 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'log.message.format.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,510 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,511 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,511 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,511 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'log.segment.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,511 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,511 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'group.max.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,512 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,512 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,512 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,512 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'log.dirs' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,512 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,512 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,512 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,513 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,513 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,513 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,513 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,513 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,513 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'replica.lag.time.max.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,514 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,514 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,514 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,514 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,514 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,515 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'num.network.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,515 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'socket.send.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,515 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,515 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'socket.receive.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,515 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,515 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'broker.rack' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,516 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,516 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,516 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'broker.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,518 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'offsets.topic.compression.codec' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,518 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'log.retention.check.interval.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,518 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,518 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:06,520 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:06,520 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:06,520 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455066519
[DEBUG] 2023-11-08 14:51:06,533 [main] io.confluent.security.auth.store.kafka.KafkaAuthStore configure - Configured auth store with configs io.confluent.security.store.kafka.KafkaStoreConfig: 
	confluent.metadata.topic.replication.factor=3%n	confluent.metadata.retry.timeout.ms=86400000%n	confluent.metadata.topic.create.timeout.ms=600000%n	confluent.metadata.refresh.timeout.ms=60000
[INFO] 2023-11-08 14:51:06,546 [main] io.confluent.security.auth.provider.ldap.LdapConfig logAll - LdapConfig values: 
	ldap.group.dn.name.pattern = 
	ldap.group.member.attribute = member
	ldap.group.member.attribute.pattern = CN=(.*),DC=test,DC=com
	ldap.group.name.attribute = cn
	ldap.group.name.attribute.pattern = 
	ldap.group.object.class = group
	ldap.group.search.base = dc=test,dc=com
	ldap.group.search.filter = 
	ldap.group.search.scope = 1
	ldap.refresh.interval.ms = 60000
	ldap.retry.backoff.max.ms = 1000
	ldap.retry.backoff.ms = 100
	ldap.retry.timeout.ms = 86400000
	ldap.sasl.jaas.config = null
	ldap.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	ldap.sasl.kerberos.min.time.before.relogin = 60000
	ldap.sasl.kerberos.service.name = ldap
	ldap.sasl.kerberos.ticket.renew.jitter = 0.05
	ldap.sasl.kerberos.ticket.renew.window.factor = 0.8
	ldap.sasl.login.callback.handler.class = null
	ldap.sasl.login.class = null
	ldap.sasl.login.connect.timeout.ms = null
	ldap.sasl.login.read.timeout.ms = null
	ldap.sasl.login.retry.backoff.max.ms = 10000
	ldap.sasl.login.retry.backoff.ms = 100
	ldap.sasl.oauthbearer.clock.skew.seconds = 30
	ldap.sasl.oauthbearer.expected.audience = null
	ldap.sasl.oauthbearer.expected.issuer = null
	ldap.sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	ldap.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	ldap.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	ldap.sasl.oauthbearer.jwks.endpoint.url = null
	ldap.sasl.oauthbearer.scope.claim.name = scope
	ldap.sasl.oauthbearer.sub.claim.name = sub
	ldap.sasl.oauthbearer.token.endpoint.url = null
	ldap.search.mode = GROUPS
	ldap.search.page.size = 0
	ldap.ssl.cipher.suites = null
	ldap.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ldap.ssl.endpoint.identification.algorithm = https
	ldap.ssl.engine.factory.class = null
	ldap.ssl.key.password = null
	ldap.ssl.keymanager.algorithm = SunX509
	ldap.ssl.keystore.certificate.chain = null
	ldap.ssl.keystore.key = null
	ldap.ssl.keystore.location = null
	ldap.ssl.keystore.password = null
	ldap.ssl.keystore.type = JKS
	ldap.ssl.protocol = TLSv1.3
	ldap.ssl.provider = null
	ldap.ssl.secure.random.implementation = null
	ldap.ssl.trustmanager.algorithm = PKIX
	ldap.ssl.truststore.certificates = null
	ldap.ssl.truststore.location = null
	ldap.ssl.truststore.password = null
	ldap.ssl.truststore.type = JKS
	ldap.user.dn.name.pattern = 
	ldap.user.memberof.attribute = memberof
	ldap.user.memberof.attribute.pattern = CN=(.*),DC=test,DC=com
	ldap.user.name.attribute = cn
	ldap.user.name.attribute.pattern = 
	ldap.user.object.class = organizationalRole
	ldap.user.password.attribute = null
	ldap.user.search.base = dc=test,dc=com
	ldap.user.search.filter = 
	ldap.user.search.scope = 1

[INFO] 2023-11-08 14:51:06,552 [main] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-2.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[INFO] 2023-11-08 14:51:06,554 [main] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-2.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[DEBUG] 2023-11-08 14:51:06,554 [main] io.confluent.security.auth.store.kafka.KafkaAuthStore startService - Starting writer for auth store [https://kafka-2.kafka.confluent.svc.cluster.local:8090]
[INFO] 2023-11-08 14:51:06,569 [main] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-auth-producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class io.confluent.security.store.kafka.clients.JsonSerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.security.store.kafka.clients.JsonSerde

[WARN] 2023-11-08 14:51:06,596 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,600 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,601 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'auto.create.topics.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,601 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,601 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,601 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,601 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,601 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'num.recovery.threads.per.data.dir' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.server.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'sasl.mechanism.inter.broker.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'default.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,602 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'broker.session.uuid' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,603 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,603 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,603 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,603 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,603 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,604 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,604 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.operator.managed' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,605 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'transaction.state.log.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,606 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'num.io.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,606 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,606 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'delete.topic.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,607 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,610 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.clientCnxnSocket' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,611 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,611 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,611 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.ssl.client.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,611 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.login.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,611 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'offsets.commit.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,612 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,612 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,612 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'authorizer.class.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,612 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,612 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,612 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,612 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,612 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,615 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,615 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'offsets.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,615 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'transaction.state.log.min.isr' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,616 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,616 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,616 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'socket.request.max.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,616 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,616 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,616 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,617 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,617 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'offsets.retention.minutes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,617 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'inter.broker.protocol.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,617 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,617 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,617 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.security.protocol.map' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,617 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'log.retention.hours' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,617 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,617 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'num.partitions' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,618 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,618 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,618 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,618 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'inter.broker.listener.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,618 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,618 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,619 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,619 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,619 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,619 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,619 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,619 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,619 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,619 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,620 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.balancer.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,620 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'node.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,620 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,620 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,620 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,620 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,620 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,621 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,621 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,622 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,622 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,622 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,622 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,622 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,622 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,623 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,623 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,623 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'log.message.format.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,623 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'log.segment.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'group.max.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'log.dirs' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,625 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,626 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,626 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,626 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,626 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'replica.lag.time.max.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,626 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,626 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'num.network.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'socket.send.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'socket.receive.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'broker.rack' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'broker.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'offsets.topic.compression.codec' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'log.retention.check.interval.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,627 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:06,627 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:06,627 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:06,627 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455066627
[INFO] 2023-11-08 14:51:06,642 [main] io.confluent.security.auth.provider.ldap.LdapConfig logAll - LdapConfig values: 
	ldap.group.dn.name.pattern = 
	ldap.group.member.attribute = member
	ldap.group.member.attribute.pattern = CN=(.*),DC=test,DC=com
	ldap.group.name.attribute = cn
	ldap.group.name.attribute.pattern = 
	ldap.group.object.class = group
	ldap.group.search.base = dc=test,dc=com
	ldap.group.search.filter = 
	ldap.group.search.scope = 1
	ldap.refresh.interval.ms = 60000
	ldap.retry.backoff.max.ms = 1000
	ldap.retry.backoff.ms = 100
	ldap.retry.timeout.ms = 86400000
	ldap.sasl.jaas.config = null
	ldap.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	ldap.sasl.kerberos.min.time.before.relogin = 60000
	ldap.sasl.kerberos.service.name = ldap
	ldap.sasl.kerberos.ticket.renew.jitter = 0.05
	ldap.sasl.kerberos.ticket.renew.window.factor = 0.8
	ldap.sasl.login.callback.handler.class = null
	ldap.sasl.login.class = null
	ldap.sasl.login.connect.timeout.ms = null
	ldap.sasl.login.read.timeout.ms = null
	ldap.sasl.login.retry.backoff.max.ms = 10000
	ldap.sasl.login.retry.backoff.ms = 100
	ldap.sasl.oauthbearer.clock.skew.seconds = 30
	ldap.sasl.oauthbearer.expected.audience = null
	ldap.sasl.oauthbearer.expected.issuer = null
	ldap.sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	ldap.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	ldap.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	ldap.sasl.oauthbearer.jwks.endpoint.url = null
	ldap.sasl.oauthbearer.scope.claim.name = scope
	ldap.sasl.oauthbearer.sub.claim.name = sub
	ldap.sasl.oauthbearer.token.endpoint.url = null
	ldap.search.mode = GROUPS
	ldap.search.page.size = 0
	ldap.ssl.cipher.suites = null
	ldap.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ldap.ssl.endpoint.identification.algorithm = https
	ldap.ssl.engine.factory.class = null
	ldap.ssl.key.password = null
	ldap.ssl.keymanager.algorithm = SunX509
	ldap.ssl.keystore.certificate.chain = null
	ldap.ssl.keystore.key = null
	ldap.ssl.keystore.location = null
	ldap.ssl.keystore.password = null
	ldap.ssl.keystore.type = JKS
	ldap.ssl.protocol = TLSv1.3
	ldap.ssl.provider = null
	ldap.ssl.secure.random.implementation = null
	ldap.ssl.trustmanager.algorithm = PKIX
	ldap.ssl.truststore.certificates = null
	ldap.ssl.truststore.location = null
	ldap.ssl.truststore.password = null
	ldap.ssl.truststore.type = JKS
	ldap.user.dn.name.pattern = 
	ldap.user.memberof.attribute = memberof
	ldap.user.memberof.attribute.pattern = CN=(.*),DC=test,DC=com
	ldap.user.name.attribute = cn
	ldap.user.name.attribute.pattern = 
	ldap.user.object.class = organizationalRole
	ldap.user.password.attribute = null
	ldap.user.search.base = dc=test,dc=com
	ldap.user.search.filter = 
	ldap.user.search.scope = 1

[INFO] 2023-11-08 14:51:06,651 [main] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-coordinator-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = _confluent-metadata-coordinator-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[INFO] 2023-11-08 14:51:06,651 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:06,651 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:06,651 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455066651
[DEBUG] 2023-11-08 14:51:06,670 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager run - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Starting metadata node coordinator
[INFO] 2023-11-08 14:51:06,677 [main] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-2.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[DEBUG] 2023-11-08 14:51:06,677 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-2.kafka.confluent.svc.cluster.local:9072 (id: -1 rack: null)
[INFO] 2023-11-08 14:51:06,696 [main] kafka.server.link.ClusterLinkMetadataManager info - [ClusterLinkMetadataManager-broker-2] Cluster link metadata manager started without metadata topic, controller will be the link coordinator.
[INFO] 2023-11-08 14:51:06,706 [main] kafka.server.link.ClusterLinkManager info - ClusterLinkManager has started up.
[INFO] 2023-11-08 14:51:06,755 [ExpirationReaper-2-AlterAcls] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-2-AlterAcls]: Starting
[INFO] 2023-11-08 14:51:06,760 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,765 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,767 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,773 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,774 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,791 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,792 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,794 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,812 [BrokerHealthManager] kafka.availability.BrokerHealthManager info - [BrokerHealthManager]: Starting
[INFO] 2023-11-08 14:51:06,916 [main] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:07,021 [main] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[INFO] 2023-11-08 14:51:07,038 [main] org.apache.kafka.common.security.authenticator.AbstractLogin login - Successfully logged in.
[WARN] 2023-11-08 14:51:07,046 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:07,047 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:07,047 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:07,047 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:07,047 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455067047
[INFO] 2023-11-08 14:51:07,089 [main] io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig logAll - EventEmitterConfig values: 

[INFO] 2023-11-08 14:51:07,094 [main] io.confluent.metrics.reporter.ConfluentMetricsReporter onUpdate - Starting Confluent metrics reporter for cluster id 9PWH12e6ROOpezLRDGV6Ag with an interval of 30000 ms
[WARN] 2023-11-08 14:51:07,175 [main] io.confluent.telemetry.ConfluentTelemetryConfig <init> - no telemetry exporters are enabled
[WARN] 2023-11-08 14:51:07,340 [main] io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade withLabel - Ignoring redefinition of existing telemetry label kafka.version
[INFO] 2023-11-08 14:51:07,368 [main] io.confluent.telemetry.ConfluentTelemetryConfig logAll - ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

[INFO] 2023-11-08 14:51:07,375 [main] io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig logAll - VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

[INFO] 2023-11-08 14:51:07,376 [main] io.confluent.telemetry.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[INFO] 2023-11-08 14:51:07,385 [main] io.confluent.telemetry.exporter.kafka.KafkaExporterConfig logAll - KafkaExporterConfig values: 
	enabled = true
	events.enabled = true
	metrics.enabled = true
	metrics.include = (io\.confluent\.kafka\.server/broker_topic/bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/messages_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_produce_requests/rate/1_min|io\.confluent\.kafka\.server/log/size|io\.confluent\.kafka\.server/log_flush/log_flush_rate_and_time_ms|io\.confluent\.kafka\.server/log_flush/log_flush_rate_and_time_ms/rate/1_min|io\.confluent\.kafka\.server/request/local_time_ms|io\.confluent\.kafka\.server/request/request_queue_time_ms|io\.confluent\.kafka\.server/request/requests/rate/1_min|io\.confluent\.kafka\.server/request/total_time_ms|io\.confluent\.kafka\.server/request_channel/request_queue_size|io\.confluent\.kafka\.server/request_channel/response_queue_size|io\.confluent\.kafka\.server/request_handler_pool/request_handler_avg_idle_percent/rate/1_min|io\.confluent\.system/jvm/os/process_cpu_load|io\.confluent\.system/volume/disk_total_bytes)
	producer.bootstrap.servers = kafka-2.kafka.confluent.svc.cluster.local:9072
	topic.create = true
	topic.max.message.bytes = 10485760
	topic.name = _confluent-telemetry-metrics
	topic.partitions = 12
	topic.replicas = 3
	topic.retention.bytes = -1
	topic.retention.ms = 259200000
	topic.roll.ms = 14400000
	type = kafka

[INFO] 2023-11-08 14:51:07,386 [main] io.confluent.telemetry.reporter.TelemetryReporter initEventLogger - Initializing the event logger
[INFO] 2023-11-08 14:51:07,393 [main] io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig logAll - EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter

[INFO] 2023-11-08 14:51:07,407 [main] io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[INFO] 2023-11-08 14:51:07,960 [main] io.confluent.telemetry.reporter.TelemetryReporter initExporters - Creating kafka exporter named '_local'
[INFO] 2023-11-08 14:51:07,967 [main] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.telemetry.serde.OpencensusMetricsProto

[INFO] 2023-11-08 14:51:07,971 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:07,971 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:07,971 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455067971
[INFO] 2023-11-08 14:51:07,973 [main] io.confluent.telemetry.reporter.TelemetryReporter startMetricCollectorTask - Starting Confluent telemetry reporter with an interval of 60000 ms for resource = (type = kafka)
[INFO] 2023-11-08 14:51:08,116 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread info - [/config/changes-event-process-thread]: Starting
[INFO] 2023-11-08 14:51:08,149 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=2] Starting socket server acceptors and processors
[INFO] 2023-11-08 14:51:08,159 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=2] Started data-plane acceptor and processor(s) for endpoint : ListenerName(REPLICATION)
[INFO] 2023-11-08 14:51:08,547 [BrokerToControllerChannelManager broker=2 name=forwarding] kafka.server.BrokerToControllerRequestThread info - [BrokerToControllerChannelManager broker=2 name=forwarding]: Recorded new controller, from now on will use broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1 tags: [])
[INFO] 2023-11-08 14:51:08,556 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 1 from controller 1 for 50 partitions
[INFO] 2023-11-08 14:51:08,558 [data-plane-kafka-request-handler-1] kafka.server.ZkAdminManager info - [Admin Manager on Broker 2]: Error processing create topic request CreatableTopic(name='__consumer_offsets', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='producer'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='segment.bytes', value='104857600'), CreateableTopicConfig(name='confluent.placement.constraints', value='')], linkName=null, mirrorTopic=null)
org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 3 larger than available brokers: 0.
[INFO] 2023-11-08 14:51:08,562 [auth-reader-1] org.apache.kafka.clients.Metadata update - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:08,566 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068565, latencyMs=1885, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-2, correlationId=0), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=-1, host='', port=-1, errorCode=15, errorMessage='')]))
[DEBUG] 2023-11-08 14:51:08,566 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Group coordinator lookup failed: 
[DEBUG] 2023-11-08 14:51:08,566 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator ensureCoordinatorReady - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Coordinator discovery failed, refreshing metadata
org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
[INFO] 2023-11-08 14:51:08,568 [metadata-service-coordinator] org.apache.kafka.clients.Metadata update - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:08,574 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[DEBUG] 2023-11-08 14:51:08,606 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068606, latencyMs=32, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-2, correlationId=4), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=-1, host='', port=-1, errorCode=15, errorMessage='')]))
[DEBUG] 2023-11-08 14:51:08,606 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Group coordinator lookup failed: 
[DEBUG] 2023-11-08 14:51:08,606 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator ensureCoordinatorReady - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Coordinator discovery failed, refreshing metadata
org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
[INFO] 2023-11-08 14:51:08,613 [kafka-producer-network-thread | _confluent-metadata-auth-producer-2] org.apache.kafka.clients.Metadata update - [Producer clientId=_confluent-metadata-auth-producer-2] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:08,671 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[INFO] 2023-11-08 14:51:08,674 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(__consumer_offsets-22, __consumer_offsets-4, __consumer_offsets-25, __consumer_offsets-49, __consumer_offsets-31, __consumer_offsets-37, __consumer_offsets-19, __consumer_offsets-13, __consumer_offsets-43, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-7, __consumer_offsets-46, __consumer_offsets-16, __consumer_offsets-28, __consumer_offsets-10, __consumer_offsets-40)
[DEBUG] 2023-11-08 14:51:08,681 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068680, latencyMs=9, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-2, correlationId=7), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=-1, host='', port=-1, errorCode=15, errorMessage='')]))
[DEBUG] 2023-11-08 14:51:08,681 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Group coordinator lookup failed: 
[DEBUG] 2023-11-08 14:51:08,681 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator ensureCoordinatorReady - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Coordinator discovery failed, refreshing metadata
org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
[INFO] 2023-11-08 14:51:08,681 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 1 epoch 1 as part of the become-leader transition for 17 partitions
[DEBUG] 2023-11-08 14:51:08,779 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[DEBUG] 2023-11-08 14:51:08,783 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068782, latencyMs=3, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-2, correlationId=9), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=0, host='kafka-0.kafka.confluent.svc.cluster.local', port=9072, errorCode=0, errorMessage='')]))
[INFO] 2023-11-08 14:51:08,783 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Discovered group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:08,791 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager onRevoked - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Metadata writer assignment revoked for generation -1
[INFO] 2023-11-08 14:51:08,792 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group](Re-)joining group
[DEBUG] 2023-11-08 14:51:08,792 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator run - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Heartbeat thread started
[DEBUG] 2023-11-08 14:51:08,818 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending JoinGroup (JoinGroupRequestData(groupId='_confluent-metadata-coordinator-group', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='metadata-service', protocols=[JoinGroupRequestProtocol(name='v0', metadata=[123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 50, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125])])) to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:08,822 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager run - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Wake up exception from poll
[INFO] 2023-11-08 14:51:08,851 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted.
[INFO] 2023-11-08 14:51:08,851 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Requesting disconnect from last known coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:08,851 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
[INFO] 2023-11-08 14:51:08,854 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-37, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,860 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-37 in /mnt/data/data0/logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,863 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-37 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-37
[INFO] 2023-11-08 14:51:08,865 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-37 broker=2] Log loaded for partition __consumer_offsets-37 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,867 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,877 [kafka-producer-network-thread | confluent-telemetry-reporter-local-producer] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-telemetry-reporter-local-producer] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:08,885 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,886 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-7 in /mnt/data/data0/logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,886 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-7 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-7
[INFO] 2023-11-08 14:51:08,886 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-7 broker=2] Log loaded for partition __consumer_offsets-7 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,887 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,893 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-22, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,894 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-22 in /mnt/data/data0/logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,894 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-22 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-22
[INFO] 2023-11-08 14:51:08,894 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-22 broker=2] Log loaded for partition __consumer_offsets-22 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,894 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,900 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,901 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-10 in /mnt/data/data0/logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,901 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-10 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-10
[INFO] 2023-11-08 14:51:08,901 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-10 broker=2] Log loaded for partition __consumer_offsets-10 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,901 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,907 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-31, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,908 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-31 in /mnt/data/data0/logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,908 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-31 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-31
[INFO] 2023-11-08 14:51:08,908 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-31 broker=2] Log loaded for partition __consumer_offsets-31 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,908 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,916 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-46, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,917 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-46 in /mnt/data/data0/logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,918 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-46 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-46
[INFO] 2023-11-08 14:51:08,918 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-46 broker=2] Log loaded for partition __consumer_offsets-46 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,918 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,923 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,924 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-1 in /mnt/data/data0/logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,924 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-1 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-1
[INFO] 2023-11-08 14:51:08,924 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-1 broker=2] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,924 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,931 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-16, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,932 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-16 in /mnt/data/data0/logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,932 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-16 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-16
[INFO] 2023-11-08 14:51:08,932 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-16 broker=2] Log loaded for partition __consumer_offsets-16 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,932 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,938 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-19, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,939 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-19 in /mnt/data/data0/logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,939 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-19 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-19
[INFO] 2023-11-08 14:51:08,940 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-19 broker=2] Log loaded for partition __consumer_offsets-19 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,940 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,945 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-34, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,946 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-34 in /mnt/data/data0/logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,946 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-34 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-34
[INFO] 2023-11-08 14:51:08,947 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-34 broker=2] Log loaded for partition __consumer_offsets-34 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,947 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:51:08,952 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[INFO] 2023-11-08 14:51:08,952 [metadata-service-coordinator] org.apache.kafka.clients.NetworkClient disconnect - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Client requested disconnect from node 2147483647
[INFO] 2023-11-08 14:51:08,953 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,954 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-4 in /mnt/data/data0/logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,954 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-4 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-4
[INFO] 2023-11-08 14:51:08,954 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-4 broker=2] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,954 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:51:08,958 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068958, latencyMs=6, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-2, correlationId=12), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=0, host='kafka-0.kafka.confluent.svc.cluster.local', port=9072, errorCode=0, errorMessage='')]))
[INFO] 2023-11-08 14:51:08,959 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Discovered group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:08,959 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted.
[INFO] 2023-11-08 14:51:08,959 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Requesting disconnect from last known coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:08,959 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-25, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,960 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-25 in /mnt/data/data0/logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,960 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-25 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-25
[INFO] 2023-11-08 14:51:08,960 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-25 broker=2] Log loaded for partition __consumer_offsets-25 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,961 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,966 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-40, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,967 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-40 in /mnt/data/data0/logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,968 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-40 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-40
[INFO] 2023-11-08 14:51:08,968 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-40 broker=2] Log loaded for partition __consumer_offsets-40 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,968 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,973 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-43, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,974 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-43 in /mnt/data/data0/logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,974 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-43 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-43
[INFO] 2023-11-08 14:51:08,974 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-43 broker=2] Log loaded for partition __consumer_offsets-43 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,974 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,980 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-13, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,982 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-13 in /mnt/data/data0/logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,982 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-13 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-13
[INFO] 2023-11-08 14:51:08,982 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-13 broker=2] Log loaded for partition __consumer_offsets-13 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,982 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,987 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-28, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,989 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-28 in /mnt/data/data0/logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,989 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-28 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-28
[INFO] 2023-11-08 14:51:08,989 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-28 broker=2] Log loaded for partition __consumer_offsets-28 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,989 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,994 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-49, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,995 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-49 in /mnt/data/data0/logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,995 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-49 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-49
[INFO] 2023-11-08 14:51:08,995 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-49 broker=2] Log loaded for partition __consumer_offsets-49 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,996 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,006 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,007 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-3 in /mnt/data/data0/logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,007 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-3 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-3
[INFO] 2023-11-08 14:51:09,007 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-3 broker=2] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,008 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,014 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-18, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,015 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-18 in /mnt/data/data0/logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,015 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-18 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-18
[INFO] 2023-11-08 14:51:09,016 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-18 broker=2] Log loaded for partition __consumer_offsets-18 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,016 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,021 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-41, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,023 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-41 in /mnt/data/data0/logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,023 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-41 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-41
[INFO] 2023-11-08 14:51:09,023 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-41 broker=2] Log loaded for partition __consumer_offsets-41 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,023 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,028 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-29, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,030 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-29 in /mnt/data/data0/logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,030 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-29 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-29
[INFO] 2023-11-08 14:51:09,030 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-29 broker=2] Log loaded for partition __consumer_offsets-29 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,030 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,035 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-44, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,037 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-44 in /mnt/data/data0/logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,037 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-44 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-44
[INFO] 2023-11-08 14:51:09,037 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-44 broker=2] Log loaded for partition __consumer_offsets-44 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,037 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,042 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-14, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,043 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-14 in /mnt/data/data0/logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,043 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-14 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-14
[INFO] 2023-11-08 14:51:09,043 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-14 broker=2] Log loaded for partition __consumer_offsets-14 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,044 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,049 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-33, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,050 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-33 in /mnt/data/data0/logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,050 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-33 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-33
[INFO] 2023-11-08 14:51:09,050 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-33 broker=2] Log loaded for partition __consumer_offsets-33 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,050 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,055 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-48, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,056 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-48 in /mnt/data/data0/logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,056 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-48 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-48
[INFO] 2023-11-08 14:51:09,056 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-48 broker=2] Log loaded for partition __consumer_offsets-48 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,056 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:51:09,059 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[INFO] 2023-11-08 14:51:09,061 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-23, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[DEBUG] 2023-11-08 14:51:09,062 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455069061, latencyMs=2, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-2, correlationId=13), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=0, host='kafka-0.kafka.confluent.svc.cluster.local', port=9072, errorCode=0, errorMessage='')]))
[INFO] 2023-11-08 14:51:09,062 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Discovered group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,062 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-23 in /mnt/data/data0/logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,062 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-23 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-23
[INFO] 2023-11-08 14:51:09,062 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-23 broker=2] Log loaded for partition __consumer_offsets-23 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,062 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,063 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group](Re-)joining group
[DEBUG] 2023-11-08 14:51:09,063 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending JoinGroup (JoinGroupRequestData(groupId='_confluent-metadata-coordinator-group', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='metadata-service', protocols=[JoinGroupRequestProtocol(name='v0', metadata=[123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 50, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125])])) to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,069 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-38, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,070 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-38 in /mnt/data/data0/logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,070 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-38 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-38
[INFO] 2023-11-08 14:51:09,070 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-38 broker=2] Log loaded for partition __consumer_offsets-38 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,070 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,075 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,076 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-8 in /mnt/data/data0/logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,076 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-8 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-8
[INFO] 2023-11-08 14:51:09,077 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-8 broker=2] Log loaded for partition __consumer_offsets-8 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,077 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,082 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,083 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-11 in /mnt/data/data0/logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,083 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-11 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-11
[INFO] 2023-11-08 14:51:09,083 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-11 broker=2] Log loaded for partition __consumer_offsets-11 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,083 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,086 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted.
[INFO] 2023-11-08 14:51:09,086 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Requesting disconnect from last known coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,086 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
[INFO] 2023-11-08 14:51:09,088 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-26, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,089 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-26 in /mnt/data/data0/logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,089 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-26 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-26
[INFO] 2023-11-08 14:51:09,089 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-26 broker=2] Log loaded for partition __consumer_offsets-26 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,089 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,094 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-45, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,095 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-45 in /mnt/data/data0/logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,095 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-45 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-45
[INFO] 2023-11-08 14:51:09,095 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-45 broker=2] Log loaded for partition __consumer_offsets-45 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,095 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,101 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-15, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,101 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-15 in /mnt/data/data0/logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,102 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-15 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-15
[INFO] 2023-11-08 14:51:09,102 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-15 broker=2] Log loaded for partition __consumer_offsets-15 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,102 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,107 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-30, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,108 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-30 in /mnt/data/data0/logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,108 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-30 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-30
[INFO] 2023-11-08 14:51:09,108 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-30 broker=2] Log loaded for partition __consumer_offsets-30 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,109 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,114 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,115 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-0 in /mnt/data/data0/logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,115 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-0 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-0
[INFO] 2023-11-08 14:51:09,115 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-0 broker=2] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,115 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,121 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-35, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,122 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-35 in /mnt/data/data0/logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,122 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-35 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-35
[INFO] 2023-11-08 14:51:09,123 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-35 broker=2] Log loaded for partition __consumer_offsets-35 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,123 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,128 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,129 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-5 in /mnt/data/data0/logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,129 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-5 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-5
[INFO] 2023-11-08 14:51:09,129 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-5 broker=2] Log loaded for partition __consumer_offsets-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,129 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,134 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-20, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,135 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-20 in /mnt/data/data0/logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,135 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-20 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-20
[INFO] 2023-11-08 14:51:09,135 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-20 broker=2] Log loaded for partition __consumer_offsets-20 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,135 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,140 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-39, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,140 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-39 in /mnt/data/data0/logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,140 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-39 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-39
[INFO] 2023-11-08 14:51:09,140 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-39 broker=2] Log loaded for partition __consumer_offsets-39 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,141 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,146 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,147 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-9 in /mnt/data/data0/logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,147 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-9 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-9
[INFO] 2023-11-08 14:51:09,147 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-9 broker=2] Log loaded for partition __consumer_offsets-9 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,147 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,152 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-24, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,153 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-24 in /mnt/data/data0/logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,153 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-24 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-24
[INFO] 2023-11-08 14:51:09,153 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-24 broker=2] Log loaded for partition __consumer_offsets-24 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,153 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,157 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-27, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,158 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-27 in /mnt/data/data0/logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,158 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-27 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-27
[INFO] 2023-11-08 14:51:09,158 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-27 broker=2] Log loaded for partition __consumer_offsets-27 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,158 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,162 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-42, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,163 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-42 in /mnt/data/data0/logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,163 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-42 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-42
[INFO] 2023-11-08 14:51:09,163 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-42 broker=2] Log loaded for partition __consumer_offsets-42 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,163 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,167 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-12, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,168 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-12 in /mnt/data/data0/logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,168 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-12 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-12
[INFO] 2023-11-08 14:51:09,168 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-12 broker=2] Log loaded for partition __consumer_offsets-12 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,168 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,172 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,173 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-2 in /mnt/data/data0/logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,173 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-2 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-2
[INFO] 2023-11-08 14:51:09,173 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-2 broker=2] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,173 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,177 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-21, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,177 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-21 in /mnt/data/data0/logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,178 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-21 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-21
[INFO] 2023-11-08 14:51:09,178 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-21 broker=2] Log loaded for partition __consumer_offsets-21 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,178 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,182 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-36, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,183 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-36 in /mnt/data/data0/logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,183 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-36 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-36
[INFO] 2023-11-08 14:51:09,183 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-36 broker=2] Log loaded for partition __consumer_offsets-36 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,183 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:51:09,186 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[INFO] 2023-11-08 14:51:09,186 [metadata-service-coordinator] org.apache.kafka.clients.NetworkClient disconnect - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Client requested disconnect from node 2147483647
[INFO] 2023-11-08 14:51:09,188 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,189 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-6 in /mnt/data/data0/logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[DEBUG] 2023-11-08 14:51:09,189 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455069189, latencyMs=3, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-2, correlationId=16), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=0, host='kafka-0.kafka.confluent.svc.cluster.local', port=9072, errorCode=0, errorMessage='')]))
[INFO] 2023-11-08 14:51:09,189 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-6 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-6
[INFO] 2023-11-08 14:51:09,189 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Discovered group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,189 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-6 broker=2] Log loaded for partition __consumer_offsets-6 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,189 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted.
[INFO] 2023-11-08 14:51:09,189 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Requesting disconnect from last known coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,189 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,194 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-47, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,195 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-47 in /mnt/data/data0/logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,195 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-47 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-47
[INFO] 2023-11-08 14:51:09,195 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-47 broker=2] Log loaded for partition __consumer_offsets-47 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,195 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,199 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-17, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,200 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-17 in /mnt/data/data0/logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,200 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-17 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-17
[INFO] 2023-11-08 14:51:09,200 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-17 broker=2] Log loaded for partition __consumer_offsets-17 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,200 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,205 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-32, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,206 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition __consumer_offsets-32 in /mnt/data/data0/logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,206 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-32 broker=2] No checkpointed highwatermark is found for partition __consumer_offsets-32
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition __consumer_offsets-32 broker=2] Log loaded for partition __consumer_offsets-32 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-11, __consumer_offsets-44, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-23, __consumer_offsets-21, __consumer_offsets-17, __consumer_offsets-32, __consumer_offsets-30, __consumer_offsets-26, __consumer_offsets-5, __consumer_offsets-38, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-47, __consumer_offsets-45, __consumer_offsets-14, __consumer_offsets-12, __consumer_offsets-41, __consumer_offsets-24, __consumer_offsets-20, __consumer_offsets-18, __consumer_offsets-0, __consumer_offsets-29, __consumer_offsets-27, __consumer_offsets-39, __consumer_offsets-8, __consumer_offsets-6, __consumer_offsets-35, __consumer_offsets-33, __consumer_offsets-2)
[INFO] 2023-11-08 14:51:09,210 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 1 for 33 partitions
[INFO] 2023-11-08 14:51:09,243 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Starting
[INFO] 2023-11-08 14:51:09,247 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(__consumer_offsets-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-35 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-47 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-38 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-17 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-29 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-32 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-41 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-23 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-14 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-20 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-44 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-26 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:09,249 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-47 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,251 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-47, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,252 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(__consumer_offsets-30 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-21 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-33 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-36 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-48 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-45 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-27 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-42 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-18 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-15 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-24 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-39 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-12 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:09,252 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Starting
[INFO] 2023-11-08 14:51:09,253 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,253 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-15, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,256 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,256 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-14, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,256 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,256 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,256 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-48, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,256 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,257 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,257 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-44 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,257 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-44, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,257 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-45, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,257 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-41 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,257 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-41, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,257 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,258 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,258 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-12, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,258 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-23, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,258 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,258 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,258 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,258 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-20, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-42, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-17, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-32 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-24, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-32, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-29 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-21, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,259 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-29, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-26 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-18, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-26, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,260 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-30, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-38 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-27, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-38, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-35 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-39, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-35, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition __consumer_offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,261 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,262 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,262 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,262 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-36, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,262 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,262 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-33, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,272 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 37 in epoch 0
[INFO] 2023-11-08 14:51:09,273 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-37 for epoch 0
[INFO] 2023-11-08 14:51:09,274 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 7 in epoch 0
[INFO] 2023-11-08 14:51:09,274 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-7 for epoch 0
[INFO] 2023-11-08 14:51:09,274 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 22 in epoch 0
[INFO] 2023-11-08 14:51:09,274 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-22 for epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 10 in epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-10 for epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 31 in epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-31 for epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 46 in epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-46 for epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 1 in epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-1 for epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 16 in epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-16 for epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 19 in epoch 0
[INFO] 2023-11-08 14:51:09,275 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-19 for epoch 0
[INFO] 2023-11-08 14:51:09,276 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 34 in epoch 0
[INFO] 2023-11-08 14:51:09,276 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-34 for epoch 0
[INFO] 2023-11-08 14:51:09,276 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 4 in epoch 0
[INFO] 2023-11-08 14:51:09,276 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-4 for epoch 0
[INFO] 2023-11-08 14:51:09,276 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-37 is aborted and paused
[INFO] 2023-11-08 14:51:09,279 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 25 in epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-25 for epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 40 in epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-40 for epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 43 in epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-43 for epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 13 in epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-13 for epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 28 in epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-28 for epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Elected as the group coordinator for partition 49 in epoch 0
[INFO] 2023-11-08 14:51:09,280 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling loading of offsets and group metadata from __consumer_offsets-49 for epoch 0
[INFO] 2023-11-08 14:51:09,282 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 3 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,282 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-3
[INFO] 2023-11-08 14:51:09,283 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 18 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,283 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-18
[INFO] 2023-11-08 14:51:09,283 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 41 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,283 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-41
[INFO] 2023-11-08 14:51:09,283 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 29 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,284 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-29
[INFO] 2023-11-08 14:51:09,284 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 44 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,284 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-44
[INFO] 2023-11-08 14:51:09,284 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 14 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,284 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-14
[INFO] 2023-11-08 14:51:09,284 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 33 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,284 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-33
[INFO] 2023-11-08 14:51:09,284 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 48 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,284 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-48
[INFO] 2023-11-08 14:51:09,285 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 23 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,285 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-23
[INFO] 2023-11-08 14:51:09,285 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 38 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,285 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-38
[INFO] 2023-11-08 14:51:09,285 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 8 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,285 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-8
[INFO] 2023-11-08 14:51:09,285 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 11 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,285 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-11
[INFO] 2023-11-08 14:51:09,285 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 26 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,286 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-26
[INFO] 2023-11-08 14:51:09,286 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 45 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,286 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-45
[INFO] 2023-11-08 14:51:09,286 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 15 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,286 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-15
[INFO] 2023-11-08 14:51:09,286 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 30 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,286 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-30
[INFO] 2023-11-08 14:51:09,286 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 0 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,286 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-0
[INFO] 2023-11-08 14:51:09,286 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 35 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-35
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 5 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-5
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 20 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-20
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 39 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-39
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 9 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-9
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 24 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,288 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-24
[INFO] 2023-11-08 14:51:09,288 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 27 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,288 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-27
[INFO] 2023-11-08 14:51:09,288 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 42 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,288 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-42
[INFO] 2023-11-08 14:51:09,288 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 12 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,288 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-12
[INFO] 2023-11-08 14:51:09,288 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 2 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,288 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-2
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 21 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-21
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 36 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-36
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 6 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-6
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 47 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-47
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 17 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-17
[INFO] 2023-11-08 14:51:09,290 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Resigned as the group coordinator for partition 32 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,290 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Scheduling unloading of offsets and group metadata from __consumer_offsets-32
[DEBUG] 2023-11-08 14:51:09,290 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[DEBUG] 2023-11-08 14:51:09,292 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455069292, latencyMs=2, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-2, correlationId=17), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=0, host='kafka-0.kafka.confluent.svc.cluster.local', port=9072, errorCode=0, errorMessage='')]))
[INFO] 2023-11-08 14:51:09,293 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Discovered group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,292 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-37 in 17 milliseconds for epoch 0, of which 3 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,294 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group](Re-)joining group
[DEBUG] 2023-11-08 14:51:09,294 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending JoinGroup (JoinGroupRequestData(groupId='_confluent-metadata-coordinator-group', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='metadata-service', protocols=[JoinGroupRequestProtocol(name='v0', metadata=[123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 50, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125])])) to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,295 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-37 is resumed
[INFO] 2023-11-08 14:51:09,295 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-7 is aborted and paused
[INFO] 2023-11-08 14:51:09,296 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 738ms correlationId 1 from controller 1 for 50 partitions
[INFO] 2023-11-08 14:51:09,296 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-7 in 22 milliseconds for epoch 0, of which 22 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,296 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-7 is resumed
[INFO] 2023-11-08 14:51:09,296 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-22 is aborted and paused
[INFO] 2023-11-08 14:51:09,297 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-22 in 23 milliseconds for epoch 0, of which 22 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,297 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-22 is resumed
[INFO] 2023-11-08 14:51:09,297 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-10 is aborted and paused
[INFO] 2023-11-08 14:51:09,297 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-10 in 22 milliseconds for epoch 0, of which 22 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,298 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-10 is resumed
[INFO] 2023-11-08 14:51:09,298 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-31 is aborted and paused
[INFO] 2023-11-08 14:51:09,298 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-31 in 23 milliseconds for epoch 0, of which 23 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,298 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-31 is resumed
[INFO] 2023-11-08 14:51:09,299 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-46 is aborted and paused
[INFO] 2023-11-08 14:51:09,299 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-46 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,299 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-46 is resumed
[INFO] 2023-11-08 14:51:09,299 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-1 is aborted and paused
[INFO] 2023-11-08 14:51:09,300 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-1 in 25 milliseconds for epoch 0, of which 25 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,300 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-1 is resumed
[INFO] 2023-11-08 14:51:09,300 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-16 is aborted and paused
[INFO] 2023-11-08 14:51:09,300 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-16 in 25 milliseconds for epoch 0, of which 25 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,301 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-16 is resumed
[INFO] 2023-11-08 14:51:09,301 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-19 is aborted and paused
[INFO] 2023-11-08 14:51:09,301 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-19 in 26 milliseconds for epoch 0, of which 26 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,301 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-19 is resumed
[INFO] 2023-11-08 14:51:09,302 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-34 is aborted and paused
[INFO] 2023-11-08 14:51:09,302 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-34 in 26 milliseconds for epoch 0, of which 26 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,302 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-34 is resumed
[INFO] 2023-11-08 14:51:09,302 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-4 is aborted and paused
[INFO] 2023-11-08 14:51:09,303 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-4 in 27 milliseconds for epoch 0, of which 26 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,303 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-4 is resumed
[INFO] 2023-11-08 14:51:09,303 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-25 is aborted and paused
[INFO] 2023-11-08 14:51:09,303 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-25 in 23 milliseconds for epoch 0, of which 23 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,303 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-25 is resumed
[INFO] 2023-11-08 14:51:09,304 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-40 is aborted and paused
[INFO] 2023-11-08 14:51:09,304 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-40 in 24 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,304 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-40 is resumed
[INFO] 2023-11-08 14:51:09,304 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-43 is aborted and paused
[INFO] 2023-11-08 14:51:09,305 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-43 in 25 milliseconds for epoch 0, of which 24 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,305 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-43 is resumed
[INFO] 2023-11-08 14:51:09,306 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-13 is aborted and paused
[INFO] 2023-11-08 14:51:09,306 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-13 in 26 milliseconds for epoch 0, of which 26 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,306 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-13 is resumed
[INFO] 2023-11-08 14:51:09,306 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-28 is aborted and paused
[INFO] 2023-11-08 14:51:09,307 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-28 in 27 milliseconds for epoch 0, of which 26 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,307 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-28 is resumed
[INFO] 2023-11-08 14:51:09,307 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-49 is aborted and paused
[INFO] 2023-11-08 14:51:09,307 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished loading offsets and group metadata from __consumer_offsets-49 in 27 milliseconds for epoch 0, of which 27 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,307 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-49 is resumed
[INFO] 2023-11-08 14:51:09,309 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-3 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,310 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-18 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,310 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-41 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,310 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-29 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,310 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-44 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,310 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-14 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,311 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-33 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,311 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-48 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,311 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-23 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,311 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-38 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,311 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-8 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,311 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-11 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,312 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-26 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,312 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-45 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,312 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-15 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,312 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-30 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,313 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2
[INFO] 2023-11-08 14:51:09,318 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-0 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,319 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-35 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,319 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-5 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,319 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-20 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,319 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-39 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,320 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-9 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,320 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-24 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,320 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-27 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,320 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-42 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,321 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-12 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,321 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-2 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,321 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-21 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,321 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-36 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,322 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-6 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,322 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-47 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,322 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-17 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,322 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=2] Finished unloading __consumer_offsets-32 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[DEBUG] 2023-11-08 14:51:09,332 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]JoinGroup failed due to non-fatal error: MEMBER_ID_REQUIRED Will set the member id as _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb and then rejoin. Sent generation was  Generation{generationId=-1, memberId='', protocol='null'}
[INFO] 2023-11-08 14:51:09,332 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator requestRejoin - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Request joining group due to: need to re-join with the given member-id
[INFO] 2023-11-08 14:51:09,332 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group](Re-)joining group
[DEBUG] 2023-11-08 14:51:09,333 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending JoinGroup (JoinGroupRequestData(groupId='_confluent-metadata-coordinator-group', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='_confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb', groupInstanceId=null, protocolType='metadata-service', protocols=[JoinGroupRequestProtocol(name='v0', metadata=[123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 50, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125])])) to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,370 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 3 from controller 1 for 64 partitions
[INFO] 2023-11-08 14:51:09,428 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent_balancer_broker_samples-15, _confluent_balancer_broker_samples-30, _confluent_balancer_partition_samples-29, _confluent_balancer_partition_samples-26, _confluent_balancer_broker_samples-0, _confluent_balancer_broker_samples-9, _confluent_balancer_broker_samples-18, _confluent_balancer_partition_samples-14, _confluent_balancer_broker_samples-12, _confluent_balancer_broker_samples-27, _confluent_balancer_broker_samples-6, _confluent_balancer_partition_samples-23, _confluent_balancer_partition_samples-20, _confluent_balancer_partition_samples-11, _confluent_balancer_broker_samples-24, _confluent_balancer_partition_samples-2, _confluent_balancer_broker_samples-21, _confluent_balancer_partition_samples-8, _confluent_balancer_broker_samples-3, _confluent_balancer_partition_samples-5, _confluent_balancer_partition_samples-17)
[INFO] 2023-11-08 14:51:09,429 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 1 epoch 1 as part of the become-leader transition for 21 partitions
[INFO] 2023-11-08 14:51:09,436 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-27, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,438 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-27 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-27 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,439 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-27 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-27
[INFO] 2023-11-08 14:51:09,441 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-27 broker=2] Log loaded for partition _confluent_balancer_broker_samples-27 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,441 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,451 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-12, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,452 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-12 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-12 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,452 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-12 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-12
[INFO] 2023-11-08 14:51:09,452 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-12 broker=2] Log loaded for partition _confluent_balancer_broker_samples-12 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,452 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,458 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-26, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,459 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-26 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-26 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,459 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-26 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-26
[INFO] 2023-11-08 14:51:09,459 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-26 broker=2] Log loaded for partition _confluent_balancer_partition_samples-26 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,459 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_partition_samples-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,465 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,466 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-11 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-11 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,466 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-11 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-11
[INFO] 2023-11-08 14:51:09,466 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-11 broker=2] Log loaded for partition _confluent_balancer_partition_samples-11 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,466 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_partition_samples-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,473 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-21, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,474 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-21 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-21 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,474 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-21 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-21
[INFO] 2023-11-08 14:51:09,474 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-21 broker=2] Log loaded for partition _confluent_balancer_broker_samples-21 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,475 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,480 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,482 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-6 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-6 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,482 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-6 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-6
[INFO] 2023-11-08 14:51:09,482 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-6 broker=2] Log loaded for partition _confluent_balancer_broker_samples-6 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,482 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,488 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,489 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-3 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-3 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,489 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-3 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-3
[INFO] 2023-11-08 14:51:09,489 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-3 broker=2] Log loaded for partition _confluent_balancer_broker_samples-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,489 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,495 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,496 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-2 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-2 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,496 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-2 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-2
[INFO] 2023-11-08 14:51:09,497 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-2 broker=2] Log loaded for partition _confluent_balancer_partition_samples-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,498 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_partition_samples-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,503 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-18, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,504 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-18 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-18 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,504 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-18 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-18
[INFO] 2023-11-08 14:51:09,504 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-18 broker=2] Log loaded for partition _confluent_balancer_broker_samples-18 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,504 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,512 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-17, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,513 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-17 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-17 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,513 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-17 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-17
[INFO] 2023-11-08 14:51:09,513 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-17 broker=2] Log loaded for partition _confluent_balancer_partition_samples-17 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,513 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_partition_samples-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,519 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-29, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,520 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-29 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-29 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,520 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-29 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-29
[INFO] 2023-11-08 14:51:09,521 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-29 broker=2] Log loaded for partition _confluent_balancer_partition_samples-29 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,521 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_partition_samples-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,526 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,527 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-9 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-9 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,527 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-9 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-9
[INFO] 2023-11-08 14:51:09,527 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-9 broker=2] Log loaded for partition _confluent_balancer_broker_samples-9 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,527 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,532 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,532 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-8 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-8 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,533 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-8 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-8
[INFO] 2023-11-08 14:51:09,533 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-8 broker=2] Log loaded for partition _confluent_balancer_partition_samples-8 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,533 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_partition_samples-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,539 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-24, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,540 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-24 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-24 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,540 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-24 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-24
[INFO] 2023-11-08 14:51:09,541 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-24 broker=2] Log loaded for partition _confluent_balancer_broker_samples-24 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,541 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,549 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-23, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,550 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-23 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-23 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,550 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-23 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-23
[INFO] 2023-11-08 14:51:09,550 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-23 broker=2] Log loaded for partition _confluent_balancer_partition_samples-23 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,550 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_partition_samples-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,555 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-20, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,556 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-20 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-20 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,557 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-20 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-20
[INFO] 2023-11-08 14:51:09,557 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-20 broker=2] Log loaded for partition _confluent_balancer_partition_samples-20 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,557 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_partition_samples-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,562 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,562 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-5 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-5 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,562 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-5 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-5
[INFO] 2023-11-08 14:51:09,563 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-5 broker=2] Log loaded for partition _confluent_balancer_partition_samples-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,563 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_partition_samples-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,567 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-15, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,567 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-15 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-15 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,568 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-15 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-15
[INFO] 2023-11-08 14:51:09,568 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-15 broker=2] Log loaded for partition _confluent_balancer_broker_samples-15 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,568 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,572 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,572 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-0 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-0 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,573 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-0 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-0
[INFO] 2023-11-08 14:51:09,573 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-0 broker=2] Log loaded for partition _confluent_balancer_broker_samples-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,573 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,577 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-14, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,578 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-14 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-14 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,578 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-14 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-14
[INFO] 2023-11-08 14:51:09,578 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-14 broker=2] Log loaded for partition _confluent_balancer_partition_samples-14 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,578 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_partition_samples-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,582 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-30, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,583 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-30 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-30 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,583 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-30 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-30
[INFO] 2023-11-08 14:51:09,584 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-30 broker=2] Log loaded for partition _confluent_balancer_broker_samples-30 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,584 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent_balancer_broker_samples-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,591 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-25, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,591 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-25 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-25 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,592 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-25 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-25
[INFO] 2023-11-08 14:51:09,592 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-25 broker=2] Log loaded for partition _confluent_balancer_broker_samples-25 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,592 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-25 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,597 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,598 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-10 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-10 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,599 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-10 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-10
[INFO] 2023-11-08 14:51:09,599 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-10 broker=2] Log loaded for partition _confluent_balancer_broker_samples-10 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,599 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,603 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-22, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,604 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-22 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-22 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,605 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-22 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-22
[INFO] 2023-11-08 14:51:09,605 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-22 broker=2] Log loaded for partition _confluent_balancer_partition_samples-22 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,605 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-22 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,610 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,611 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-7 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-7 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,611 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-7 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-7
[INFO] 2023-11-08 14:51:09,611 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-7 broker=2] Log loaded for partition _confluent_balancer_partition_samples-7 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,611 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,616 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,617 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-4 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-4 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,617 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-4 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-4
[INFO] 2023-11-08 14:51:09,618 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-4 broker=2] Log loaded for partition _confluent_balancer_partition_samples-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,618 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,622 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-19, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,623 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-19 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-19 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,623 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-19 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-19
[INFO] 2023-11-08 14:51:09,623 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-19 broker=2] Log loaded for partition _confluent_balancer_partition_samples-19 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,623 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-19 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,628 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,629 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-0 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-0 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,629 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-0 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-0
[INFO] 2023-11-08 14:51:09,629 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-0 broker=2] Log loaded for partition _confluent_balancer_partition_samples-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,629 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,634 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-30, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,635 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-30 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-30 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,635 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-30 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-30
[INFO] 2023-11-08 14:51:09,635 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-30 broker=2] Log loaded for partition _confluent_balancer_partition_samples-30 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,635 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-30 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,640 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-29, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,641 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-29 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-29 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,641 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-29 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-29
[INFO] 2023-11-08 14:51:09,641 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-29 broker=2] Log loaded for partition _confluent_balancer_broker_samples-29 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,641 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-29 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,645 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-15, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,646 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-15 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-15 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,646 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-15 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-15
[INFO] 2023-11-08 14:51:09,646 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-15 broker=2] Log loaded for partition _confluent_balancer_partition_samples-15 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,646 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-15 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,650 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-14, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,651 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-14 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-14 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,651 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-14 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-14
[INFO] 2023-11-08 14:51:09,651 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-14 broker=2] Log loaded for partition _confluent_balancer_broker_samples-14 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,651 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-14 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,655 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-12, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,655 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-12 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-12 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,655 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-12 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-12
[INFO] 2023-11-08 14:51:09,655 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-12 broker=2] Log loaded for partition _confluent_balancer_partition_samples-12 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,655 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-12 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,659 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-28, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,659 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-28 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-28 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,659 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-28 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-28
[INFO] 2023-11-08 14:51:09,659 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-28 broker=2] Log loaded for partition _confluent_balancer_broker_samples-28 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,660 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-28 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,664 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-27, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,665 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-27 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-27 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,665 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-27 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-27
[INFO] 2023-11-08 14:51:09,665 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-27 broker=2] Log loaded for partition _confluent_balancer_partition_samples-27 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,665 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-27 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,669 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,669 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-5 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-5 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,669 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-5 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-5
[INFO] 2023-11-08 14:51:09,669 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-5 broker=2] Log loaded for partition _confluent_balancer_broker_samples-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,669 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,673 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-17, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,674 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-17 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-17 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,674 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-17 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-17
[INFO] 2023-11-08 14:51:09,674 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-17 broker=2] Log loaded for partition _confluent_balancer_broker_samples-17 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,674 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-17 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,678 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,679 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-2 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-2 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,679 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-2 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-2
[INFO] 2023-11-08 14:51:09,679 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-2 broker=2] Log loaded for partition _confluent_balancer_broker_samples-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,679 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,682 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-16, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,683 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-16 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-16 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,683 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-16 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-16
[INFO] 2023-11-08 14:51:09,683 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-16 broker=2] Log loaded for partition _confluent_balancer_partition_samples-16 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,683 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-16 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,686 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,687 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-1 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-1 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,687 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-1 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-1
[INFO] 2023-11-08 14:51:09,687 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-1 broker=2] Log loaded for partition _confluent_balancer_partition_samples-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,687 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,691 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-13, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,691 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-13 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-13 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,691 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-13 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-13
[INFO] 2023-11-08 14:51:09,691 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-13 broker=2] Log loaded for partition _confluent_balancer_broker_samples-13 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,692 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-13 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,695 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-31, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,696 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-31 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-31 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,696 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-31 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-31
[INFO] 2023-11-08 14:51:09,696 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-31 broker=2] Log loaded for partition _confluent_balancer_partition_samples-31 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,696 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-31 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,699 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-28, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,700 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-28 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-28 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,700 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-28 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-28
[INFO] 2023-11-08 14:51:09,700 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-28 broker=2] Log loaded for partition _confluent_balancer_partition_samples-28 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,700 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-28 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,704 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-13, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,704 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-13 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-13 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,704 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-13 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-13
[INFO] 2023-11-08 14:51:09,704 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-13 broker=2] Log loaded for partition _confluent_balancer_partition_samples-13 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,705 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-13 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,708 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-24, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,709 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-24 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-24 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,709 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-24 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-24
[INFO] 2023-11-08 14:51:09,709 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-24 broker=2] Log loaded for partition _confluent_balancer_partition_samples-24 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,709 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-24 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,713 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-23, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,713 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-23 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-23 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,713 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-23 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-23
[INFO] 2023-11-08 14:51:09,713 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-23 broker=2] Log loaded for partition _confluent_balancer_broker_samples-23 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,713 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-23 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,717 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,717 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-9 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-9 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,718 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-9 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-9
[INFO] 2023-11-08 14:51:09,718 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-9 broker=2] Log loaded for partition _confluent_balancer_partition_samples-9 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,718 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,721 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,722 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-8 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-8 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,722 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-8 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-8
[INFO] 2023-11-08 14:51:09,722 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-8 broker=2] Log loaded for partition _confluent_balancer_broker_samples-8 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,722 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,726 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-21, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,726 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-21 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-21 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,727 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-21 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-21
[INFO] 2023-11-08 14:51:09,727 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-21 broker=2] Log loaded for partition _confluent_balancer_partition_samples-21 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,727 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-21 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,730 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-20, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,731 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-20 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-20 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,731 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-20 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-20
[INFO] 2023-11-08 14:51:09,731 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-20 broker=2] Log loaded for partition _confluent_balancer_broker_samples-20 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,731 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-20 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,735 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,736 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-1 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-1 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,736 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-1 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-1
[INFO] 2023-11-08 14:51:09,736 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-1 broker=2] Log loaded for partition _confluent_balancer_broker_samples-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,736 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,739 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-31, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,740 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-31 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-31 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,740 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-31 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-31
[INFO] 2023-11-08 14:51:09,740 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-31 broker=2] Log loaded for partition _confluent_balancer_broker_samples-31 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,740 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-31 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,744 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-16, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,745 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-16 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-16 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,745 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-16 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-16
[INFO] 2023-11-08 14:51:09,745 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-16 broker=2] Log loaded for partition _confluent_balancer_broker_samples-16 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,745 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-16 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,749 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,749 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-11 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-11 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,749 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-11 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-11
[INFO] 2023-11-08 14:51:09,749 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-11 broker=2] Log loaded for partition _confluent_balancer_broker_samples-11 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,750 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,753 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,754 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-10 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-10 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,754 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-10 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-10
[INFO] 2023-11-08 14:51:09,754 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-10 broker=2] Log loaded for partition _confluent_balancer_partition_samples-10 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,754 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,757 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-26, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,757 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-26 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-26 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,758 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-26 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-26
[INFO] 2023-11-08 14:51:09,758 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-26 broker=2] Log loaded for partition _confluent_balancer_broker_samples-26 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,758 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-26 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,762 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,763 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-7 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-7 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,763 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-7 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-7
[INFO] 2023-11-08 14:51:09,763 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-7 broker=2] Log loaded for partition _confluent_balancer_broker_samples-7 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,763 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,767 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-25, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,768 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-25 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-25 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,768 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-25 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-25
[INFO] 2023-11-08 14:51:09,768 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-25 broker=2] Log loaded for partition _confluent_balancer_partition_samples-25 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,768 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-25 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,772 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,773 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-6 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-6 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,773 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-6 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-6
[INFO] 2023-11-08 14:51:09,773 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-6 broker=2] Log loaded for partition _confluent_balancer_partition_samples-6 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,773 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,776 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-22, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,777 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-22 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-22 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,777 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-22 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-22
[INFO] 2023-11-08 14:51:09,777 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-22 broker=2] Log loaded for partition _confluent_balancer_broker_samples-22 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,777 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-22 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,819 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-19, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,820 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-19 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-19 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,820 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-19 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-19
[INFO] 2023-11-08 14:51:09,821 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-19 broker=2] Log loaded for partition _confluent_balancer_broker_samples-19 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,821 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-19 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,825 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,826 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-4 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-4 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,826 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-4 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-4
[INFO] 2023-11-08 14:51:09,826 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-4 broker=2] Log loaded for partition _confluent_balancer_broker_samples-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,826 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_broker_samples-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,830 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-18, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,830 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-18 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-18 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,831 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-18 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-18
[INFO] 2023-11-08 14:51:09,831 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-18 broker=2] Log loaded for partition _confluent_balancer_partition_samples-18 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,831 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-18 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,835 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,836 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-3 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-3 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,836 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-3 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-3
[INFO] 2023-11-08 14:51:09,836 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-3 broker=2] Log loaded for partition _confluent_balancer_partition_samples-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,836 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent_balancer_partition_samples-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,838 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent_balancer_broker_samples-23, _confluent_balancer_broker_samples-25, _confluent_balancer_partition_samples-7, _confluent_balancer_partition_samples-9, _confluent_balancer_broker_samples-29, _confluent_balancer_partition_samples-30, _confluent_balancer_broker_samples-17, _confluent_balancer_broker_samples-19, _confluent_balancer_partition_samples-1, _confluent_balancer_partition_samples-3, _confluent_balancer_partition_samples-22, _confluent_balancer_partition_samples-24, _confluent_balancer_broker_samples-8, _confluent_balancer_broker_samples-10, _confluent_balancer_partition_samples-28, _confluent_balancer_broker_samples-31, _confluent_balancer_partition_samples-16, _confluent_balancer_partition_samples-18, _confluent_balancer_broker_samples-2, _confluent_balancer_broker_samples-4, _confluent_balancer_partition_samples-6, _confluent_balancer_broker_samples-22, _confluent_balancer_partition_samples-10, _confluent_balancer_broker_samples-26, _confluent_balancer_partition_samples-12, _confluent_balancer_broker_samples-28, _confluent_balancer_broker_samples-14, _confluent_balancer_partition_samples-0, _confluent_balancer_partition_samples-31, _confluent_balancer_broker_samples-16, _confluent_balancer_partition_samples-4, _confluent_balancer_broker_samples-20, _confluent_balancer_broker_samples-7, _confluent_balancer_partition_samples-21, _confluent_balancer_broker_samples-11, _confluent_balancer_partition_samples-25, _confluent_balancer_broker_samples-13, _confluent_balancer_partition_samples-27, _confluent_balancer_partition_samples-13, _confluent_balancer_broker_samples-1, _confluent_balancer_partition_samples-15, _confluent_balancer_broker_samples-5, _confluent_balancer_partition_samples-19)
[INFO] 2023-11-08 14:51:09,838 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 3 for 43 partitions
[INFO] 2023-11-08 14:51:09,841 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent_balancer_partition_samples-27 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-13 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-16 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-28 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-31 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-18 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-24 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-12 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-21 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-25 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-30 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-22 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-19 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-15 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:09,841 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent_balancer_broker_samples-17 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-28 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-13 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-26 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-19 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-22 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-25 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-20 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-29 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-31 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-14 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-23 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-16 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:09,842 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 472ms correlationId 3 from controller 1 for 64 partitions
[INFO] 2023-11-08 14:51:09,846 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Add 64 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-22 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-22, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-25 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-25, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-12 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-12, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-28 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,865 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-28, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-30 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-30, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-16 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-16, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-19 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-19, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-21 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-21, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-24 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-24, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,866 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-13 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-13, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-27 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-27, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-31 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-31, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-15 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-15, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-18 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-18, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,867 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,341 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-23 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,341 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-23, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,341 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-26 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-26, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-29 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-29, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-14 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-14, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-17 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-17, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-31 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-31, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,342 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-20 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-20, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-22 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-22, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-25 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-25, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-28 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-28, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-13 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,343 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-13, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,344 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-16 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,344 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-16, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,344 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,344 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,344 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,344 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:10,344 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-19 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:10,344 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-19, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[DEBUG] 2023-11-08 14:51:15,340 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful JoinGroup response: JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='metadata-service', protocolName='v0', leader='_confluent-metadata-coordinator-1-042022d4-5dfd-4955-aa0c-2ff1512f68b3', memberId='_confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb', members=[])
[DEBUG] 2023-11-08 14:51:15,340 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator enable - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Enabling heartbeat thread
[INFO] 2023-11-08 14:51:15,340 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Successfully joined group with generation Generation{generationId=1, memberId='_confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb', protocol='v0'}
[DEBUG] 2023-11-08 14:51:15,340 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onJoinFollower - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending follower SyncGroup to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null) at generation Generation{generationId=1, memberId='_confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb', protocol='v0'}: SyncGroupRequestData(groupId='_confluent-metadata-coordinator-group', generationId=1, memberId='_confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb', groupInstanceId=null, protocolType='metadata-service', protocolName='v0', assignments=[])
[DEBUG] 2023-11-08 14:51:15,567 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful SyncGroup response: SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='metadata-service', protocolName='v0', assignment=[123, 34, 118, 101, 114, 115, 105, 111, 110, 34, 58, 48, 44, 34, 101, 114, 114, 111, 114, 34, 58, 48, 44, 34, 110, 111, 100, 101, 115, 34, 58, 123, 34, 95, 99, 111, 110, 102, 108, 117, 101, 110, 116, 45, 109, 101, 116, 97, 100, 97, 116, 97, 45, 99, 111, 111, 114, 100, 105, 110, 97, 116, 111, 114, 45, 49, 45, 48, 52, 50, 48, 50, 50, 100, 52, 45, 53, 100, 102, 100, 45, 52, 57, 53, 53, 45, 97, 97, 48, 99, 45, 50, 102, 102, 49, 53, 49, 50, 102, 54, 56, 98, 51, 34, 58, 123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 49, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125, 44, 34, 95, 99, 111, 110, 102, 108, 117, 101, 110, 116, 45, 109, 101, 116, 97, 100, 97, 116, 97, 45, 99, 111, 111, 114, 100, 105, 110, 97, 116, 111, 114, 45, 48, 45, 100, 99, 57, 101, 98, 102, 53, 102, 45, 48, 49, 52, 52, 45, 52, 57, 55, 55, 45, 97, 101, 99, 56, 45, 98, 102, 56, 100, 55, 55, 53, 97, 101, 56, 52, 100, 34, 58, 123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 48, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125, 44, 34, 95, 99, 111, 110, 102, 108, 117, 101, 110, 116, 45, 109, 101, 116, 97, 100, 97, 116, 97, 45, 99, 111, 111, 114, 100, 105, 110, 97, 116, 111, 114, 45, 50, 45, 48, 51, 52, 50, 52, 56, 97, 97, 45, 99, 98, 55, 53, 45, 52, 53, 101, 97, 45, 57, 54, 99, 53, 45, 101, 100, 57, 98, 49, 48, 48, 97, 57, 97, 100, 98, 34, 58, 123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 50, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125, 125, 44, 34, 119, 114, 105, 116, 101, 114, 77, 101, 109, 98, 101, 114, 73, 100, 34, 58, 34, 95, 99, 111, 110, 102, 108, 117, 101, 110, 116, 45, 109, 101, 116, 97, 100, 97, 116, 97, 45, 99, 111, 111, 114, 100, 105, 110, 97, 116, 111, 114, 45, 48, 45, 100, 99, 57, 101, 98, 102, 53, 102, 45, 48, 49, 52, 52, 45, 52, 57, 55, 55, 45, 97, 101, 99, 56, 45, 98, 102, 56, 100, 55, 55, 53, 97, 101, 56, 52, 100, 34, 44, 34, 119, 114, 105, 116, 101, 114, 78, 111, 100, 101, 77, 101, 116, 97, 100, 97, 116, 97, 34, 58, 123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 48, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125, 125])
[INFO] 2023-11-08 14:51:15,567 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Successfully synced group in generation Generation{generationId=1, memberId='_confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb', protocol='v0'}
[INFO] 2023-11-08 14:51:15,576 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager onAssigned - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Metadata writer assignment complete: generation MetadataServiceAssignment(error=0, nodes={_confluent-metadata-coordinator-1-042022d4-5dfd-4955-aa0c-2ff1512f68b3=NodeMetadata(urls={https=https://kafka-1.kafka.confluent.svc.cluster.local:8090}), _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d=NodeMetadata(urls={https=https://kafka-0.kafka.confluent.svc.cluster.local:8090}), _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb=NodeMetadata(urls={https=https://kafka-2.kafka.confluent.svc.cluster.local:8090})}, writerMemberId='_confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d', writerNodeMetadata=NodeMetadata(urls={https=https://kafka-0.kafka.confluent.svc.cluster.local:8090}), version=0) assignment 1
[DEBUG] 2023-11-08 14:51:15,577 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager run - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Wake up exception from poll
[INFO] 2023-11-08 14:51:15,578 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager lambda$onAssigned$2 - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Not starting writer from node NodeMetadata(urls={https=https://kafka-2.kafka.confluent.svc.cluster.local:8090}) since new writer is NodeMetadata(urls={https=https://kafka-0.kafka.confluent.svc.cluster.local:8090}) with generation 1
[INFO] 2023-11-08 14:51:15,728 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 5 from controller 1 for 6 partitions
[INFO] 2023-11-08 14:51:15,735 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-metadata-auth-3, _confluent-metadata-auth-0)
[INFO] 2023-11-08 14:51:15,736 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 1 epoch 1 as part of the become-leader transition for 2 partitions
[INFO] 2023-11-08 14:51:15,742 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,743 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-3 in /mnt/data/data0/logs/_confluent-metadata-auth-3 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,744 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-3 broker=2] No checkpointed highwatermark is found for partition _confluent-metadata-auth-3
[INFO] 2023-11-08 14:51:15,745 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-3 broker=2] Log loaded for partition _confluent-metadata-auth-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,745 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-metadata-auth-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,749 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,750 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-0 in /mnt/data/data0/logs/_confluent-metadata-auth-0 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,750 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-0 broker=2] No checkpointed highwatermark is found for partition _confluent-metadata-auth-0
[INFO] 2023-11-08 14:51:15,750 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-0 broker=2] Log loaded for partition _confluent-metadata-auth-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,750 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-metadata-auth-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,755 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,756 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-5 in /mnt/data/data0/logs/_confluent-metadata-auth-5 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,756 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-5 broker=2] No checkpointed highwatermark is found for partition _confluent-metadata-auth-5
[INFO] 2023-11-08 14:51:15,756 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-5 broker=2] Log loaded for partition _confluent-metadata-auth-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,756 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-metadata-auth-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,760 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,760 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-4 in /mnt/data/data0/logs/_confluent-metadata-auth-4 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,760 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-4 broker=2] No checkpointed highwatermark is found for partition _confluent-metadata-auth-4
[INFO] 2023-11-08 14:51:15,760 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-4 broker=2] Log loaded for partition _confluent-metadata-auth-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,761 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-metadata-auth-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,765 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[DEBUG] 2023-11-08 14:51:15,765 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaUtils waitForTopic - Topic _confluent-metadata-auth has the expected 6 partitions, returning
[INFO] 2023-11-08 14:51:15,765 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-1 in /mnt/data/data0/logs/_confluent-metadata-auth-1 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,766 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-1 broker=2] No checkpointed highwatermark is found for partition _confluent-metadata-auth-1
[INFO] 2023-11-08 14:51:15,766 [auth-reader-1] org.apache.kafka.clients.consumer.KafkaConsumer assign - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Subscribed to partition(s): _confluent-metadata-auth-1, _confluent-metadata-auth-0, _confluent-metadata-auth-3, _confluent-metadata-auth-2, _confluent-metadata-auth-5, _confluent-metadata-auth-4
[INFO] 2023-11-08 14:51:15,766 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-1 broker=2] Log loaded for partition _confluent-metadata-auth-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,766 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-metadata-auth-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,770 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,771 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-1
[INFO] 2023-11-08 14:51:15,771 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-2 in /mnt/data/data0/logs/_confluent-metadata-auth-2 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,772 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-2 broker=2] No checkpointed highwatermark is found for partition _confluent-metadata-auth-2
[INFO] 2023-11-08 14:51:15,772 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-0
[INFO] 2023-11-08 14:51:15,772 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-2 broker=2] Log loaded for partition _confluent-metadata-auth-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,772 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-3
[INFO] 2023-11-08 14:51:15,772 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-metadata-auth-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,772 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-2
[INFO] 2023-11-08 14:51:15,772 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-5
[INFO] 2023-11-08 14:51:15,772 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-4
[INFO] 2023-11-08 14:51:15,772 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-metadata-auth-1, _confluent-metadata-auth-2, _confluent-metadata-auth-5, _confluent-metadata-auth-4)
[INFO] 2023-11-08 14:51:15,772 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 5 for 4 partitions
[INFO] 2023-11-08 14:51:15,773 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions Map(_confluent-metadata-auth-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metadata-auth-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:15,774 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions Map(_confluent-metadata-auth-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metadata-auth-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:15,775 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 47ms correlationId 5 from controller 1 for 6 partitions
[INFO] 2023-11-08 14:51:15,779 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Add 6 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6
[DEBUG] 2023-11-08 14:51:15,826 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader initialize - auth topic partitions : [_confluent-metadata-auth-1, _confluent-metadata-auth-0, _confluent-metadata-auth-3, _confluent-metadata-auth-2, _confluent-metadata-auth-5, _confluent-metadata-auth-4]
[INFO] 2023-11-08 14:51:15,826 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-1
[INFO] 2023-11-08 14:51:15,826 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-0
[INFO] 2023-11-08 14:51:15,826 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-3
[INFO] 2023-11-08 14:51:15,826 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-2
[INFO] 2023-11-08 14:51:15,826 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-5
[INFO] 2023-11-08 14:51:15,826 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-2, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-4
[INFO] 2023-11-08 14:51:15,882 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-metadata-auth-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:15,882 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metadata-auth-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:15,882 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-metadata-auth-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:15,883 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metadata-auth-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[DEBUG] 2023-11-08 14:51:15,962 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader run - preparing latest auth record entries, size : 0
[DEBUG] 2023-11-08 14:51:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:0 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:0 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:15,968 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,968 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:1 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,978 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,978 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:1 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,978 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,978 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:2 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,980 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,980 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:2 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[INFO] 2023-11-08 14:51:16,024 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-metadata-auth-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,025 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metadata-auth-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,025 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-metadata-auth-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,025 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metadata-auth-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[DEBUG] 2023-11-08 14:51:16,081 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,081 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:0 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:16,085 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,085 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:0 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:16,095 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,095 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:1 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,095 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,095 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:2 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,097 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,097 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:1 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,097 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,097 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:2 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,165 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,165 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:0 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:16,169 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,169 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:0 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:16,181 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,181 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:1 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,181 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,181 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:2 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,184 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,184 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:1 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[INFO] 2023-11-08 14:51:16,184 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader lambda$null$0 - Kafka Auth reader initialized on all partitions
[INFO] 2023-11-08 14:51:16,185 [auth-reader-1] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-2.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[INFO] 2023-11-08 14:51:16,185 [auth-reader-1] io.confluent.security.store.kafka.KafkaStoreConfig logAll - KafkaStoreConfig values: 
	confluent.metadata.refresh.timeout.ms = 60000
	confluent.metadata.retry.timeout.ms = 86400000
	confluent.metadata.topic.create.timeout.ms = 600000
	confluent.metadata.topic.replication.factor = 3

[INFO] 2023-11-08 14:51:16,187 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'auto.create.topics.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.recovery.threads.per.data.dir' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.server.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.mechanism.inter.broker.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'default.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.session.uuid' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,231 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.operator.managed' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'transaction.state.log.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.io.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'delete.topic.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.clientCnxnSocket' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.client.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.login.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.commit.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'authorizer.class.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,232 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'transaction.state.log.min.isr' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.request.max.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.retention.minutes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'inter.broker.protocol.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.security.protocol.map' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.retention.hours' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.partitions' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'inter.broker.listener.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,233 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.balancer.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'node.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.message.format.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,234 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.segment.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'group.max.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.dirs' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replica.lag.time.max.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.network.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.send.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,235 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.receive.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.rack' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.topic.compression.codec' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.retention.check.interval.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:16,236 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455076236
[INFO] 2023-11-08 14:51:16,238 [auth-reader-1] io.confluent.security.store.kafka.KafkaStoreConfig logAll - KafkaStoreConfig values: 
	confluent.metadata.refresh.timeout.ms = 60000
	confluent.metadata.retry.timeout.ms = 86400000
	confluent.metadata.topic.create.timeout.ms = 600000
	confluent.metadata.topic.replication.factor = 3

[INFO] 2023-11-08 14:51:16,239 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'auto.create.topics.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,242 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.recovery.threads.per.data.dir' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.server.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.mechanism.inter.broker.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'default.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.session.uuid' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.operator.managed' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'transaction.state.log.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.io.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'delete.topic.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.clientCnxnSocket' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.client.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,243 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.login.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.commit.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'authorizer.class.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'transaction.state.log.min.isr' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.request.max.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.retention.minutes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'inter.broker.protocol.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.security.protocol.map' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.retention.hours' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,244 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.partitions' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'inter.broker.listener.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.balancer.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'node.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,245 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.message.format.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.segment.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'group.max.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.dirs' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replica.lag.time.max.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,246 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.network.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.send.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.receive.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.rack' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.topic.compression.codec' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.retention.check.interval.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:16,247 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455076247
[WARN] 2023-11-08 14:51:16,248 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser registerAppInfo - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=_confluent-metadata-admin-2
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:655)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:592)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:143)
	at io.confluent.security.auth.provider.ConfluentProvider.createMdsAdminClient(ConfluentProvider.java:336)
	at io.confluent.security.auth.provider.ConfluentProvider.lambda$start$0(ConfluentProvider.java:242)
	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at io.confluent.security.store.kafka.clients.KafkaReader.lambda$null$0(KafkaReader.java:96)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at io.confluent.security.store.kafka.clients.KafkaReader$PartitionState.onConsume(KafkaReader.java:310)
	at io.confluent.security.store.kafka.clients.KafkaReader.processConsumerRecord(KafkaReader.java:283)
	at java.base/java.lang.Iterable.forEach(Iterable.java:75)
	at io.confluent.security.store.kafka.clients.KafkaReader.run(KafkaReader.java:217)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:16,250 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,250 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:2 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[INFO] 2023-11-08 14:51:16,251 [audit-init-1] io.confluent.security.audit.AuditLogConfig logAll - AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 

[INFO] 2023-11-08 14:51:16,254 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=2] Started data-plane acceptor and processor(s) for endpoint : ListenerName(INTERNAL)
[INFO] 2023-11-08 14:51:16,259 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=2] Started data-plane acceptor and processor(s) for endpoint : ListenerName(TOKEN)
[INFO] 2023-11-08 14:51:16,262 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=2] Started data-plane acceptor and processor(s) for endpoint : ListenerName(EXTERNAL)
[INFO] 2023-11-08 14:51:16,263 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=2] Started socket server acceptors and processors
[INFO] 2023-11-08 14:51:16,279 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:16,282 [audit-init-1] io.confluent.telemetry.events.EventLoggerConfig logAll - EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter

[INFO] 2023-11-08 14:51:16,285 [main] io.confluent.http.server.KafkaHttpServerConfig logAll - KafkaHttpServerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:16,291 [audit-init-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporterConfig logAll - NonBlockingKafkaExporterConfig values: 
	event.logger.exporter.kafka.blocking = false
	event.logger.exporter.kafka.bootstrap.servers = kafka-2.kafka.confluent.svc.cluster.local:9072
	event.logger.exporter.kafka.request.timeout.ms = 12000
	event.logger.exporter.kafka.topic.config = {"topics":[{"name":"confluent-audit-log-events","partitions":0,"replicationFactor":0,"config":{"retention.ms":"7776000000"}}]}
	event.logger.exporter.kafka.topic.create = true
	event.logger.exporter.kafka.topic.partitions = 12
	event.logger.exporter.kafka.topic.replicas = 3
	event.logger.exporter.kafka.topic.retention.bytes = -1
	event.logger.exporter.kafka.topic.retention.ms = 2592000000
	event.logger.exporter.kafka.topic.roll.ms = 14400000

[INFO] 2023-11-08 14:51:16,292 [audit-init-1] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-event-logger
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 0
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[INFO] 2023-11-08 14:51:16,295 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:16,295 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:16,295 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455076295
[INFO] 2023-11-08 14:51:16,296 [audit-init-1] io.confluent.telemetry.events.EventLoggerConfig logAll - EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter

[INFO] 2023-11-08 14:51:16,300 [audit-init-1] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:51:16,302 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:16,302 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:16,302 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455076302
[INFO] 2023-11-08 14:51:16,305 [audit-init-1] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-event-logger
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 0
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[INFO] 2023-11-08 14:51:16,314 [kafka-producer-network-thread | confluent-event-logger] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-event-logger] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:16,318 [main] io.confluent.rest.RestConfig logAll - RestConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = []
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8080
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = 
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = 
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:16,327 [main] org.eclipse.jetty.util.log initialized - Logging initialized @24453ms to org.eclipse.jetty.util.log.Slf4jLog
[INFO] 2023-11-08 14:51:16,364 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 7 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:51:16,365 [main] io.confluent.http.server.KafkaHttpApplicationLoader load - Application provider 'MetadataApiApplicationProvider' provided 1 instance(s).
[INFO] 2023-11-08 14:51:16,371 [audit-init-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - all topics exist
[INFO] 2023-11-08 14:51:16,372 [audit-init-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - Event logger is waiting for metadata for topics: [confluent-audit-log-events]
[INFO] 2023-11-08 14:51:16,377 [main] io.confluent.rbacapi.app.RbacApiAppConfig logAll - RbacApiAppConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	api.flavor = CP
	authentication.method = BEARER
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	cluster.registry.clusters = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	openapi.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	user.store = LDAP
	user.store.file.path = 
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:16,378 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(confluent-audit-log-events-4, confluent-audit-log-events-1, confluent-audit-log-events-10, confluent-audit-log-events-7)
[INFO] 2023-11-08 14:51:16,378 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[WARN] 2023-11-08 14:51:16,390 [kafka-producer-network-thread | confluent-event-logger] org.apache.kafka.clients.NetworkClient handleSuccessfulResponse - [Producer clientId=confluent-event-logger] Error while fetching metadata with correlation id 3 : {confluent-audit-log-events=UNKNOWN_TOPIC_OR_PARTITION}
[INFO] 2023-11-08 14:51:16,391 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,391 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-4 in /mnt/data/data0/logs/confluent-audit-log-events-4 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,392 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-4 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-4
[INFO] 2023-11-08 14:51:16,393 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-4 broker=2] Log loaded for partition confluent-audit-log-events-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,393 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader confluent-audit-log-events-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,397 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,398 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-1 in /mnt/data/data0/logs/confluent-audit-log-events-1 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,398 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-1 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-1
[INFO] 2023-11-08 14:51:16,398 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-1 broker=2] Log loaded for partition confluent-audit-log-events-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,398 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader confluent-audit-log-events-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,403 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,404 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-10 in /mnt/data/data0/logs/confluent-audit-log-events-10 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,404 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-10 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-10
[INFO] 2023-11-08 14:51:16,404 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-10 broker=2] Log loaded for partition confluent-audit-log-events-10 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,404 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader confluent-audit-log-events-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,408 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,409 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-7 in /mnt/data/data0/logs/confluent-audit-log-events-7 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,409 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-7 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-7
[INFO] 2023-11-08 14:51:16,409 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-7 broker=2] Log loaded for partition confluent-audit-log-events-7 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,410 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader confluent-audit-log-events-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,414 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,415 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-2 in /mnt/data/data0/logs/confluent-audit-log-events-2 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,415 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-2 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-2
[INFO] 2023-11-08 14:51:16,415 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-2 broker=2] Log loaded for partition confluent-audit-log-events-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,416 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower confluent-audit-log-events-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,428 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,429 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-3 in /mnt/data/data0/logs/confluent-audit-log-events-3 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,429 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-3 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-3
[INFO] 2023-11-08 14:51:16,429 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-3 broker=2] Log loaded for partition confluent-audit-log-events-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,429 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower confluent-audit-log-events-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,440 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,441 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-5 in /mnt/data/data0/logs/confluent-audit-log-events-5 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,441 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-5 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-5
[INFO] 2023-11-08 14:51:16,441 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-5 broker=2] Log loaded for partition confluent-audit-log-events-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,442 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower confluent-audit-log-events-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,454 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,455 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-0 in /mnt/data/data0/logs/confluent-audit-log-events-0 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,455 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-0 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-0
[INFO] 2023-11-08 14:51:16,455 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-0 broker=2] Log loaded for partition confluent-audit-log-events-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,456 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower confluent-audit-log-events-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,466 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,467 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-11 in /mnt/data/data0/logs/confluent-audit-log-events-11 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,467 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-11 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-11
[INFO] 2023-11-08 14:51:16,467 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-11 broker=2] Log loaded for partition confluent-audit-log-events-11 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,467 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower confluent-audit-log-events-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,476 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,476 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-6 in /mnt/data/data0/logs/confluent-audit-log-events-6 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,477 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-6 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-6
[INFO] 2023-11-08 14:51:16,477 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-6 broker=2] Log loaded for partition confluent-audit-log-events-6 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,477 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower confluent-audit-log-events-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,481 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,529 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-8 in /mnt/data/data0/logs/confluent-audit-log-events-8 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,529 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-8 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-8
[INFO] 2023-11-08 14:51:16,529 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-8 broker=2] Log loaded for partition confluent-audit-log-events-8 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,529 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower confluent-audit-log-events-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,544 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,545 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-9 in /mnt/data/data0/logs/confluent-audit-log-events-9 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,545 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-9 broker=2] No checkpointed highwatermark is found for partition confluent-audit-log-events-9
[INFO] 2023-11-08 14:51:16,545 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition confluent-audit-log-events-9 broker=2] Log loaded for partition confluent-audit-log-events-9 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,545 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower confluent-audit-log-events-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,545 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(confluent-audit-log-events-0, confluent-audit-log-events-3, confluent-audit-log-events-2, confluent-audit-log-events-8, confluent-audit-log-events-6, confluent-audit-log-events-5, confluent-audit-log-events-11, confluent-audit-log-events-9)
[INFO] 2023-11-08 14:51:16,545 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 7 for 8 partitions
[INFO] 2023-11-08 14:51:16,546 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(confluent-audit-log-events-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:16,546 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(confluent-audit-log-events-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:16,547 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 183ms correlationId 7 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:51:16,550 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8
[INFO] 2023-11-08 14:51:16,558 [main] org.hibernate.validator.internal.util.Version <clinit> - HV000001: Hibernate Validator 6.1.7.Final
[INFO] 2023-11-08 14:51:16,593 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent-audit-log-events-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,593 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,593 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent-audit-log-events-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,593 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,593 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent-audit-log-events-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,594 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,594 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent-audit-log-events-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,594 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,947 [main] io.confluent.auditlogapi.kafka.DestinationTopicManagerConfig logAll - DestinationTopicManagerConfig values: 
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 

[INFO] 2023-11-08 14:51:17,031 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent-audit-log-events-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:17,031 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:17,031 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent-audit-log-events-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:17,031 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:17,032 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent-audit-log-events-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:17,032 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:17,032 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent-audit-log-events-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:17,032 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:17,056 [main] io.confluent.http.server.KafkaHttpApplicationLoader load - Application provider 'RbacApplicationProvider' provided 1 instance(s).
[INFO] 2023-11-08 14:51:17,059 [main] io.confluent.http.server.KafkaHttpServerConfig logAll - KafkaHttpServerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:17,067 [main] io.confluent.http.server.KafkaHttpServerConfig logAll - KafkaHttpServerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:17,068 [main] kafka.server.KafkaConfig logAll - KafkaConfig values: 
	advertised.listeners = EXTERNAL://rb2.my.domain:9092,INTERNAL://kafka-2.kafka.confluent.svc.cluster.local:9071,REPLICATION://kafka-2.kafka.confluent.svc.cluster.local:9072,TOKEN://kafka-2.kafka.confluent.svc.cluster.local:9073
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 2
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = 2
	broker.session.timeout.ms = 9000
	broker.session.uuid = GLYD9RvdRwqiJGkc5Hmbsw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 120
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = true
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.reporting.enable = false
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1200000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = REPLICATION
	inter.broker.protocol.version = 2.6
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = EXTERNAL:SASL_SSL,INTERNAL:SASL_SSL,REPLICATION:SASL_SSL,TOKEN:SASL_SSL
	listeners = EXTERNAL://:9092,INTERNAL://:9071,REPLICATION://:9072,TOKEN://:9073
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /mnt/data/data0/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.6
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	node.id = 2
	num.io.threads = 8
	num.network.threads = 4
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 15000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 3
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.coordinator.enable = false
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 45000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [PLAIN]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = org.apache.zookeeper.ClientCnxnSocketNetty
	zookeeper.connect = zookeeper.confluent.svc.cluster.local:2182/kafka-confluent
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 22500
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = true
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	zookeeper.ssl.truststore.password = [hidden]
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[INFO] 2023-11-08 14:51:17,076 [main] io.confluent.kafkarest.KafkaRestConfig logAll - KafkaRestConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	advertised.listeners = []
	api.endpoints.allowlist = []
	api.endpoints.blocklist = []
	api.v2.enable = false
	api.v3.enable = true
	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
	api.v3.produce.rate.limit.enabled = false
	api.v3.produce.rate.limit.grace.period.ms = 30000
	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
	api.v3.produce.rate.limit.max.requests.per.sec = 10000
	api.v3.produce.response.thread.pool.size = 16
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	bootstrap.servers = kafka.confluent.svc.cluster.local:9073
	client.init.timeout.ms = 60000
	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	client.sasl.kerberos.min.time.before.relogin = 60000
	client.sasl.kerberos.service.name = 
	client.sasl.kerberos.ticket.renew.jitter = 0.05
	client.sasl.kerberos.ticket.renew.window.factor = 0.8
	client.sasl.mechanism = OAUTHBEARER
	client.security.protocol = SASL_SSL
	client.ssl.cipher.suites = 
	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
	client.ssl.endpoint.identification.algorithm = 
	client.ssl.key.password = [hidden]
	client.ssl.keymanager.algorithm = SunX509
	client.ssl.keystore.location = 
	client.ssl.keystore.password = [hidden]
	client.ssl.keystore.type = JKS
	client.ssl.protocol = TLS
	client.ssl.provider = 
	client.ssl.trustmanager.algorithm = PKIX
	client.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	client.ssl.truststore.password = [hidden]
	client.ssl.truststore.type = JKS
	client.timeout.ms = 500
	client.zk.session.timeout.ms = 30000
	compression.enable = true
	confluent.resource.name.authority = 
	consumer.instance.timeout.ms = 300000
	consumer.iterator.backoff.ms = 50
	consumer.iterator.timeout.ms = 1
	consumer.request.max.bytes = 67108864
	consumer.request.timeout.ms = 1000
	consumer.threads = 50
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	fetch.min.bytes = -1
	host.name = 
	http2.enabled = true
	id = 
	idle.timeout.ms = 30000
	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension, io.confluent.kafkarest.security.KafkaRestSecurityResourceExtension]
	listener.protocol.map = []
	listeners = []
	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.jmx.prefix = kafka.rest
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8082
	producer.threads = 5
	proxy.protocol.enabled = false
	rate.limit.backend = guava
	rate.limit.costs = 
	rate.limit.default.cost = 1
	rate.limit.enable = false
	rate.limit.permits.per.sec = 50
	rate.limit.timeout.ms = 0
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
	rest.servlet.initializor.classes = [io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler]
	schema.registry.url = http://localhost:8081
	shutdown.graceful.ms = 1000
	simpleconsumer.pool.size.max = 25
	simpleconsumer.pool.timeout.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []
	zookeeper.connect = 

[INFO] 2023-11-08 14:51:17,077 [main] io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig logAll - EventEmitterConfig values: 

[INFO] 2023-11-08 14:51:17,077 [main] io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig logAll - EventEmitterConfig values: 

[INFO] 2023-11-08 14:51:17,078 [main] io.confluent.telemetry.ConfluentTelemetryConfig logAll - ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

[INFO] 2023-11-08 14:51:17,078 [main] io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig logAll - VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

[INFO] 2023-11-08 14:51:17,079 [main] io.confluent.telemetry.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[WARN] 2023-11-08 14:51:17,079 [main] io.confluent.telemetry.ConfluentTelemetryConfig <init> - no telemetry exporters are enabled
[WARN] 2023-11-08 14:51:17,079 [main] io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade withLabel - Ignoring redefinition of existing telemetry label kafka_rest.version
[INFO] 2023-11-08 14:51:17,080 [main] io.confluent.telemetry.ConfluentTelemetryConfig logAll - ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

[INFO] 2023-11-08 14:51:17,080 [main] io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig logAll - VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

[INFO] 2023-11-08 14:51:17,080 [main] io.confluent.telemetry.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[WARN] 2023-11-08 14:51:17,080 [main] io.confluent.telemetry.ConfluentTelemetryConfig <init> - no telemetry exporters are enabled
[INFO] 2023-11-08 14:51:17,080 [main] io.confluent.telemetry.reporter.TelemetryReporter initEventLogger - Initializing the event logger
[INFO] 2023-11-08 14:51:17,080 [main] io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig logAll - EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter

[INFO] 2023-11-08 14:51:17,081 [main] io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[INFO] 2023-11-08 14:51:17,084 [main] io.confluent.telemetry.reporter.TelemetryReporter startMetricCollectorTask - Starting Confluent telemetry reporter with an interval of 60000 ms for resource = (type = kafka_rest)
[INFO] 2023-11-08 14:51:17,089 [main] io.confluent.http.server.KafkaHttpApplicationLoader load - Application provider 'KafkaRestApplicationProvider' provided 1 instance(s).
[INFO] 2023-11-08 14:51:17,098 [main] io.confluent.rest.ApplicationServer createThreadPool - Initial capacity 128, increased by 64, maximum capacity 2147483647.
[INFO] 2023-11-08 14:51:17,240 [main] io.confluent.rest.FileWatcher onFileChange - Configure watch file change: /mnt/sslcerts/keystore.jks
[INFO] 2023-11-08 14:51:17,242 [main] io.confluent.rest.ApplicationServer createSslContextFactory - Enabled SSL cert auto reload for: /mnt/sslcerts/keystore.jks
[INFO] 2023-11-08 14:51:17,262 [main] io.confluent.rest.ApplicationServer getConnectionFactories - Adding listener with HTTP/2: https://0.0.0.0:8090
[INFO] 2023-11-08 14:51:17,308 [main] io.confluent.kafka.http.server.KafkaHttpServerLoader load - Loaded KafkaHttpServer implementation class io.confluent.http.server.KafkaHttpServerImpl
[INFO] 2023-11-08 14:51:17,309 [main] io.confluent.http.server.KafkaHttpServerImpl transition - KafkaHttpServer transitioned from NEW to STARTING..
[INFO] 2023-11-08 14:51:17,373 [audit-init-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - Event logger has metadata for all topics
[INFO] 2023-11-08 14:51:17,409 [ce-kafka-http-server-start-thread] io.confluent.rest.Application configureHandler - Binding MetadataApiApplication to all listeners.
[INFO] 2023-11-08 14:51:17,522 [ce-kafka-http-server-start-thread] io.confluent.tokenapi.jwt.JwtConfig logAll - JwtConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	api.flavor = CP
	authentication.method = BEARER
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	cluster.registry.clusters = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	openapi.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	token.issuer = Confluent
	token.max.lifetime.ms = 3600000
	token.roles = clusters
	user.store = LDAP
	user.store.file.path = 
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:17,524 [ce-kafka-http-server-start-thread] io.confluent.tokenapi.jwt.JwsConfig logAll - JwsConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	api.flavor = CP
	authentication.method = BEARER
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	cluster.registry.clusters = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	openapi.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	token.issuer = Confluent
	token.key.path = /mnt/secrets/mds-token/mdsTokenKeyPair.pem
	token.max.lifetime.ms = 3600000
	token.roles = clusters
	token.signature.algorithm = RS256
	user.store = LDAP
	user.store.file.path = 
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:17,593 [ce-kafka-http-server-start-thread] io.confluent.rest.Application configureHandler - Binding RbacApiApplication to all listeners.
[WARN] 2023-11-08 14:51:17,686 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[INFO] 2023-11-08 14:51:17,687 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.config.SchemaRegistryConfig logAll - SchemaRegistryConfig values: 
	auto.register.schemas = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[INFO] 2023-11-08 14:51:17,711 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 9 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:51:17,733 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.security.config.ConfluentSecureKafkaRestConfig logAll - ConfluentSecureKafkaRestConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	advertised.listeners = []
	api.endpoints.allowlist = []
	api.endpoints.blocklist = []
	api.v2.enable = false
	api.v3.enable = true
	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
	api.v3.produce.rate.limit.enabled = false
	api.v3.produce.rate.limit.grace.period.ms = 30000
	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
	api.v3.produce.rate.limit.max.requests.per.sec = 10000
	api.v3.produce.response.thread.pool.size = 16
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	bootstrap.servers = kafka.confluent.svc.cluster.local:9073
	client.init.timeout.ms = 60000
	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	client.sasl.kerberos.min.time.before.relogin = 60000
	client.sasl.kerberos.service.name = 
	client.sasl.kerberos.ticket.renew.jitter = 0.05
	client.sasl.kerberos.ticket.renew.window.factor = 0.8
	client.sasl.mechanism = OAUTHBEARER
	client.security.protocol = SASL_SSL
	client.ssl.cipher.suites = 
	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
	client.ssl.endpoint.identification.algorithm = 
	client.ssl.key.password = [hidden]
	client.ssl.keymanager.algorithm = SunX509
	client.ssl.keystore.location = 
	client.ssl.keystore.password = [hidden]
	client.ssl.keystore.type = JKS
	client.ssl.protocol = TLS
	client.ssl.provider = 
	client.ssl.trustmanager.algorithm = PKIX
	client.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	client.ssl.truststore.password = [hidden]
	client.ssl.truststore.type = JKS
	client.timeout.ms = 500
	client.zk.session.timeout.ms = 30000
	compression.enable = true
	confluent.license = 
	confluent.license.topic = _confluent-license
	confluent.metadata.bootstrap.server.urls = https://kafka.confluent.svc.cluster.local:8090
	confluent.resource.name.authority = 
	confluent.rest.auth.propagate.method = SSL
	confluent.rest.auth.ssl.principal.mapping.rules = DEFAULT
	consumer.instance.timeout.ms = 300000
	consumer.iterator.backoff.ms = 50
	consumer.iterator.timeout.ms = 1
	consumer.request.max.bytes = 67108864
	consumer.request.timeout.ms = 1000
	consumer.threads = 50
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	fetch.min.bytes = -1
	host.name = 
	http2.enabled = true
	id = 
	idle.timeout.ms = 30000
	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension, io.confluent.kafkarest.security.KafkaRestSecurityResourceExtension]
	listener.protocol.map = []
	listeners = []
	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.jmx.prefix = kafka.rest
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8082
	producer.threads = 5
	proxy.protocol.enabled = false
	rate.limit.backend = guava
	rate.limit.costs = 
	rate.limit.default.cost = 1
	rate.limit.enable = false
	rate.limit.permits.per.sec = 50
	rate.limit.timeout.ms = 0
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
	rest.servlet.initializor.classes = [io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler]
	schema.registry.url = http://localhost:8081
	shutdown.graceful.ms = 1000
	simpleconsumer.pool.size.max = 25
	simpleconsumer.pool.timeout.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []
	zookeeper.connect = 

[INFO] 2023-11-08 14:51:17,757 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-command-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:17,758 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-command-0 in /mnt/data/data0/logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=2}
[INFO] 2023-11-08 14:51:17,763 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-command-0 broker=2] No checkpointed highwatermark is found for partition _confluent-command-0
[INFO] 2023-11-08 14:51:17,764 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-command-0 broker=2] Log loaded for partition _confluent-command-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:17,764 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-command-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:17,764 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-command-0)
[INFO] 2023-11-08 14:51:17,765 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 9 for 1 partitions
[INFO] 2023-11-08 14:51:17,765 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions Map(_confluent-command-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:17,766 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 55ms correlationId 9 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:51:17,774 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10
[WARN] 2023-11-08 14:51:17,776 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[INFO] 2023-11-08 14:51:17,803 [LicenseBackgroundFetcher STARTING] io.confluent.common.security.license.LicenseBackgroundFetcher startUp - Setting up License Manager...
[INFO] 2023-11-08 14:51:17,804 [ce-kafka-http-server-start-thread] io.confluent.rest.Application configureHandler - Binding EmbeddedKafkaRestApplication to all listeners.
[INFO] 2023-11-08 14:51:17,828 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:51:17,830 [ce-kafka-http-server-start-thread] io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler$BearerConfig logAll - BearerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	expose.internal.connect.endpoints = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = []
	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.jmx.prefix = kafka.rest
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8080
	proxy.protocol.enabled = false
	public.key.path = /mnt/secrets/mds-token/mdsPublicKey.pem
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = [io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler]
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	token.issuer = Confluent
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:17,830 [ce-kafka-http-server-start-thread] io.confluent.kafka.clients.plugins.auth.jwt.JwtAuthenticatorConfig logAll - JwtAuthenticatorConfig values: 
	allowUnsafeURL = false
	audience = 
	issuer = Confluent
	jkuDomainWhiteList = []
	jwksLocation = /mnt/secrets/mds-token/mdsPublicKey.pem
	verificationKeyRefreshInterval = 3600000
	verificationKeyResolver = pemfile

[INFO] 2023-11-08 14:51:17,840 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:17,840 [ce-kafka-http-server-start-thread] io.confluent.security.auth.client.RestClientConfig logAll - RestClientConfig values: 
	confluent.metadata.basic.auth.credentials.path = null
	confluent.metadata.basic.auth.credentials.provider = USER_INFO
	confluent.metadata.basic.auth.user.info = [hidden]
	confluent.metadata.bootstrap.server.urls = [https://kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.enable.server.urls.refresh = true
	confluent.metadata.http.auth.credentials.provider = BASIC
	confluent.metadata.http.request.timeout.ms = 10000
	confluent.metadata.request.timeout.ms = 30000
	confluent.metadata.server.urls.fail.on.401 = false
	confluent.metadata.server.urls.max.age.ms = 600000
	confluent.metadata.server.urls.max.retries = 5
	confluent.metadata.token.auth.credential = [hidden]

[INFO] 2023-11-08 14:51:17,840 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[INFO] 2023-11-08 14:51:17,850 [ce-kafka-http-server-start-thread] io.confluent.security.auth.client.RestClientConfig logAll - RestClientConfig values: 
	confluent.metadata.basic.auth.credentials.path = null
	confluent.metadata.basic.auth.credentials.provider = USER_INFO
	confluent.metadata.basic.auth.user.info = [hidden]
	confluent.metadata.bootstrap.server.urls = [https://kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.enable.server.urls.refresh = true
	confluent.metadata.http.auth.credentials.provider = BASIC
	confluent.metadata.http.request.timeout.ms = 10000
	confluent.metadata.request.timeout.ms = 30000
	confluent.metadata.server.urls.fail.on.401 = false
	confluent.metadata.server.urls.max.age.ms = 600000
	confluent.metadata.server.urls.max.retries = 5
	confluent.metadata.token.auth.credential = [hidden]

[WARN] 2023-11-08 14:51:17,897 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,898 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:17,898 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:17,898 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:17,899 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455077898
[WARN] 2023-11-08 14:51:17,899 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser registerAppInfo - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=confluent-metrics-reporter
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:436)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:292)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:319)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:304)
	at io.confluent.metrics.reporter.ConfluentMetricsReporter.configure(ConfluentMetricsReporter.java:147)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstance(AbstractConfig.java:405)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:478)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:459)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:552)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:143)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:49)
	at io.confluent.license.LicenseStore.createTopicDescription(LicenseStore.java:413)
	at io.confluent.license.LicenseStore.<init>(LicenseStore.java:122)
	at io.confluent.license.LicenseStore.<init>(LicenseStore.java:104)
	at io.confluent.license.LicenseStore.<init>(LicenseStore.java:93)
	at io.confluent.license.LicenseManager.<init>(LicenseManager.java:151)
	at io.confluent.common.security.license.LicenseBackgroundFetcher.startUp(LicenseBackgroundFetcher.java:67)
	at com.google.common.util.concurrent.AbstractScheduledService$ServiceDelegate$2.run(AbstractScheduledService.java:258)
	at com.google.common.util.concurrent.Callables$4.run(Callables.java:119)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:17,901 [ce-kafka-http-server-start-thread] io.confluent.security.auth.client.RestClientConfig$SslClientConfig logAll - SslClientConfig values: 
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,904 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,908 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,908 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,908 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,908 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,908 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,908 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,908 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,908 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,908 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,908 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,909 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:17,910 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455077910
[INFO] 2023-11-08 14:51:17,936 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,026 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[ERROR] 2023-11-08 14:51:18,027 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,028 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,033 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,036 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:51:18,037 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:51:18,037 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:51:18,037 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:51:18,040 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-command-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:18,042 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-command-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:18,040 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,042 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,042 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,042 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:51:18,042 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,042 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,049 [LicenseBackgroundFetcher STARTING] io.confluent.license.LicenseStore start - Starting License Store
[INFO] 2023-11-08 14:51:18,049 [LicenseBackgroundFetcher STARTING] org.apache.kafka.connect.util.KafkaBasedLog start - Starting KafkaBasedLog with topic _confluent-command
[INFO] 2023-11-08 14:51:18,050 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:51:18,051 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:18,053 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[DEBUG] 2023-11-08 14:51:18,131 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 1, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,133 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,133 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[WARN] 2023-11-08 14:51:18,142 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,143 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,143 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,145 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,145 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078143
[WARN] 2023-11-08 14:51:18,149 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,149 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,149 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,149 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,149 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,149 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,150 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,151 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,152 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,152 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,152 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,152 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,152 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,152 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,152 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,152 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,152 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078152
[INFO] 2023-11-08 14:51:18,164 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.Server doStart - jetty-9.4.44.v20210927; built: 2021-09-27T23:02:44.612Z; git: 8da83308eeca865e495e53ef315a249d63ba9332; jvm 11.0.14.1+1-LTS
[DEBUG] 2023-11-08 14:51:18,174 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 2, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,175 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,175 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,181 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,235 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:51:18,237 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,243 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[DEBUG] 2023-11-08 14:51:18,247 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 3, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,249 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,252 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,243 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:51:18,258 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:51:18,264 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:51:18,285 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,285 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,285 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,285 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:51:18,286 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,286 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,287 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 30
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[INFO] 2023-11-08 14:51:18,289 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:18,289 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:18,298 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,298 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,299 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,299 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,299 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078298
[DEBUG] 2023-11-08 14:51:18,333 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 4, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,334 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,335 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,344 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:18,345 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.session doStart - DefaultSessionIdManager workerName=node0
[INFO] 2023-11-08 14:51:18,348 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.session doStart - No SessionScavenger set, using defaults
[INFO] 2023-11-08 14:51:18,352 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.session startScavenging - node0 Scavenging every 600000ms
[WARN] 2023-11-08 14:51:18,354 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,354 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,354 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'auto.register.schemas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,354 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,354 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,354 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,355 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,356 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,357 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,357 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,357 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,357 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,357 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,357 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,357 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,357 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,358 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,358 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,358 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'schema.registry.url' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,358 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[WARN] 2023-11-08 14:51:18,358 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,358 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,358 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,359 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,359 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,359 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,359 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,359 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,359 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,359 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'use.latest.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,360 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078360
[DEBUG] 2023-11-08 14:51:18,362 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:51:18,383 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.Metadata update - [Consumer clientId=confluent-license-consumer, groupId=null] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,383 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporter onUpdate - Starting Confluent metrics reporter for cluster id 9PWH12e6ROOpezLRDGV6Ag with an interval of 30000 ms
[INFO] 2023-11-08 14:51:18,386 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,386 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:51:18,386 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:51:18,386 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:51:18,386 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:51:18,390 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,390 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,390 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,390 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:51:18,390 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,390 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,393 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.consumer for confluent-license-consumer unregistered
[INFO] 2023-11-08 14:51:18,395 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[INFO] 2023-11-08 14:51:18,397 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:18,398 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:18,404 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,405 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,405 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,405 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,405 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078405
[WARN] 2023-11-08 14:51:18,417 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,417 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,417 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,417 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,418 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,422 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,422 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,422 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,422 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,422 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,422 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,422 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,422 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,423 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,424 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,425 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,425 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,425 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,425 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,425 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,425 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,425 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,425 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,425 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,428 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,429 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,429 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,429 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,429 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078429
[DEBUG] 2023-11-08 14:51:18,435 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 5, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[INFO] 2023-11-08 14:51:18,436 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 30
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[ERROR] 2023-11-08 14:51:18,436 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,437 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,438 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:18,443 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:18,449 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,450 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,450 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,450 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,450 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078450
[WARN] 2023-11-08 14:51:18,450 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser registerAppInfo - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=confluent-metrics-reporter
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:436)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:292)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:319)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:304)
	at io.confluent.metrics.reporter.ConfluentMetricsReporter.configure(ConfluentMetricsReporter.java:147)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstance(AbstractConfig.java:405)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:478)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:459)
	at org.apache.kafka.clients.consumer.KafkaConsumer.buildMetrics(KafkaConsumer.java:872)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:699)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:664)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:612)
	at org.apache.kafka.connect.util.KafkaBasedLog.createConsumer(KafkaBasedLog.java:377)
	at org.apache.kafka.connect.util.KafkaBasedLog.start(KafkaBasedLog.java:221)
	at io.confluent.license.LicenseStore.startLog(LicenseStore.java:263)
	at io.confluent.license.LicenseStore.start(LicenseStore.java:249)
	at io.confluent.license.LicenseManager.<init>(LicenseManager.java:229)
	at io.confluent.license.LicenseManager.<init>(LicenseManager.java:211)
	at io.confluent.license.LicenseManager.<init>(LicenseManager.java:151)
	at io.confluent.common.security.license.LicenseBackgroundFetcher.startUp(LicenseBackgroundFetcher.java:67)
	at com.google.common.util.concurrent.AbstractScheduledService$ServiceDelegate$2.run(AbstractScheduledService.java:258)
	at com.google.common.util.concurrent.Callables$4.run(Callables.java:119)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[WARN] 2023-11-08 14:51:18,463 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,463 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,463 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'auto.register.schemas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,463 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,463 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,463 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,463 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,464 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'schema.registry.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,465 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'use.latest.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,466 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078466
[INFO] 2023-11-08 14:51:18,475 [kafka-producer-network-thread | confluent-license-producer] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-license-producer] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,475 [kafka-producer-network-thread | confluent-license-producer] io.confluent.metrics.reporter.ConfluentMetricsReporter onUpdate - Starting Confluent metrics reporter for cluster id 9PWH12e6ROOpezLRDGV6Ag with an interval of 30000 ms
[INFO] 2023-11-08 14:51:18,479 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,497 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.Metadata update - [Consumer clientId=confluent-license-consumer, groupId=null] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,497 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporter onUpdate - Starting Confluent metrics reporter for cluster id 9PWH12e6ROOpezLRDGV6Ag with an interval of 30000 ms
[INFO] 2023-11-08 14:51:18,500 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.KafkaConsumer assign - [Consumer clientId=confluent-license-consumer, groupId=null] Subscribed to partition(s): _confluent-command-0
[INFO] 2023-11-08 14:51:18,504 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=confluent-license-consumer, groupId=null] Seeking to EARLIEST offset of partition _confluent-command-0
[INFO] 2023-11-08 14:51:18,511 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,559 [LicenseBackgroundFetcher STARTING] org.apache.kafka.connect.util.KafkaBasedLog start - Finished reading KafkaBasedLog for topic _confluent-command
[INFO] 2023-11-08 14:51:18,559 [LicenseBackgroundFetcher STARTING] org.apache.kafka.connect.util.KafkaBasedLog start - Started KafkaBasedLog for topic _confluent-command
[INFO] 2023-11-08 14:51:18,559 [LicenseBackgroundFetcher STARTING] io.confluent.license.LicenseStore start - Started License Store
[INFO] 2023-11-08 14:51:18,560 [LicenseBackgroundFetcher STARTING] io.confluent.common.security.license.LicenseBackgroundFetcher startUp - Finished setting up License Manager.
[DEBUG] 2023-11-08 14:51:18,563 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 6, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,564 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,564 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,716 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 7, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,718 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,718 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[WARN] 2023-11-08 14:51:18,825 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.metadataapi.resources.MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.metadataapi.resources.MetadataResource will be ignored. 
[DEBUG] 2023-11-08 14:51:18,879 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 8, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,880 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,880 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,964 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@248bfd87{/v1/metadata,null,AVAILABLE}
[WARN] 2023-11-08 14:51:19,059 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.tokenapi.resources.v1.V1TokenResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.tokenapi.resources.v1.V1TokenResource will be ignored. 
[WARN] 2023-11-08 14:51:19,059 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1FeaturesResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1FeaturesResource will be ignored. 
[WARN] 2023-11-08 14:51:19,060 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1AuthorizeResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1AuthorizeResource will be ignored. 
[WARN] 2023-11-08 14:51:19,060 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1RolesResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1RolesResource will be ignored. 
[WARN] 2023-11-08 14:51:19,060 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1LookupResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1LookupResource will be ignored. 
[WARN] 2023-11-08 14:51:19,060 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1PrincipalsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1PrincipalsResource will be ignored. 
[WARN] 2023-11-08 14:51:19,060 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1MetadataServiceResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1MetadataServiceResource will be ignored. 
[DEBUG] 2023-11-08 14:51:19,060 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 9, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[WARN] 2023-11-08 14:51:19,060 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1AclResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1AclResource will be ignored. 
[WARN] 2023-11-08 14:51:19,061 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1OperationsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1OperationsResource will be ignored. 
[WARN] 2023-11-08 14:51:19,061 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1UserGroupResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1UserGroupResource will be ignored. 
[WARN] 2023-11-08 14:51:19,061 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1ClusterRegistryResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1ClusterRegistryResource will be ignored. 
[WARN] 2023-11-08 14:51:19,061 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1AuditLogConfigResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1AuditLogConfigResource will be ignored. 
[ERROR] 2023-11-08 14:51:19,061 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:19,062 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:19,118 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:51:19,119 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:19,120 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:19,124 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,124 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:19,125 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,125 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,125 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079124
[WARN] 2023-11-08 14:51:19,125 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser registerAppInfo - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=confluent-metrics-reporter
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:436)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:292)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:319)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:304)
	at io.confluent.metrics.reporter.ConfluentMetricsReporter.configure(ConfluentMetricsReporter.java:147)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstance(AbstractConfig.java:405)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:478)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:459)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:552)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:143)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:49)
	at io.confluent.license.LicenseManager$BasicClusterClient.clusterId(LicenseManager.java:743)
	at io.confluent.license.LicenseManager.registerOrValidateLicense(LicenseManager.java:458)
	at io.confluent.common.security.license.LicenseBackgroundFetcher.runOneIteration(LicenseBackgroundFetcher.java:88)
	at com.google.common.util.concurrent.AbstractScheduledService$ServiceDelegate$Task.run(AbstractScheduledService.java:221)
	at com.google.common.util.concurrent.Callables$4.run(Callables.java:119)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[WARN] 2023-11-08 14:51:19,128 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,128 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,128 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,128 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,128 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,128 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,128 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,128 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,130 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,131 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079130
[INFO] 2023-11-08 14:51:19,148 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:19,173 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:51:19,176 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:19,176 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:51:19,176 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:51:19,176 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:51:19,176 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:51:19,177 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:19,177 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:19,177 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:19,177 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:51:19,178 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:19,178 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:19,178 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[INFO] 2023-11-08 14:51:19,226 [ce-kafka-http-server-start-thread] org.eclipse.jetty.util.ssl.SslContextFactory load - x509=X509@5196c49b(ca,h=[testca],a=[],w=[]) for Server@27a4e25b[provider=null,keyStore=file:///mnt/sslcerts/keystore.jks,trustStore=file:///mnt/sslcerts/truststore.jks]
[INFO] 2023-11-08 14:51:19,230 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@f823dd1{/security,null,AVAILABLE}
[DEBUG] 2023-11-08 14:51:19,262 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 10, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:19,263 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:19,263 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[WARN] 2023-11-08 14:51:19,270 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[WARN] 2023-11-08 14:51:19,274 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[WARN] 2023-11-08 14:51:19,326 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[WARN] 2023-11-08 14:51:19,341 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[DEBUG] 2023-11-08 14:51:19,484 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 11, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:19,485 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:19,485 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:19,759 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 12, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:19,760 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:19,760 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:19,784 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@4e26685f{/kafka,null,AVAILABLE}
[INFO] 2023-11-08 14:51:19,808 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@5675acd7{/ws,null,AVAILABLE}
[INFO] 2023-11-08 14:51:19,809 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@2de9045b{/ws,null,AVAILABLE}
[INFO] 2023-11-08 14:51:19,809 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@331bc882{/ws,null,AVAILABLE}
[INFO] 2023-11-08 14:51:19,832 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.AbstractConnector doStart - Started NetworkTrafficServerConnector@642ec6{SSL, (ssl, alpn, h2, http/1.1)}{0.0.0.0:8090}
[INFO] 2023-11-08 14:51:19,833 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.Server doStart - Started @27959ms
[INFO] 2023-11-08 14:51:19,834 [ce-kafka-http-server-start-thread] io.confluent.http.server.KafkaHttpServerImpl transition - KafkaHttpServer transitioned from STARTING to RUNNING..
[INFO] 2023-11-08 14:51:19,861 [main] kafka.server.KafkaServer info - [KafkaServer id=2] Skipping durability audit instantiation
[INFO] 2023-11-08 14:51:19,864 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,864 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,864 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079864
[INFO] 2023-11-08 14:51:19,864 [main] kafka.server.KafkaServer info - [KafkaServer id=2] started
[INFO] 2023-11-08 14:51:19,865 [main] io.confluent.license.validator.LicenseConfig logAll - LicenseConfig values: 
	confluent.license = 
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 3

[INFO] 2023-11-08 14:51:19,866 [main] io.confluent.license.validator.LicenseConfig logAll - LicenseConfig values: 
	confluent.license = 
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 3

[INFO] 2023-11-08 14:51:19,866 [main] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,869 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,870 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:19,870 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,870 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,870 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079870
[INFO] 2023-11-08 14:51:19,920 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for _confluent-license-admin-2 unregistered
[INFO] 2023-11-08 14:51:19,922 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:19,922 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:19,922 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:19,924 [confluent-license-manager] io.confluent.license.LicenseStore start - Starting License Store
[INFO] 2023-11-08 14:51:19,924 [confluent-license-manager] org.apache.kafka.connect.util.KafkaBasedLog start - Starting KafkaBasedLog with topic _confluent-command
[INFO] 2023-11-08 14:51:19,926 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:19,929 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,929 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,929 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,930 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,931 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,932 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,932 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,932 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,932 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,932 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,932 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,933 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,933 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,933 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,933 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,933 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,933 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,933 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,933 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:19,934 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,935 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,935 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079934
[INFO] 2023-11-08 14:51:19,972 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for _confluent-license-admin-2 unregistered
[INFO] 2023-11-08 14:51:19,974 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:19,974 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:19,974 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:19,975 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,979 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,980 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,981 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079981
[INFO] 2023-11-08 14:51:20,003 [confluent-license-manager] org.apache.kafka.clients.Metadata update - [Consumer clientId=_confluent-license-consumer-2, groupId=null] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:20,005 [confluent-license-manager] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:20,005 [confluent-license-manager] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:20,005 [confluent-license-manager] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:20,006 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.consumer for _confluent-license-consumer-2 unregistered
[INFO] 2023-11-08 14:51:20,007 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-producer-2
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[WARN] 2023-11-08 14:51:20,011 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,012 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,012 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,012 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,013 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,013 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,013 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,014 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,014 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,014 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,014 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,014 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,015 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,015 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,015 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,015 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,015 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,016 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,016 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,016 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,016 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,016 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,016 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,016 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,016 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,017 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,017 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,017 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,017 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,017 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,017 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,017 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,017 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,018 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,018 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,018 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,018 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,018 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,018 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,018 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,018 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,019 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,019 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,019 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,019 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,019 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,019 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,019 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,019 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,019 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,020 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,020 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,020 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,020 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,021 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,021 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,021 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[DEBUG] 2023-11-08 14:51:20,021 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 13, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[INFO] 2023-11-08 14:51:20,024 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:20,025 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:20,025 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455080024
[INFO] 2023-11-08 14:51:20,026 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-consumer-2
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[WARN] 2023-11-08 14:51:20,047 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,048 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,048 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,048 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,048 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,048 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,048 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,049 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,049 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,049 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,049 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,049 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,049 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,049 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,050 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,050 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,050 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,050 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,050 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,050 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,050 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,051 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,051 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,051 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,051 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,051 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,051 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,051 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,051 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,052 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:20,053 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:20,053 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:20,053 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455080053
[INFO] 2023-11-08 14:51:20,058 [kafka-producer-network-thread | _confluent-license-producer-2] org.apache.kafka.clients.Metadata update - [Producer clientId=_confluent-license-producer-2] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:20,074 [confluent-license-manager] org.apache.kafka.clients.Metadata update - [Consumer clientId=_confluent-license-consumer-2, groupId=null] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:20,077 [confluent-license-manager] org.apache.kafka.clients.consumer.KafkaConsumer assign - [Consumer clientId=_confluent-license-consumer-2, groupId=null] Subscribed to partition(s): _confluent-command-0
[INFO] 2023-11-08 14:51:20,078 [confluent-license-manager] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-license-consumer-2, groupId=null] Seeking to EARLIEST offset of partition _confluent-command-0
[INFO] 2023-11-08 14:51:20,172 [confluent-license-manager] org.apache.kafka.connect.util.KafkaBasedLog start - Finished reading KafkaBasedLog for topic _confluent-command
[INFO] 2023-11-08 14:51:20,173 [confluent-license-manager] org.apache.kafka.connect.util.KafkaBasedLog start - Started KafkaBasedLog for topic _confluent-command
[INFO] 2023-11-08 14:51:20,173 [confluent-license-manager] io.confluent.license.LicenseStore start - Started License Store
[DEBUG] 2023-11-08 14:51:20,240 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler searchForLdapUser - Searching for user with null LDAP context: Must create new context. LdapAuthenticateCallbackHandler@38cbebf4 Thread:qtp1625817721-188 user:kafka searchCounter:0 
[TRACE] 2023-11-08 14:51:20,341 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 188]
[TRACE] 2023-11-08 14:51:20,347 [qtp1625817721-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 180]
[TRACE] 2023-11-08 14:51:20,348 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[TRACE] 2023-11-08 14:51:20,361 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=sr,dc=test,dc=com [Thread 179]
[TRACE] 2023-11-08 14:51:20,361 [qtp1625817721-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=sr,dc=test,dc=com [Thread 180]
[TRACE] 2023-11-08 14:51:20,362 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 188]
[DEBUG] 2023-11-08 14:51:20,363 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for sr
[DEBUG] 2023-11-08 14:51:20,364 [qtp1625817721-180] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for sr
[DEBUG] 2023-11-08 14:51:20,365 [qtp1625817721-188] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[TRACE] 2023-11-08 14:51:20,390 [qtp1625817721-177] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 177]
[TRACE] 2023-11-08 14:51:20,390 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 200]
[TRACE] 2023-11-08 14:51:20,391 [qtp1625817721-177] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 177]
[TRACE] 2023-11-08 14:51:20,392 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 200]
[DEBUG] 2023-11-08 14:51:20,394 [qtp1625817721-177] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[DEBUG] 2023-11-08 14:51:20,394 [qtp1625817721-200] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[INFO] 2023-11-08 14:51:20,418 [qtp1625817721-188] io.confluent.mds.request.logger log - 3 * Server has received a request on thread qtp1625817721-188null3 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull3 > User Principal: kafka
3 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null3 > Cache-Control: no-cachenull3 > Connection: keep-alivenull3 > Content-Type: application/jsonnull3 > Host: kafka.confluent.svc.cluster.local:8090null3 > Pragma: no-cachenull3 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:20,418 [qtp1625817721-180] io.confluent.mds.request.logger log - 2 * Server has received a request on thread qtp1625817721-180null2 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull2 > User Principal: sr
2 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null2 > Cache-Control: no-cachenull2 > Connection: keep-alivenull2 > Content-Type: application/jsonnull2 > Host: kafka.confluent.svc.cluster.local:8090null2 > Pragma: no-cachenull2 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:20,418 [qtp1625817721-200] io.confluent.mds.request.logger log - 4 * Server has received a request on thread qtp1625817721-200null4 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull4 > User Principal: ksql
4 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null4 > Cache-Control: no-cachenull4 > Connection: keep-alivenull4 > Content-Type: application/jsonnull4 > Host: kafka.confluent.svc.cluster.local:8090null4 > Pragma: no-cachenull4 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:20,418 [qtp1625817721-177] io.confluent.mds.request.logger log - 5 * Server has received a request on thread qtp1625817721-177null5 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull5 > User Principal: ksql
5 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null5 > Cache-Control: no-cachenull5 > Connection: keep-alivenull5 > Content-Type: application/jsonnull5 > Host: kafka.confluent.svc.cluster.local:8090null5 > Pragma: no-cachenull5 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:20,418 [qtp1625817721-179] io.confluent.mds.request.logger log - 1 * Server has received a request on thread qtp1625817721-179null1 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull1 > User Principal: sr
1 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null1 > Cache-Control: no-cachenull1 > Connection: keep-alivenull1 > Content-Type: application/jsonnull1 > Host: kafka.confluent.svc.cluster.local:8090null1 > Pragma: no-cachenull1 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:20,454 [qtp1625817721-179] io.confluent.mds.request.logger log - 1 * Server responded with a response on thread qtp1625817721-179null1 < 200null1 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:20,454 [qtp1625817721-177] io.confluent.mds.request.logger log - 5 * Server responded with a response on thread qtp1625817721-177null5 < 200null5 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:20,466 [qtp1625817721-188] io.confluent.mds.request.logger log - 3 * Server responded with a response on thread qtp1625817721-188null3 < 200null3 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:20,466 [qtp1625817721-180] io.confluent.mds.request.logger log - 2 * Server responded with a response on thread qtp1625817721-180null2 < 200null2 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:20,466 [qtp1625817721-200] io.confluent.mds.request.logger log - 4 * Server responded with a response on thread qtp1625817721-200null4 < 200null4 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[DEBUG] 2023-11-08 14:51:20,477 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient run - Successfully fetched MDS URLs ([https://kafka-1.kafka.confluent.svc.cluster.local:8090, https://kafka-2.kafka.confluent.svc.cluster.local:8090, https://kafka-0.kafka.confluent.svc.cluster.local:8090, https://kafka.confluent.svc.cluster.local:8090])
[INFO] 2023-11-08 14:51:20,488 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 141
[INFO] 2023-11-08 14:51:20,488 [qtp1625817721-177] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 636 "-" "Java/11.0.14.1" 99
[INFO] 2023-11-08 14:51:20,489 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 142
[INFO] 2023-11-08 14:51:20,489 [qtp1625817721-177] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 636 "-" "Java/11.0.14.1" 100
[INFO] 2023-11-08 14:51:20,489 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 142
[INFO] 2023-11-08 14:51:20,489 [qtp1625817721-177] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 636 "-" "Java/11.0.14.1" 100
[INFO] 2023-11-08 14:51:20,489 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 99
[INFO] 2023-11-08 14:51:20,489 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 265
[INFO] 2023-11-08 14:51:20,489 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 99
[INFO] 2023-11-08 14:51:20,489 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 142
[INFO] 2023-11-08 14:51:20,489 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 265
[INFO] 2023-11-08 14:51:20,490 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 100
[INFO] 2023-11-08 14:51:20,490 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 266
[INFO] 2023-11-08 14:51:20,490 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 143
[INFO] 2023-11-08 14:51:20,490 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 143
[INFO] 2023-11-08 14:51:20,979 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-2
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:20,981 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,981 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,981 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,981 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,981 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,981 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:20,982 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455080982
[INFO] 2023-11-08 14:51:21,011 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for _confluent-license-admin-2 unregistered
[INFO] 2023-11-08 14:51:21,013 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:21,013 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:21,013 [kafka-admin-client-thread | _confluent-license-admin-2] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:21,013 [confluent-license-manager] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[INFO] 2023-11-08 14:51:21,018 [main] kafka.metrics.BrokerLoad startMetric - Starting delay for broker load metric
[DEBUG] 2023-11-08 14:51:21,348 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:21,350 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:21,438 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 188]
[TRACE] 2023-11-08 14:51:21,439 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 200]
[TRACE] 2023-11-08 14:51:21,440 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=sr,dc=test,dc=com [Thread 188]
[TRACE] 2023-11-08 14:51:21,441 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=sr,dc=test,dc=com [Thread 200]
[DEBUG] 2023-11-08 14:51:21,441 [qtp1625817721-188] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for sr
[DEBUG] 2023-11-08 14:51:21,443 [qtp1625817721-200] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for sr
[INFO] 2023-11-08 14:51:21,443 [qtp1625817721-188] io.confluent.mds.request.logger log - 6 * Server has received a request on thread qtp1625817721-188null6 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull6 > User Principal: sr
6 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null6 > Cache-Control: no-cachenull6 > Connection: keep-alivenull6 > Content-Type: application/jsonnull6 > Host: kafka.confluent.svc.cluster.local:8090null6 > Pragma: no-cachenull6 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:21,445 [qtp1625817721-200] io.confluent.mds.request.logger log - 7 * Server has received a request on thread qtp1625817721-200null7 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull7 > User Principal: sr
7 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null7 > Cache-Control: no-cachenull7 > Connection: keep-alivenull7 > Content-Type: application/jsonnull7 > Host: kafka.confluent.svc.cluster.local:8090null7 > Pragma: no-cachenull7 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:21,446 [qtp1625817721-188] io.confluent.mds.request.logger log - 6 * Server responded with a response on thread qtp1625817721-188null6 < 200null6 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:21,447 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:21,447 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:21,447 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:21,450 [qtp1625817721-200] io.confluent.mds.request.logger log - 7 * Server responded with a response on thread qtp1625817721-200null7 < 200null7 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:21,452 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:21,452 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:21,453 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 14
[TRACE] 2023-11-08 14:51:22,024 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 188]
[TRACE] 2023-11-08 14:51:22,027 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 188]
[DEBUG] 2023-11-08 14:51:22,031 [qtp1625817721-188] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:51:22,032 [qtp1625817721-188] io.confluent.mds.request.logger log - 8 * Server has received a request on thread qtp1625817721-188null8 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull8 > User Principal: c3
8 > authorization: Basic YzM6YzMtc2VjcmV0null8 > host: kafka.confluent.svc.cluster.local:8090null8 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:51:22,033 [qtp1625817721-188] io.confluent.mds.request.logger log - 8 * Server responded with a response on thread qtp1625817721-188null8 < 200null8 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:51:22,036 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:51:22 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 12
[INFO] 2023-11-08 14:51:22,037 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:51:22 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 13
[INFO] 2023-11-08 14:51:22,037 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:51:22 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 13
[DEBUG] 2023-11-08 14:51:24,348 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:24,350 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:24,576 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 200]
[TRACE] 2023-11-08 14:51:24,577 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 200]
[DEBUG] 2023-11-08 14:51:24,580 [qtp1625817721-200] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[INFO] 2023-11-08 14:51:24,583 [qtp1625817721-200] io.confluent.mds.request.logger log - 9 * Server has received a request on thread qtp1625817721-200null9 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull9 > User Principal: ksql
9 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null9 > Cache-Control: no-cachenull9 > Connection: keep-alivenull9 > Content-Type: application/jsonnull9 > Host: kafka.confluent.svc.cluster.local:8090null9 > Pragma: no-cachenull9 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:24,584 [qtp1625817721-200] io.confluent.mds.request.logger log - 9 * Server responded with a response on thread qtp1625817721-200null9 < 200null9 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:24,585 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:24 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:24,585 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:24 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:24,585 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:24 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[TRACE] 2023-11-08 14:51:24,847 [qtp1625817721-175] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 175]
[TRACE] 2023-11-08 14:51:24,856 [qtp1625817721-175] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 175]
[DEBUG] 2023-11-08 14:51:24,873 [qtp1625817721-175] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[INFO] 2023-11-08 14:51:24,874 [qtp1625817721-175] io.confluent.mds.request.logger log - 10 * Server has received a request on thread qtp1625817721-175null10 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull10 > User Principal: ksql
10 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null10 > Cache-Control: no-cachenull10 > Connection: keep-alivenull10 > Content-Type: application/jsonnull10 > Host: kafka.confluent.svc.cluster.local:8090null10 > Pragma: no-cachenull10 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:24,876 [qtp1625817721-175] io.confluent.mds.request.logger log - 10 * Server responded with a response on thread qtp1625817721-175null10 < 200null10 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:24,877 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:24 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 30
[INFO] 2023-11-08 14:51:24,877 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:24 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 30
[INFO] 2023-11-08 14:51:24,878 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:24 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 31
[DEBUG] 2023-11-08 14:51:27,351 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:27,352 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:51:30,356 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:30,360 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:33,213 [qtp1625817721-195] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 195]
[TRACE] 2023-11-08 14:51:33,213 [qtp1625817721-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 181]
[TRACE] 2023-11-08 14:51:33,215 [qtp1625817721-195] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 195]
[TRACE] 2023-11-08 14:51:33,216 [qtp1625817721-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 181]
[DEBUG] 2023-11-08 14:51:33,217 [qtp1625817721-195] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:33,219 [qtp1625817721-181] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:33,219 [qtp1625817721-195] io.confluent.mds.request.logger log - 11 * Server has received a request on thread qtp1625817721-195null11 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull11 > User Principal: connect
11 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null11 > Cache-Control: no-cachenull11 > Connection: keep-alivenull11 > Content-Type: application/jsonnull11 > Host: kafka.confluent.svc.cluster.local:8090null11 > Pragma: no-cachenull11 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:33,221 [qtp1625817721-181] io.confluent.mds.request.logger log - 12 * Server has received a request on thread qtp1625817721-181null12 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull12 > User Principal: connect
12 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null12 > Cache-Control: no-cachenull12 > Connection: keep-alivenull12 > Content-Type: application/jsonnull12 > Host: kafka.confluent.svc.cluster.local:8090null12 > Pragma: no-cachenull12 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:33,222 [qtp1625817721-181] io.confluent.mds.request.logger log - 12 * Server responded with a response on thread qtp1625817721-181null12 < 200null12 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:33,223 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:33 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:51:33,223 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:33 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:51:33,223 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:33 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:51:33,225 [qtp1625817721-195] io.confluent.mds.request.logger log - 11 * Server responded with a response on thread qtp1625817721-195null11 < 200null11 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:33,226 [qtp1625817721-195] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:33 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:33,226 [qtp1625817721-195] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:33 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:33,226 [qtp1625817721-195] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:33 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[DEBUG] 2023-11-08 14:51:33,356 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:33,358 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:34,363 [qtp1625817721-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 186]
[TRACE] 2023-11-08 14:51:34,365 [qtp1625817721-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 186]
[TRACE] 2023-11-08 14:51:34,366 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[DEBUG] 2023-11-08 14:51:34,367 [qtp1625817721-186] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[TRACE] 2023-11-08 14:51:34,367 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 179]
[INFO] 2023-11-08 14:51:34,368 [qtp1625817721-186] io.confluent.mds.request.logger log - 13 * Server has received a request on thread qtp1625817721-186null13 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull13 > User Principal: connect
13 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null13 > Cache-Control: no-cachenull13 > Connection: keep-alivenull13 > Content-Type: application/jsonnull13 > Host: kafka.confluent.svc.cluster.local:8090null13 > Pragma: no-cachenull13 > User-Agent: Java/11.0.14.1null
[DEBUG] 2023-11-08 14:51:34,369 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:34,370 [qtp1625817721-186] io.confluent.mds.request.logger log - 13 * Server responded with a response on thread qtp1625817721-186null13 < 200null13 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:34,370 [qtp1625817721-179] io.confluent.mds.request.logger log - 14 * Server has received a request on thread qtp1625817721-179null14 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull14 > User Principal: connect
14 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null14 > Cache-Control: no-cachenull14 > Connection: keep-alivenull14 > Content-Type: application/jsonnull14 > Host: kafka.confluent.svc.cluster.local:8090null14 > Pragma: no-cachenull14 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:34,371 [qtp1625817721-186] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:34,372 [qtp1625817721-186] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:34,372 [qtp1625817721-186] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:34,374 [qtp1625817721-179] io.confluent.mds.request.logger log - 14 * Server responded with a response on thread qtp1625817721-179null14 < 200null14 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:34,375 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:34,375 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:34,375 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 9
[TRACE] 2023-11-08 14:51:34,481 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 200]
[TRACE] 2023-11-08 14:51:34,481 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[TRACE] 2023-11-08 14:51:34,483 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 200]
[TRACE] 2023-11-08 14:51:34,483 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 179]
[DEBUG] 2023-11-08 14:51:34,485 [qtp1625817721-200] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:34,485 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:34,486 [qtp1625817721-200] io.confluent.mds.request.logger log - 15 * Server has received a request on thread qtp1625817721-200null15 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull15 > User Principal: connect
15 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null15 > Cache-Control: no-cachenull15 > Connection: keep-alivenull15 > Content-Type: application/jsonnull15 > Host: kafka.confluent.svc.cluster.local:8090null15 > Pragma: no-cachenull15 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:34,487 [qtp1625817721-179] io.confluent.mds.request.logger log - 16 * Server has received a request on thread qtp1625817721-179null16 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull16 > User Principal: connect
16 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null16 > Cache-Control: no-cachenull16 > Connection: keep-alivenull16 > Content-Type: application/jsonnull16 > Host: kafka.confluent.svc.cluster.local:8090null16 > Pragma: no-cachenull16 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:34,488 [qtp1625817721-179] io.confluent.mds.request.logger log - 16 * Server responded with a response on thread qtp1625817721-179null16 < 200null16 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:34,489 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:34,489 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:34,490 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:34,491 [qtp1625817721-200] io.confluent.mds.request.logger log - 15 * Server responded with a response on thread qtp1625817721-200null15 < 200null15 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:34,492 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:51:34,493 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,493 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[TRACE] 2023-11-08 14:51:34,545 [qtp1625817721-175] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 175]
[TRACE] 2023-11-08 14:51:34,547 [qtp1625817721-175] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 175]
[DEBUG] 2023-11-08 14:51:34,549 [qtp1625817721-175] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:34,551 [qtp1625817721-175] io.confluent.mds.request.logger log - 17 * Server has received a request on thread qtp1625817721-175null17 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull17 > User Principal: kafka
17 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null17 > Cache-Control: no-cachenull17 > Connection: keep-alivenull17 > Content-Type: application/jsonnull17 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null17 > Pragma: no-cachenull17 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:34,557 [qtp1625817721-175] io.confluent.mds.request.logger log - 17 * Server responded with a response on thread qtp1625817721-175null17 < 200null17 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:34,559 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 639 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:34,559 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 639 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:34,560 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 639 "-" "Java/11.0.14.1" 15
[TRACE] 2023-11-08 14:51:34,608 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 188]
[TRACE] 2023-11-08 14:51:34,611 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 188]
[TRACE] 2023-11-08 14:51:34,612 [qtp1625817721-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 180]
[TRACE] 2023-11-08 14:51:34,613 [qtp1625817721-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 180]
[DEBUG] 2023-11-08 14:51:34,613 [qtp1625817721-188] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:34,616 [qtp1625817721-188] io.confluent.mds.request.logger log - 18 * Server has received a request on thread qtp1625817721-188null18 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull18 > User Principal: connect
18 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null18 > Cache-Control: no-cachenull18 > Connection: keep-alivenull18 > Content-Type: application/jsonnull18 > Host: kafka.confluent.svc.cluster.local:8090null18 > Pragma: no-cachenull18 > User-Agent: Java/11.0.14.1null
[DEBUG] 2023-11-08 14:51:34,617 [qtp1625817721-180] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:34,618 [qtp1625817721-180] io.confluent.mds.request.logger log - 19 * Server has received a request on thread qtp1625817721-180null19 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull19 > User Principal: connect
19 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null19 > Cache-Control: no-cachenull19 > Connection: keep-alivenull19 > Content-Type: application/jsonnull19 > Host: kafka.confluent.svc.cluster.local:8090null19 > Pragma: no-cachenull19 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:34,618 [qtp1625817721-188] io.confluent.mds.request.logger log - 18 * Server responded with a response on thread qtp1625817721-188null18 < 200null18 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:34,619 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:51:34,620 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,620 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,622 [qtp1625817721-180] io.confluent.mds.request.logger log - 19 * Server responded with a response on thread qtp1625817721-180null19 < 200null19 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:34,623 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,623 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,623 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[TRACE] 2023-11-08 14:51:34,715 [qtp1625817721-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 192]
[TRACE] 2023-11-08 14:51:34,717 [qtp1625817721-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 192]
[TRACE] 2023-11-08 14:51:34,717 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 200]
[TRACE] 2023-11-08 14:51:34,719 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 200]
[DEBUG] 2023-11-08 14:51:34,719 [qtp1625817721-192] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:34,722 [qtp1625817721-192] io.confluent.mds.request.logger log - 20 * Server has received a request on thread qtp1625817721-192null20 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull20 > User Principal: connect
20 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null20 > Cache-Control: no-cachenull20 > Connection: keep-alivenull20 > Content-Type: application/jsonnull20 > Host: kafka.confluent.svc.cluster.local:8090null20 > Pragma: no-cachenull20 > User-Agent: Java/11.0.14.1null
[DEBUG] 2023-11-08 14:51:34,723 [qtp1625817721-200] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:34,724 [qtp1625817721-200] io.confluent.mds.request.logger log - 21 * Server has received a request on thread qtp1625817721-200null21 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull21 > User Principal: connect
21 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null21 > Cache-Control: no-cachenull21 > Connection: keep-alivenull21 > Content-Type: application/jsonnull21 > Host: kafka.confluent.svc.cluster.local:8090null21 > Pragma: no-cachenull21 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:34,724 [qtp1625817721-192] io.confluent.mds.request.logger log - 20 * Server responded with a response on thread qtp1625817721-192null20 < 200null20 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:34,725 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:34,725 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:34,725 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:34,728 [qtp1625817721-200] io.confluent.mds.request.logger log - 21 * Server responded with a response on thread qtp1625817721-200null21 < 200null21 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:34,729 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,729 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,729 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[TRACE] 2023-11-08 14:51:34,809 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 188]
[TRACE] 2023-11-08 14:51:34,811 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 188]
[TRACE] 2023-11-08 14:51:34,811 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[TRACE] 2023-11-08 14:51:34,813 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 179]
[DEBUG] 2023-11-08 14:51:34,813 [qtp1625817721-188] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:34,814 [qtp1625817721-188] io.confluent.mds.request.logger log - 22 * Server has received a request on thread qtp1625817721-188null22 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull22 > User Principal: connect
22 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null22 > Cache-Control: no-cachenull22 > Connection: keep-alivenull22 > Content-Type: application/jsonnull22 > Host: kafka.confluent.svc.cluster.local:8090null22 > Pragma: no-cachenull22 > User-Agent: Java/11.0.14.1null
[DEBUG] 2023-11-08 14:51:34,815 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:34,816 [qtp1625817721-179] io.confluent.mds.request.logger log - 23 * Server has received a request on thread qtp1625817721-179null23 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull23 > User Principal: connect
23 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null23 > Cache-Control: no-cachenull23 > Connection: keep-alivenull23 > Content-Type: application/jsonnull23 > Host: kafka.confluent.svc.cluster.local:8090null23 > Pragma: no-cachenull23 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:34,817 [qtp1625817721-179] io.confluent.mds.request.logger log - 23 * Server responded with a response on thread qtp1625817721-179null23 < 200null23 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:34,818 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:51:34,818 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:51:34,818 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:51:34,819 [qtp1625817721-188] io.confluent.mds.request.logger log - 22 * Server responded with a response on thread qtp1625817721-188null22 < 200null22 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:34,820 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,820 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,820 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[TRACE] 2023-11-08 14:51:34,914 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[TRACE] 2023-11-08 14:51:34,914 [qtp1625817721-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 180]
[TRACE] 2023-11-08 14:51:34,916 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 179]
[TRACE] 2023-11-08 14:51:34,917 [qtp1625817721-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 180]
[DEBUG] 2023-11-08 14:51:34,918 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:34,919 [qtp1625817721-180] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:34,921 [qtp1625817721-180] io.confluent.mds.request.logger log - 25 * Server has received a request on thread qtp1625817721-180null25 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull25 > User Principal: connect
25 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null25 > Cache-Control: no-cachenull25 > Connection: keep-alivenull25 > Content-Type: application/jsonnull25 > Host: kafka.confluent.svc.cluster.local:8090null25 > Pragma: no-cachenull25 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:34,921 [qtp1625817721-179] io.confluent.mds.request.logger log - 24 * Server has received a request on thread qtp1625817721-179null24 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull24 > User Principal: connect
24 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null24 > Cache-Control: no-cachenull24 > Connection: keep-alivenull24 > Content-Type: application/jsonnull24 > Host: kafka.confluent.svc.cluster.local:8090null24 > Pragma: no-cachenull24 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:34,922 [qtp1625817721-180] io.confluent.mds.request.logger log - 25 * Server responded with a response on thread qtp1625817721-180null25 < 200null25 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:34,923 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:34,923 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:34,923 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:34,925 [qtp1625817721-179] io.confluent.mds.request.logger log - 24 * Server responded with a response on thread qtp1625817721-179null24 < 200null24 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:34,926 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,926 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:51:34,926 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[TRACE] 2023-11-08 14:51:35,014 [qtp1625817721-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 180]
[TRACE] 2023-11-08 14:51:35,015 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[TRACE] 2023-11-08 14:51:35,016 [qtp1625817721-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 180]
[TRACE] 2023-11-08 14:51:35,016 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 179]
[DEBUG] 2023-11-08 14:51:35,018 [qtp1625817721-180] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:35,018 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:35,020 [qtp1625817721-179] io.confluent.mds.request.logger log - 27 * Server has received a request on thread qtp1625817721-179null27 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull27 > User Principal: connect
27 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null27 > Cache-Control: no-cachenull27 > Connection: keep-alivenull27 > Content-Type: application/jsonnull27 > Host: kafka.confluent.svc.cluster.local:8090null27 > Pragma: no-cachenull27 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:35,020 [qtp1625817721-180] io.confluent.mds.request.logger log - 26 * Server has received a request on thread qtp1625817721-180null26 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull26 > User Principal: connect
26 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null26 > Cache-Control: no-cachenull26 > Connection: keep-alivenull26 > Content-Type: application/jsonnull26 > Host: kafka.confluent.svc.cluster.local:8090null26 > Pragma: no-cachenull26 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:35,022 [qtp1625817721-179] io.confluent.mds.request.logger log - 27 * Server responded with a response on thread qtp1625817721-179null27 < 200null27 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:35,023 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:35,023 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:35,023 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:35,025 [qtp1625817721-180] io.confluent.mds.request.logger log - 26 * Server responded with a response on thread qtp1625817721-180null26 < 200null26 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:35,026 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:35,026 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:35,026 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[TRACE] 2023-11-08 14:51:35,137 [qtp1625817721-194] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 194]
[TRACE] 2023-11-08 14:51:35,138 [qtp1625817721-194] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 194]
[DEBUG] 2023-11-08 14:51:35,140 [qtp1625817721-194] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:35,142 [qtp1625817721-194] io.confluent.mds.request.logger log - 28 * Server has received a request on thread qtp1625817721-194null28 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull28 > User Principal: connect
28 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null28 > Cache-Control: no-cachenull28 > Connection: keep-alivenull28 > Content-Type: application/jsonnull28 > Host: kafka.confluent.svc.cluster.local:8090null28 > Pragma: no-cachenull28 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:35,143 [qtp1625817721-194] io.confluent.mds.request.logger log - 28 * Server responded with a response on thread qtp1625817721-194null28 < 200null28 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:35,144 [qtp1625817721-194] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:35,144 [qtp1625817721-194] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:35,144 [qtp1625817721-194] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:35,245 [data-plane-kafka-request-handler-4] kafka.authorizer.logger logAuthorization - Principal = User:connect is Denied Operation = Describe from host = 10.40.0.14 on resource = Topic:LITERAL:confluent.connect-offsets
[TRACE] 2023-11-08 14:51:35,257 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[TRACE] 2023-11-08 14:51:35,259 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 179]
[DEBUG] 2023-11-08 14:51:35,261 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:35,263 [qtp1625817721-179] io.confluent.mds.request.logger log - 29 * Server has received a request on thread qtp1625817721-179null29 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull29 > User Principal: connect
29 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null29 > Cache-Control: no-cachenull29 > Connection: keep-alivenull29 > Content-Type: application/jsonnull29 > Host: kafka.confluent.svc.cluster.local:8090null29 > Pragma: no-cachenull29 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:35,264 [qtp1625817721-179] io.confluent.mds.request.logger log - 29 * Server responded with a response on thread qtp1625817721-179null29 < 200null29 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:35,265 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:35,266 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:35,266 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:51:35 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[DEBUG] 2023-11-08 14:51:36,356 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:36,358 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:51:37,098 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:37,113 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:37,113 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:37,113 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:37,113 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:37,113 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455097113
[INFO] 2023-11-08 14:51:37,168 [kafka-admin-client-thread | adminclient-2] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for adminclient-2 unregistered
[INFO] 2023-11-08 14:51:37,169 [kafka-admin-client-thread | adminclient-2] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:37,169 [kafka-admin-client-thread | adminclient-2] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:37,169 [kafka-admin-client-thread | adminclient-2] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:37,170 [confluent-metrics-reporter-scheduler] io.confluent.metrics.reporter.ConfluentMetricsReporter createTopicIfNotPresent - Attempted to create metrics reporter topic _confluent-metrics but the topic was already created. This may happen the first time ConfluentMetricsReporter is started and multiple brokers attempt to create the topic simultaneously.
[WARN] 2023-11-08 14:51:37,290 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.NetworkClient handleSuccessfulResponse - [Producer clientId=confluent-metrics-reporter] Error while fetching metadata with correlation id 3 : {_confluent-metrics=UNKNOWN_TOPIC_OR_PARTITION}
[WARN] 2023-11-08 14:51:37,792 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.NetworkClient handleSuccessfulResponse - [Producer clientId=confluent-metrics-reporter] Error while fetching metadata with correlation id 4 : {_confluent-metrics=UNKNOWN_TOPIC_OR_PARTITION}
[INFO] 2023-11-08 14:51:37,908 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 11 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:51:37,918 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-metrics-4, _confluent-metrics-7, _confluent-metrics-10, _confluent-metrics-1)
[INFO] 2023-11-08 14:51:37,918 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 11 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:51:37,924 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,924 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-4 in /mnt/data/data0/logs/_confluent-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,925 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-4 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-4
[INFO] 2023-11-08 14:51:37,925 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-4 broker=2] Log loaded for partition _confluent-metrics-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,925 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-metrics-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,929 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,930 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-7 in /mnt/data/data0/logs/_confluent-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,930 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-7 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-7
[INFO] 2023-11-08 14:51:37,930 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-7 broker=2] Log loaded for partition _confluent-metrics-7 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,930 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-metrics-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,934 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,935 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-10 in /mnt/data/data0/logs/_confluent-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,935 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-10 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-10
[INFO] 2023-11-08 14:51:37,935 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-10 broker=2] Log loaded for partition _confluent-metrics-10 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,936 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-metrics-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,940 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,940 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-1 in /mnt/data/data0/logs/_confluent-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,940 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-1 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-1
[INFO] 2023-11-08 14:51:37,941 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-1 broker=2] Log loaded for partition _confluent-metrics-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,941 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-metrics-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,944 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,945 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-5 in /mnt/data/data0/logs/_confluent-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,945 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-5 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-5
[INFO] 2023-11-08 14:51:37,945 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-5 broker=2] Log loaded for partition _confluent-metrics-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,945 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-metrics-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,949 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,950 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-6 in /mnt/data/data0/logs/_confluent-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,950 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-6 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-6
[INFO] 2023-11-08 14:51:37,950 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-6 broker=2] Log loaded for partition _confluent-metrics-6 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,950 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-metrics-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,953 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,954 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-9 in /mnt/data/data0/logs/_confluent-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,954 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-9 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-9
[INFO] 2023-11-08 14:51:37,954 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-9 broker=2] Log loaded for partition _confluent-metrics-9 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,954 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-metrics-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,957 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,958 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-8 in /mnt/data/data0/logs/_confluent-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,958 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-8 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-8
[INFO] 2023-11-08 14:51:37,958 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-8 broker=2] Log loaded for partition _confluent-metrics-8 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,958 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-metrics-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,962 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,963 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-11 in /mnt/data/data0/logs/_confluent-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,963 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-11 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-11
[INFO] 2023-11-08 14:51:37,963 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-11 broker=2] Log loaded for partition _confluent-metrics-11 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,963 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-metrics-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,967 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,968 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-0 in /mnt/data/data0/logs/_confluent-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,968 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-0 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-0
[INFO] 2023-11-08 14:51:37,968 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-0 broker=2] Log loaded for partition _confluent-metrics-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,968 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-metrics-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,971 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,972 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-3 in /mnt/data/data0/logs/_confluent-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,972 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-3 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-3
[INFO] 2023-11-08 14:51:37,972 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-3 broker=2] Log loaded for partition _confluent-metrics-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,972 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-metrics-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,975 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,976 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-metrics-2 in /mnt/data/data0/logs/_confluent-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,976 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-2 broker=2] No checkpointed highwatermark is found for partition _confluent-metrics-2
[INFO] 2023-11-08 14:51:37,976 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-metrics-2 broker=2] Log loaded for partition _confluent-metrics-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,976 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-metrics-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,976 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-metrics-11, _confluent-metrics-9, _confluent-metrics-8, _confluent-metrics-5, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-2, _confluent-metrics-0)
[INFO] 2023-11-08 14:51:37,976 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 11 for 8 partitions
[INFO] 2023-11-08 14:51:37,977 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-metrics-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:37,978 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-metrics-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:37,978 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 70ms correlationId 11 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:51:37,981 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12
[INFO] 2023-11-08 14:51:38,190 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-metrics-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,190 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,190 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-metrics-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,191 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,191 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-metrics-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,191 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,191 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-metrics-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,191 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,272 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-metrics-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,272 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-metrics-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-metrics-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-metrics-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[DEBUG] 2023-11-08 14:51:39,356 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:39,358 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:40,187 [qtp1625817721-195] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 195]
[TRACE] 2023-11-08 14:51:40,188 [qtp1625817721-195] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 195]
[DEBUG] 2023-11-08 14:51:40,190 [qtp1625817721-195] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:51:40,191 [qtp1625817721-195] io.confluent.mds.request.logger log - 30 * Server has received a request on thread qtp1625817721-195null30 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull30 > User Principal: c3
30 > authorization: Basic YzM6YzMtc2VjcmV0null30 > host: kafka.confluent.svc.cluster.local:8090null30 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:51:40,192 [qtp1625817721-195] io.confluent.mds.request.logger log - 30 * Server responded with a response on thread qtp1625817721-195null30 < 200null30 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:51:40,193 [qtp1625817721-195] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:51:40 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:51:40,193 [qtp1625817721-195] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:51:40 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:51:40,193 [qtp1625817721-195] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:51:40 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[DEBUG] 2023-11-08 14:51:42,356 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:42,358 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:43,844 [qtp1625817721-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 180]
[TRACE] 2023-11-08 14:51:43,845 [qtp1625817721-174] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 174]
[TRACE] 2023-11-08 14:51:43,846 [qtp1625817721-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 180]
[TRACE] 2023-11-08 14:51:43,847 [qtp1625817721-174] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 174]
[DEBUG] 2023-11-08 14:51:43,849 [qtp1625817721-180] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[INFO] 2023-11-08 14:51:43,850 [qtp1625817721-180] io.confluent.mds.request.logger log - 31 * Server has received a request on thread qtp1625817721-180null31 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull31 > User Principal: ksql
31 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null31 > Cache-Control: no-cachenull31 > Connection: keep-alivenull31 > Content-Type: application/jsonnull31 > Host: kafka.confluent.svc.cluster.local:8090null31 > Pragma: no-cachenull31 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:43,852 [qtp1625817721-180] io.confluent.mds.request.logger log - 31 * Server responded with a response on thread qtp1625817721-180null31 < 200null31 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:43,853 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:43 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:43,853 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:43 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:43,853 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:43 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[DEBUG] 2023-11-08 14:51:43,855 [qtp1625817721-174] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[INFO] 2023-11-08 14:51:43,856 [qtp1625817721-174] io.confluent.mds.request.logger log - 32 * Server has received a request on thread qtp1625817721-174null32 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull32 > User Principal: ksql
32 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null32 > Cache-Control: no-cachenull32 > Connection: keep-alivenull32 > Content-Type: application/jsonnull32 > Host: kafka.confluent.svc.cluster.local:8090null32 > Pragma: no-cachenull32 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:43,862 [qtp1625817721-174] io.confluent.mds.request.logger log - 32 * Server responded with a response on thread qtp1625817721-174null32 < 200null32 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:43,864 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:43 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 636 "-" "Java/11.0.14.1" 20
[INFO] 2023-11-08 14:51:43,864 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:43 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 636 "-" "Java/11.0.14.1" 20
[INFO] 2023-11-08 14:51:43,864 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:43 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 636 "-" "Java/11.0.14.1" 20
[TRACE] 2023-11-08 14:51:44,646 [qtp1625817721-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 246]
[TRACE] 2023-11-08 14:51:44,647 [qtp1625817721-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 246]
[DEBUG] 2023-11-08 14:51:44,649 [qtp1625817721-246] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[DEBUG] 2023-11-08 14:51:44,653 [qtp1625817721-246] io.confluent.rbacapi.rest.MdsWriterProxyServlet extractProxyUri - Forwarding request to leader https://kafka-0.kafka.confluent.svc.cluster.local:8090
[DEBUG] 2023-11-08 14:51:44,653 [qtp1625817721-246] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 service - 955840331 rewriting: https://kafka.confluent.svc.cluster.local:8090/security/leader/security/1.0/principals/User:c3/roles/SystemAdmin -> https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:c3/roles/SystemAdmin
[DEBUG] 2023-11-08 14:51:44,665 [qtp1625817721-246] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 sendProxyRequest - 955840331 proxying to upstream:
POST /security/leader/security/1.0/principals/User:c3/roles/SystemAdmin HTTP/1.1
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Host: kafka.confluent.svc.cluster.local:8090
Content-Length: 56
Content-Type: application/json

HttpRequest[POST /security/1.0/principals/User:c3/roles/SystemAdmin HTTP/1.1]@4c5d2d14
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Content-Length: 56
Content-Type: application/json
Via: 1.1 kafka-2
X-Forwarded-For: 10.40.2.7
X-Forwarded-Proto: https
X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090
X-Forwarded-Server: 10.40.0.15
[DEBUG] 2023-11-08 14:51:44,691 [qtp1625817721-246] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 955840331 asynchronous read 56 bytes on HttpInputOverHTTP@37a6158e[c=56,q=0,[0]=null,s=ASYNC]
[DEBUG] 2023-11-08 14:51:44,691 [qtp1625817721-246] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 955840331 proxying content to upstream: 56 bytes
[DEBUG] 2023-11-08 14:51:44,726 [qtp1625817721-244] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 955840331 asynchronous read -1 bytes on HttpInputOverHTTP@37a6158e[c=56,q=0,[0]=null,s=AEOF]
[DEBUG] 2023-11-08 14:51:44,726 [qtp1625817721-244] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 955840331 asynchronous read complete on HttpInputOverHTTP@37a6158e[c=56,q=0,[0]=null,s=EOF]
[DEBUG] 2023-11-08 14:51:44,726 [qtp1625817721-251] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onAllDataRead - 955840331 proxying content to upstream completed
[DEBUG] 2023-11-08 14:51:44,927 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:3 key RoleBindingKey{principal=User:c3, role='SystemAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:51:44,930 [qtp1625817721-180] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onServerResponseHeaders - 955840331 proxying to downstream:
HttpResponse[HTTP/1.1 204 No Content]@36136bb3
Date: Wed, 08 Nov 2023 14:51:44 GMT

[DEBUG] 2023-11-08 14:51:44,933 [qtp1625817721-180] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onProxyResponseSuccess - 955840331 proxying successful
[INFO] 2023-11-08 14:51:44,935 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:44 +0000] "POST /security/1.0/principals/User:c3/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 290
[INFO] 2023-11-08 14:51:44,935 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:44 +0000] "POST /security/1.0/principals/User:c3/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 290
[INFO] 2023-11-08 14:51:44,935 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:44 +0000] "POST /security/1.0/principals/User:c3/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 290
[DEBUG] 2023-11-08 14:51:44,935 [qtp1625817721-180] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onComplete - 955840331 proxying complete
[TRACE] 2023-11-08 14:51:45,012 [qtp1625817721-253] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 253]
[TRACE] 2023-11-08 14:51:45,014 [qtp1625817721-253] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 253]
[DEBUG] 2023-11-08 14:51:45,016 [qtp1625817721-253] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[DEBUG] 2023-11-08 14:51:45,017 [qtp1625817721-253] io.confluent.rbacapi.rest.MdsWriterProxyServlet extractProxyUri - Forwarding request to leader https://kafka-0.kafka.confluent.svc.cluster.local:8090
[DEBUG] 2023-11-08 14:51:45,017 [qtp1625817721-253] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 service - 303751026 rewriting: https://kafka.confluent.svc.cluster.local:8090/security/leader/security/1.0/principals/User:ksql/roles/ResourceOwner/bindings -> https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:ksql/roles/ResourceOwner/bindings
[DEBUG] 2023-11-08 14:51:45,018 [qtp1625817721-253] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 sendProxyRequest - 303751026 proxying to upstream:
POST /security/leader/security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Host: kafka.confluent.svc.cluster.local:8090
Content-Length: 199
Content-Type: application/json

HttpRequest[POST /security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1]@1cdd2341
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Content-Length: 199
Content-Type: application/json
Via: 1.1 kafka-2
X-Forwarded-For: 10.40.2.7
X-Forwarded-Proto: https
X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090
X-Forwarded-Server: 10.40.0.15
[DEBUG] 2023-11-08 14:51:45,019 [qtp1625817721-253] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 303751026 asynchronous read 199 bytes on HttpInputOverHTTP@18e2b402[c=199,q=0,[0]=null,s=ASYNC]
[DEBUG] 2023-11-08 14:51:45,019 [qtp1625817721-253] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 303751026 proxying content to upstream: 199 bytes
[DEBUG] 2023-11-08 14:51:45,020 [qtp1625817721-253] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 303751026 asynchronous read -1 bytes on HttpInputOverHTTP@18e2b402[c=199,q=0,[0]=null,s=AEOF]
[DEBUG] 2023-11-08 14:51:45,020 [qtp1625817721-253] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 303751026 asynchronous read complete on HttpInputOverHTTP@18e2b402[c=199,q=0,[0]=null,s=AEOF]
[DEBUG] 2023-11-08 14:51:45,020 [qtp1625817721-253] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onAllDataRead - 303751026 proxying content to upstream completed
[DEBUG] 2023-11-08 14:51:45,059 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:4 key RoleBindingKey{principal=User:ksql, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, ksql-cluster=confluent.ksqldb_}')'} newValue RoleBindingValue(resources=[KsqlCluster:LITERAL:ksql-cluster]) oldValue null
[DEBUG] 2023-11-08 14:51:45,061 [qtp1625817721-192] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onServerResponseHeaders - 303751026 proxying to downstream:
HttpResponse[HTTP/1.1 204 No Content]@46369ac
Date: Wed, 08 Nov 2023 14:51:45 GMT

[DEBUG] 2023-11-08 14:51:45,063 [qtp1625817721-192] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onProxyResponseSuccess - 303751026 proxying successful
[INFO] 2023-11-08 14:51:45,064 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 53
[INFO] 2023-11-08 14:51:45,064 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 53
[INFO] 2023-11-08 14:51:45,064 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 53
[DEBUG] 2023-11-08 14:51:45,064 [qtp1625817721-192] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onComplete - 303751026 proxying complete
[DEBUG] 2023-11-08 14:51:45,244 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:5 key RoleBindingKey{principal=User:ksql, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:LITERAL:_confluent-ksql-confluent.ksqldb__command_topic, Group:PREFIXED:_confluent-ksql-confluent.ksqldb_, Topic:LITERAL:confluent.ksqldb_ksql_processing_log]) oldValue null
[DEBUG] 2023-11-08 14:51:45,355 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:3 key RoleBindingKey{principal=User:ksql, role='DeveloperWrite', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Cluster:LITERAL:kafka-cluster, TransactionalId:PREFIXED:confluent.ksqldb_]) oldValue null
[DEBUG] 2023-11-08 14:51:45,357 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:45,358 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:45,931 [qtp1625817721-248] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 248]
[TRACE] 2023-11-08 14:51:45,932 [qtp1625817721-248] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 248]
[DEBUG] 2023-11-08 14:51:45,934 [qtp1625817721-248] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[DEBUG] 2023-11-08 14:51:45,935 [qtp1625817721-248] io.confluent.rbacapi.rest.MdsWriterProxyServlet extractProxyUri - Forwarding request to leader https://kafka-0.kafka.confluent.svc.cluster.local:8090
[DEBUG] 2023-11-08 14:51:45,935 [qtp1625817721-248] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 service - 1586663646 rewriting: https://kafka.confluent.svc.cluster.local:8090/security/leader/security/1.0/principals/User:sr/roles/SecurityAdmin -> https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:sr/roles/SecurityAdmin
[DEBUG] 2023-11-08 14:51:45,936 [qtp1625817721-248] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 sendProxyRequest - 1586663646 proxying to upstream:
POST /security/leader/security/1.0/principals/User:sr/roles/SecurityAdmin HTTP/1.1
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Host: kafka.confluent.svc.cluster.local:8090
Content-Length: 112
Content-Type: application/json

HttpRequest[POST /security/1.0/principals/User:sr/roles/SecurityAdmin HTTP/1.1]@5055757c
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Content-Length: 112
Content-Type: application/json
Via: 1.1 kafka-2
X-Forwarded-For: 10.40.2.7
X-Forwarded-Proto: https
X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090
X-Forwarded-Server: 10.40.0.15
[DEBUG] 2023-11-08 14:51:45,936 [qtp1625817721-248] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 1586663646 asynchronous read 112 bytes on HttpInputOverHTTP@546afbae[c=112,q=0,[0]=null,s=ASYNC]
[DEBUG] 2023-11-08 14:51:45,937 [qtp1625817721-248] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 1586663646 proxying content to upstream: 112 bytes
[DEBUG] 2023-11-08 14:51:45,937 [qtp1625817721-248] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 1586663646 asynchronous read -1 bytes on HttpInputOverHTTP@546afbae[c=112,q=0,[0]=null,s=AEOF]
[DEBUG] 2023-11-08 14:51:45,937 [qtp1625817721-248] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 1586663646 asynchronous read complete on HttpInputOverHTTP@546afbae[c=112,q=0,[0]=null,s=AEOF]
[DEBUG] 2023-11-08 14:51:45,937 [qtp1625817721-248] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onAllDataRead - 1586663646 proxying content to upstream completed
[DEBUG] 2023-11-08 14:51:45,969 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:3 key RoleBindingKey{principal=User:sr, role='SecurityAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, schema-registry-cluster=id_schemaregistry_confluent}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:51:45,972 [qtp1625817721-180] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onServerResponseHeaders - 1586663646 proxying to downstream:
HttpResponse[HTTP/1.1 204 No Content]@16e6f033
Date: Wed, 08 Nov 2023 14:51:45 GMT

[DEBUG] 2023-11-08 14:51:45,973 [qtp1625817721-180] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onProxyResponseSuccess - 1586663646 proxying successful
[INFO] 2023-11-08 14:51:45,974 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:sr/roles/SecurityAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 43
[INFO] 2023-11-08 14:51:45,974 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:sr/roles/SecurityAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 43
[INFO] 2023-11-08 14:51:45,974 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:sr/roles/SecurityAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 43
[DEBUG] 2023-11-08 14:51:45,974 [qtp1625817721-180] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onComplete - 1586663646 proxying complete
[DEBUG] 2023-11-08 14:51:46,252 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:4 key RoleBindingKey{principal=User:sr, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:LITERAL:_confluent-license, Topic:LITERAL:_confluent-command, Group:LITERAL:id_schemaregistry_confluent, Topic:LITERAL:_exporter_states, Topic:LITERAL:_schemas_schemaregistry_confluent, Topic:LITERAL:_exporter_configs]) oldValue null
[DEBUG] 2023-11-08 14:51:46,441 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:3 key RoleBindingKey{principal=User:connect, role='SecurityAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, connect-cluster=confluent.connect}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:51:46,641 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:4 key RoleBindingKey{principal=User:connect, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:PREFIXED:confluent.connect-, Group:LITERAL:confluent.connect]) oldValue null
[TRACE] 2023-11-08 14:51:46,723 [qtp1625817721-241] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 241]
[TRACE] 2023-11-08 14:51:46,724 [qtp1625817721-241] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 241]
[DEBUG] 2023-11-08 14:51:46,726 [qtp1625817721-241] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[DEBUG] 2023-11-08 14:51:46,727 [qtp1625817721-241] io.confluent.rbacapi.rest.MdsWriterProxyServlet extractProxyUri - Forwarding request to leader https://kafka-0.kafka.confluent.svc.cluster.local:8090
[DEBUG] 2023-11-08 14:51:46,727 [qtp1625817721-241] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 service - 43411060 rewriting: https://kafka.confluent.svc.cluster.local:8090/security/leader/security/1.0/principals/User:connect/roles/DeveloperWrite/bindings -> https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:connect/roles/DeveloperWrite/bindings
[DEBUG] 2023-11-08 14:51:46,728 [qtp1625817721-241] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 sendProxyRequest - 43411060 proxying to upstream:
POST /security/leader/security/1.0/principals/User:connect/roles/DeveloperWrite/bindings HTTP/1.1
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Host: kafka.confluent.svc.cluster.local:8090
Content-Length: 168
Content-Type: application/json

HttpRequest[POST /security/1.0/principals/User:connect/roles/DeveloperWrite/bindings HTTP/1.1]@c518856
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Content-Length: 168
Content-Type: application/json
Via: 1.1 kafka-2
X-Forwarded-For: 10.40.2.7
X-Forwarded-Proto: https
X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090
X-Forwarded-Server: 10.40.0.15
[DEBUG] 2023-11-08 14:51:46,729 [qtp1625817721-241] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 43411060 asynchronous read 168 bytes on HttpInputOverHTTP@86e0d04[c=168,q=0,[0]=null,s=ASYNC]
[DEBUG] 2023-11-08 14:51:46,729 [qtp1625817721-241] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 43411060 proxying content to upstream: 168 bytes
[DEBUG] 2023-11-08 14:51:46,729 [qtp1625817721-241] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 43411060 asynchronous read -1 bytes on HttpInputOverHTTP@86e0d04[c=168,q=0,[0]=null,s=AEOF]
[DEBUG] 2023-11-08 14:51:46,730 [qtp1625817721-241] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 43411060 asynchronous read complete on HttpInputOverHTTP@86e0d04[c=168,q=0,[0]=null,s=AEOF]
[DEBUG] 2023-11-08 14:51:46,730 [qtp1625817721-241] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onAllDataRead - 43411060 proxying content to upstream completed
[DEBUG] 2023-11-08 14:51:46,749 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:3 key RoleBindingKey{principal=User:connect, role='DeveloperWrite', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:PREFIXED:_confluent-monitoring]) oldValue null
[DEBUG] 2023-11-08 14:51:46,751 [qtp1625817721-192] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onServerResponseHeaders - 43411060 proxying to downstream:
HttpResponse[HTTP/1.1 204 No Content]@29d16294
Date: Wed, 08 Nov 2023 14:51:46 GMT

[DEBUG] 2023-11-08 14:51:46,752 [qtp1625817721-192] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onProxyResponseSuccess - 43411060 proxying successful
[INFO] 2023-11-08 14:51:46,753 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 31
[INFO] 2023-11-08 14:51:46,753 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 31
[INFO] 2023-11-08 14:51:46,753 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 31
[DEBUG] 2023-11-08 14:51:46,753 [qtp1625817721-192] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onComplete - 43411060 proxying complete
[TRACE] 2023-11-08 14:51:47,833 [qtp1625817721-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 192]
[TRACE] 2023-11-08 14:51:47,835 [qtp1625817721-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 192]
[DEBUG] 2023-11-08 14:51:47,837 [qtp1625817721-192] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[INFO] 2023-11-08 14:51:47,838 [qtp1625817721-192] io.confluent.mds.request.logger log - 33 * Server has received a request on thread qtp1625817721-192null33 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull33 > User Principal: ksql
33 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null33 > Cache-Control: no-cachenull33 > Connection: keep-alivenull33 > Content-Type: application/jsonnull33 > Host: kafka.confluent.svc.cluster.local:8090null33 > Pragma: no-cachenull33 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:47,840 [qtp1625817721-192] io.confluent.mds.request.logger log - 33 * Server responded with a response on thread qtp1625817721-192null33 < 200null33 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:47,841 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:47 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:47,841 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:47 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:47,841 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:47 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[TRACE] 2023-11-08 14:51:48,094 [qtp1625817721-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 192]
[TRACE] 2023-11-08 14:51:48,095 [qtp1625817721-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 192]
[DEBUG] 2023-11-08 14:51:48,099 [qtp1625817721-192] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[INFO] 2023-11-08 14:51:48,100 [qtp1625817721-192] io.confluent.mds.request.logger log - 34 * Server has received a request on thread qtp1625817721-192null34 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull34 > User Principal: ksql
34 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null34 > Cache-Control: no-cachenull34 > Connection: keep-alivenull34 > Content-Type: application/jsonnull34 > Host: kafka.confluent.svc.cluster.local:8090null34 > Pragma: no-cachenull34 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:48,101 [qtp1625817721-192] io.confluent.mds.request.logger log - 34 * Server responded with a response on thread qtp1625817721-192null34 < 200null34 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:48,102 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:48 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:48,102 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:48 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:48,102 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.1.9 - ksql [08/Nov/2023:14:51:48 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[DEBUG] 2023-11-08 14:51:48,357 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:48,358 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:51:48,476 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:48,478 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,478 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:48,478 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:48,478 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:48,478 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455108478
[INFO] 2023-11-08 14:51:48,497 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:48,499 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,500 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:48,500 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:48,500 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:48,500 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455108500
[INFO] 2023-11-08 14:51:48,507 [kafka-admin-client-thread | adminclient-3] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for adminclient-3 unregistered
[INFO] 2023-11-08 14:51:48,508 [kafka-admin-client-thread | adminclient-3] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:48,508 [kafka-admin-client-thread | adminclient-3] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:48,508 [kafka-admin-client-thread | adminclient-3] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:48,531 [kafka-admin-client-thread | adminclient-4] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for adminclient-4 unregistered
[INFO] 2023-11-08 14:51:48,532 [kafka-admin-client-thread | adminclient-4] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:48,533 [kafka-admin-client-thread | adminclient-4] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:48,533 [kafka-admin-client-thread | adminclient-4] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[DEBUG] 2023-11-08 14:51:48,536 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:5 key RoleBindingKey{principal=User:krp, role='DeveloperWrite', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:PREFIXED:_confluent-monitoring]) oldValue null
[INFO] 2023-11-08 14:51:48,686 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:51:48,686 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:48,687 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:48,689 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,689 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:48,690 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:48,690 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:48,690 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455108689
[WARN] 2023-11-08 14:51:48,692 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,692 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,692 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,692 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,692 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,692 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,692 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,692 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,695 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:48,695 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:48,695 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:48,695 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455108695
[TRACE] 2023-11-08 14:51:48,701 [qtp1625817721-247] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 247]
[TRACE] 2023-11-08 14:51:48,703 [qtp1625817721-247] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 247]
[DEBUG] 2023-11-08 14:51:48,705 [qtp1625817721-247] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[DEBUG] 2023-11-08 14:51:48,706 [qtp1625817721-247] io.confluent.rbacapi.rest.MdsWriterProxyServlet extractProxyUri - Forwarding request to leader https://kafka-0.kafka.confluent.svc.cluster.local:8090
[DEBUG] 2023-11-08 14:51:48,706 [qtp1625817721-247] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 service - 406423750 rewriting: https://kafka.confluent.svc.cluster.local:8090/security/leader/security/1.0/principals/User:krp/roles/ResourceOwner/bindings -> https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:krp/roles/ResourceOwner/bindings
[DEBUG] 2023-11-08 14:51:48,706 [qtp1625817721-247] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 sendProxyRequest - 406423750 proxying to upstream:
POST /security/leader/security/1.0/principals/User:krp/roles/ResourceOwner/bindings HTTP/1.1
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Host: kafka.confluent.svc.cluster.local:8090
Content-Length: 241
Content-Type: application/json

HttpRequest[POST /security/1.0/principals/User:krp/roles/ResourceOwner/bindings HTTP/1.1]@16fe65b1
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Content-Length: 241
Content-Type: application/json
Via: 1.1 kafka-2
X-Forwarded-For: 10.40.2.7
X-Forwarded-Proto: https
X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090
X-Forwarded-Server: 10.40.0.15
[INFO] 2023-11-08 14:51:48,707 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:48,707 [qtp1625817721-247] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 406423750 asynchronous read 241 bytes on HttpInputOverHTTP@12c1e3b6[c=241,q=0,[0]=null,s=ASYNC]
[DEBUG] 2023-11-08 14:51:48,707 [qtp1625817721-247] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 406423750 proxying content to upstream: 241 bytes
[DEBUG] 2023-11-08 14:51:48,707 [qtp1625817721-247] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 406423750 asynchronous read -1 bytes on HttpInputOverHTTP@12c1e3b6[c=241,q=0,[0]=null,s=AEOF]
[DEBUG] 2023-11-08 14:51:48,708 [qtp1625817721-247] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 406423750 asynchronous read complete on HttpInputOverHTTP@12c1e3b6[c=241,q=0,[0]=null,s=AEOF]
[DEBUG] 2023-11-08 14:51:48,708 [qtp1625817721-247] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onAllDataRead - 406423750 proxying content to upstream completed
[INFO] 2023-11-08 14:51:48,720 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 13
[INFO] 2023-11-08 14:51:48,726 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[DEBUG] 2023-11-08 14:51:48,726 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:6 key RoleBindingKey{principal=User:krp, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:LITERAL:_confluent-license, Topic:LITERAL:_confluent-command]) oldValue null
[INFO] 2023-11-08 14:51:48,727 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:48,727 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:51:48,727 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:51:48,727 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:51:48,728 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:51:48,729 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:48,729 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[DEBUG] 2023-11-08 14:51:48,729 [qtp1625817721-180] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onServerResponseHeaders - 406423750 proxying to downstream:
HttpResponse[HTTP/1.1 204 No Content]@6ca21fe5
Date: Wed, 08 Nov 2023 14:51:48 GMT

[INFO] 2023-11-08 14:51:48,729 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:48,730 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:51:48,730 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[DEBUG] 2023-11-08 14:51:48,730 [qtp1625817721-180] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onProxyResponseSuccess - 406423750 proxying successful
[INFO] 2023-11-08 14:51:48,730 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:48,731 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[INFO] 2023-11-08 14:51:48,731 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:48 +0000] "POST /security/1.0/principals/User:krp/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 30
[INFO] 2023-11-08 14:51:48,731 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:48 +0000] "POST /security/1.0/principals/User:krp/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 30
[INFO] 2023-11-08 14:51:48,731 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:48 +0000] "POST /security/1.0/principals/User:krp/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 30
[DEBUG] 2023-11-08 14:51:48,731 [qtp1625817721-180] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onComplete - 406423750 proxying complete
[DEBUG] 2023-11-08 14:51:51,360 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:51,362 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:51:54,359 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:54,361 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:51:57,359 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:57,360 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:00,359 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:00,361 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:01,855 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 14 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:52:01,857 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_schemas_schemaregistry_confluent-0)
[INFO] 2023-11-08 14:52:01,857 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 14 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions
[INFO] 2023-11-08 14:52:01,866 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_schemas_schemaregistry_confluent-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:01,867 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _schemas_schemaregistry_confluent-0 in /mnt/data/data0/logs/_schemas_schemaregistry_confluent-0 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:01,868 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _schemas_schemaregistry_confluent-0 broker=2] No checkpointed highwatermark is found for partition _schemas_schemaregistry_confluent-0
[INFO] 2023-11-08 14:52:01,868 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _schemas_schemaregistry_confluent-0 broker=2] Log loaded for partition _schemas_schemaregistry_confluent-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:01,868 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _schemas_schemaregistry_confluent-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:01,869 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 14ms correlationId 14 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:52:01,871 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 15
[INFO] 2023-11-08 14:52:02,551 [data-plane-kafka-request-handler-7] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Dynamic member with unknown member id joins group id_schemaregistry_confluent in Empty state. Created a new member id sr-1-54ef5fb9-ff4a-4f2f-b76e-1fc185d8981f and request the member to rejoin with this id.
[INFO] 2023-11-08 14:52:02,569 [data-plane-kafka-request-handler-1] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Preparing to rebalance group id_schemaregistry_confluent in state PreparingRebalance with old generation 0 (__consumer_offsets-25) (reason: Adding new member sr-1-54ef5fb9-ff4a-4f2f-b76e-1fc185d8981f with group instance id None)
[DEBUG] 2023-11-08 14:52:03,359 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:03,385 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:05,159 [qtp1625817721-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 181]
[TRACE] 2023-11-08 14:52:05,159 [qtp1625817721-177] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 177]
[TRACE] 2023-11-08 14:52:05,162 [qtp1625817721-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 181]
[TRACE] 2023-11-08 14:52:05,162 [qtp1625817721-177] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 177]
[DEBUG] 2023-11-08 14:52:05,176 [qtp1625817721-177] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:52:05,176 [qtp1625817721-181] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:05,178 [qtp1625817721-177] io.confluent.mds.request.logger log - 35 * Server has received a request on thread qtp1625817721-177null35 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull35 > User Principal: connect
35 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null35 > Cache-Control: no-cachenull35 > Connection: keep-alivenull35 > Content-Type: application/jsonnull35 > Host: kafka.confluent.svc.cluster.local:8090null35 > Pragma: no-cachenull35 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:05,178 [qtp1625817721-181] io.confluent.mds.request.logger log - 36 * Server has received a request on thread qtp1625817721-181null36 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull36 > User Principal: connect
36 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null36 > Cache-Control: no-cachenull36 > Connection: keep-alivenull36 > Content-Type: application/jsonnull36 > Host: kafka.confluent.svc.cluster.local:8090null36 > Pragma: no-cachenull36 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:05,179 [qtp1625817721-177] io.confluent.mds.request.logger log - 35 * Server responded with a response on thread qtp1625817721-177null35 < 200null35 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:05,180 [qtp1625817721-177] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:05 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 21
[INFO] 2023-11-08 14:52:05,181 [qtp1625817721-177] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:05 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 22
[INFO] 2023-11-08 14:52:05,181 [qtp1625817721-177] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:05 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 22
[INFO] 2023-11-08 14:52:05,184 [qtp1625817721-181] io.confluent.mds.request.logger log - 36 * Server responded with a response on thread qtp1625817721-181null36 < 200null36 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:05,186 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:05 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 27
[INFO] 2023-11-08 14:52:05,186 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:05 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 27
[INFO] 2023-11-08 14:52:05,186 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:05 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 27
[INFO] 2023-11-08 14:52:05,575 [executor-Rebalance] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Stabilized group id_schemaregistry_confluent generation 1 (__consumer_offsets-25) with 1 members
[INFO] 2023-11-08 14:52:05,595 [data-plane-kafka-request-handler-2] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 2]: Assignment received from leader sr-1-54ef5fb9-ff4a-4f2f-b76e-1fc185d8981f for group id_schemaregistry_confluent for generation 1. The group has 1 members, 0 of which are static.
[DEBUG] 2023-11-08 14:52:06,360 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:06,361 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:06,443 [qtp1625817721-177] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 177]
[TRACE] 2023-11-08 14:52:06,443 [qtp1625817721-253] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 253]
[TRACE] 2023-11-08 14:52:06,445 [qtp1625817721-177] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 177]
[TRACE] 2023-11-08 14:52:06,445 [qtp1625817721-253] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 253]
[DEBUG] 2023-11-08 14:52:06,446 [qtp1625817721-177] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:52:06,447 [qtp1625817721-253] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:06,448 [qtp1625817721-177] io.confluent.mds.request.logger log - 37 * Server has received a request on thread qtp1625817721-177null37 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull37 > User Principal: connect
37 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null37 > Cache-Control: no-cachenull37 > Connection: keep-alivenull37 > Content-Type: application/jsonnull37 > Host: kafka.confluent.svc.cluster.local:8090null37 > Pragma: no-cachenull37 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:06,449 [qtp1625817721-253] io.confluent.mds.request.logger log - 38 * Server has received a request on thread qtp1625817721-253null38 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull38 > User Principal: connect
38 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null38 > Cache-Control: no-cachenull38 > Connection: keep-alivenull38 > Content-Type: application/jsonnull38 > Host: kafka.confluent.svc.cluster.local:8090null38 > Pragma: no-cachenull38 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:06,449 [qtp1625817721-177] io.confluent.mds.request.logger log - 37 * Server responded with a response on thread qtp1625817721-177null37 < 200null37 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:06,450 [qtp1625817721-177] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:52:06,450 [qtp1625817721-177] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:52:06,450 [qtp1625817721-177] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:52:06,453 [qtp1625817721-253] io.confluent.mds.request.logger log - 38 * Server responded with a response on thread qtp1625817721-253null38 < 200null38 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:06,454 [qtp1625817721-253] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:52:06,455 [qtp1625817721-253] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:52:06,455 [qtp1625817721-253] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[TRACE] 2023-11-08 14:52:06,575 [qtp1625817721-245] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 245]
[TRACE] 2023-11-08 14:52:06,575 [qtp1625817721-241] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 241]
[TRACE] 2023-11-08 14:52:06,577 [qtp1625817721-245] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 245]
[TRACE] 2023-11-08 14:52:06,577 [qtp1625817721-241] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 241]
[DEBUG] 2023-11-08 14:52:06,579 [qtp1625817721-245] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:52:06,580 [qtp1625817721-241] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:06,581 [qtp1625817721-245] io.confluent.mds.request.logger log - 39 * Server has received a request on thread qtp1625817721-245null39 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull39 > User Principal: connect
39 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null39 > Cache-Control: no-cachenull39 > Connection: keep-alivenull39 > Content-Type: application/jsonnull39 > Host: kafka.confluent.svc.cluster.local:8090null39 > Pragma: no-cachenull39 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:06,581 [qtp1625817721-241] io.confluent.mds.request.logger log - 40 * Server has received a request on thread qtp1625817721-241null40 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull40 > User Principal: connect
40 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null40 > Cache-Control: no-cachenull40 > Connection: keep-alivenull40 > Content-Type: application/jsonnull40 > Host: kafka.confluent.svc.cluster.local:8090null40 > Pragma: no-cachenull40 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:06,582 [qtp1625817721-245] io.confluent.mds.request.logger log - 39 * Server responded with a response on thread qtp1625817721-245null39 < 200null39 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:06,583 [qtp1625817721-245] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:06,583 [qtp1625817721-245] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:06,584 [qtp1625817721-245] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:06,586 [qtp1625817721-241] io.confluent.mds.request.logger log - 40 * Server responded with a response on thread qtp1625817721-241null40 < 200null40 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:06,587 [qtp1625817721-241] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:52:06,587 [qtp1625817721-241] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:52:06,587 [qtp1625817721-241] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[TRACE] 2023-11-08 14:52:06,704 [qtp1625817721-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 181]
[TRACE] 2023-11-08 14:52:06,704 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 188]
[TRACE] 2023-11-08 14:52:06,707 [qtp1625817721-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 181]
[TRACE] 2023-11-08 14:52:06,707 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 188]
[DEBUG] 2023-11-08 14:52:06,709 [qtp1625817721-181] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:52:06,709 [qtp1625817721-188] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:06,711 [qtp1625817721-188] io.confluent.mds.request.logger log - 41 * Server has received a request on thread qtp1625817721-188null41 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull41 > User Principal: connect
41 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null41 > Cache-Control: no-cachenull41 > Connection: keep-alivenull41 > Content-Type: application/jsonnull41 > Host: kafka.confluent.svc.cluster.local:8090null41 > Pragma: no-cachenull41 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:06,711 [qtp1625817721-181] io.confluent.mds.request.logger log - 42 * Server has received a request on thread qtp1625817721-181null42 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull42 > User Principal: connect
42 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null42 > Cache-Control: no-cachenull42 > Connection: keep-alivenull42 > Content-Type: application/jsonnull42 > Host: kafka.confluent.svc.cluster.local:8090null42 > Pragma: no-cachenull42 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:06,712 [qtp1625817721-181] io.confluent.mds.request.logger log - 42 * Server responded with a response on thread qtp1625817721-181null42 < 200null42 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:06,713 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:06,714 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:06,714 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:06,716 [qtp1625817721-188] io.confluent.mds.request.logger log - 41 * Server responded with a response on thread qtp1625817721-188null41 < 200null41 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:06,717 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:06,717 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:06,717 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[TRACE] 2023-11-08 14:52:06,828 [qtp1625817721-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 246]
[TRACE] 2023-11-08 14:52:06,828 [qtp1625817721-250] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 250]
[TRACE] 2023-11-08 14:52:06,830 [qtp1625817721-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 246]
[TRACE] 2023-11-08 14:52:06,831 [qtp1625817721-250] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 250]
[DEBUG] 2023-11-08 14:52:06,832 [qtp1625817721-246] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:52:06,833 [qtp1625817721-250] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:06,835 [qtp1625817721-246] io.confluent.mds.request.logger log - 43 * Server has received a request on thread qtp1625817721-246null43 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull43 > User Principal: connect
43 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null43 > Cache-Control: no-cachenull43 > Connection: keep-alivenull43 > Content-Type: application/jsonnull43 > Host: kafka.confluent.svc.cluster.local:8090null43 > Pragma: no-cachenull43 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:06,835 [qtp1625817721-250] io.confluent.mds.request.logger log - 44 * Server has received a request on thread qtp1625817721-250null44 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull44 > User Principal: connect
44 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null44 > Cache-Control: no-cachenull44 > Connection: keep-alivenull44 > Content-Type: application/jsonnull44 > Host: kafka.confluent.svc.cluster.local:8090null44 > Pragma: no-cachenull44 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:06,836 [qtp1625817721-246] io.confluent.mds.request.logger log - 43 * Server responded with a response on thread qtp1625817721-246null43 < 200null43 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:06,837 [qtp1625817721-246] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:06,837 [qtp1625817721-246] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:06,837 [qtp1625817721-246] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:06,840 [qtp1625817721-250] io.confluent.mds.request.logger log - 44 * Server responded with a response on thread qtp1625817721-250null44 < 200null44 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:06,842 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:52:06,842 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:52:06,842 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[TRACE] 2023-11-08 14:52:06,920 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 188]
[TRACE] 2023-11-08 14:52:06,923 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 188]
[TRACE] 2023-11-08 14:52:06,924 [qtp1625817721-156] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 156]
[DEBUG] 2023-11-08 14:52:06,925 [qtp1625817721-188] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[TRACE] 2023-11-08 14:52:06,927 [qtp1625817721-156] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 156]
[INFO] 2023-11-08 14:52:06,927 [qtp1625817721-188] io.confluent.mds.request.logger log - 45 * Server has received a request on thread qtp1625817721-188null45 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull45 > User Principal: connect
45 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null45 > Cache-Control: no-cachenull45 > Connection: keep-alivenull45 > Content-Type: application/jsonnull45 > Host: kafka.confluent.svc.cluster.local:8090null45 > Pragma: no-cachenull45 > User-Agent: Java/11.0.14.1null
[DEBUG] 2023-11-08 14:52:06,928 [qtp1625817721-156] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:06,930 [qtp1625817721-156] io.confluent.mds.request.logger log - 46 * Server has received a request on thread qtp1625817721-156null46 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull46 > User Principal: connect
46 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null46 > Cache-Control: no-cachenull46 > Connection: keep-alivenull46 > Content-Type: application/jsonnull46 > Host: kafka.confluent.svc.cluster.local:8090null46 > Pragma: no-cachenull46 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:06,931 [qtp1625817721-156] io.confluent.mds.request.logger log - 46 * Server responded with a response on thread qtp1625817721-156null46 < 200null46 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:06,931 [qtp1625817721-188] io.confluent.mds.request.logger log - 45 * Server responded with a response on thread qtp1625817721-188null45 < 200null45 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:06,932 [qtp1625817721-156] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:06,932 [qtp1625817721-156] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:06,932 [qtp1625817721-156] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:06,933 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:06,933 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:06,933 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[TRACE] 2023-11-08 14:52:07,022 [qtp1625817721-253] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 253]
[TRACE] 2023-11-08 14:52:07,022 [qtp1625817721-239] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 239]
[TRACE] 2023-11-08 14:52:07,024 [qtp1625817721-253] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 253]
[TRACE] 2023-11-08 14:52:07,025 [qtp1625817721-239] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 239]
[DEBUG] 2023-11-08 14:52:07,026 [qtp1625817721-253] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:52:07,027 [qtp1625817721-239] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:07,028 [qtp1625817721-253] io.confluent.mds.request.logger log - 47 * Server has received a request on thread qtp1625817721-253null47 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull47 > User Principal: connect
47 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null47 > Cache-Control: no-cachenull47 > Connection: keep-alivenull47 > Content-Type: application/jsonnull47 > Host: kafka.confluent.svc.cluster.local:8090null47 > Pragma: no-cachenull47 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:07,028 [qtp1625817721-239] io.confluent.mds.request.logger log - 48 * Server has received a request on thread qtp1625817721-239null48 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull48 > User Principal: connect
48 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null48 > Cache-Control: no-cachenull48 > Connection: keep-alivenull48 > Content-Type: application/jsonnull48 > Host: kafka.confluent.svc.cluster.local:8090null48 > Pragma: no-cachenull48 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:07,030 [qtp1625817721-253] io.confluent.mds.request.logger log - 47 * Server responded with a response on thread qtp1625817721-253null47 < 200null47 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:07,031 [qtp1625817721-253] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:07,031 [qtp1625817721-253] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:07,031 [qtp1625817721-253] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:07,034 [qtp1625817721-239] io.confluent.mds.request.logger log - 48 * Server responded with a response on thread qtp1625817721-239null48 < 200null48 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:07,035 [qtp1625817721-239] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:07,035 [qtp1625817721-239] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:07,035 [qtp1625817721-239] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[TRACE] 2023-11-08 14:52:07,115 [qtp1625817721-248] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 248]
[TRACE] 2023-11-08 14:52:07,117 [qtp1625817721-248] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 248]
[TRACE] 2023-11-08 14:52:07,118 [qtp1625817721-245] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 245]
[DEBUG] 2023-11-08 14:52:07,119 [qtp1625817721-248] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[TRACE] 2023-11-08 14:52:07,120 [qtp1625817721-245] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 245]
[INFO] 2023-11-08 14:52:07,121 [qtp1625817721-248] io.confluent.mds.request.logger log - 49 * Server has received a request on thread qtp1625817721-248null49 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull49 > User Principal: ksql
49 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null49 > Cache-Control: no-cachenull49 > Connection: keep-alivenull49 > Content-Type: application/jsonnull49 > Host: kafka.confluent.svc.cluster.local:8090null49 > Pragma: no-cachenull49 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:07,123 [qtp1625817721-248] io.confluent.mds.request.logger log - 49 * Server responded with a response on thread qtp1625817721-248null49 < 200null49 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[DEBUG] 2023-11-08 14:52:07,123 [qtp1625817721-245] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[INFO] 2023-11-08 14:52:07,124 [qtp1625817721-248] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:52:07,124 [qtp1625817721-248] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:52:07,124 [qtp1625817721-245] io.confluent.mds.request.logger log - 50 * Server has received a request on thread qtp1625817721-245null50 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull50 > User Principal: ksql
50 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null50 > Cache-Control: no-cachenull50 > Connection: keep-alivenull50 > Content-Type: application/jsonnull50 > Host: kafka.confluent.svc.cluster.local:8090null50 > Pragma: no-cachenull50 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:07,125 [qtp1625817721-248] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:07,130 [qtp1625817721-245] io.confluent.mds.request.logger log - 50 * Server responded with a response on thread qtp1625817721-245null50 < 200null50 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:07,131 [qtp1625817721-245] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 636 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:52:07,132 [qtp1625817721-245] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 636 "-" "Java/11.0.14.1" 15
[INFO] 2023-11-08 14:52:07,132 [qtp1625817721-245] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 636 "-" "Java/11.0.14.1" 15
[TRACE] 2023-11-08 14:52:07,190 [qtp1625817721-242] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 242]
[TRACE] 2023-11-08 14:52:07,192 [qtp1625817721-242] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 242]
[TRACE] 2023-11-08 14:52:07,195 [qtp1625817721-240] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 240]
[TRACE] 2023-11-08 14:52:07,199 [qtp1625817721-240] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 240]
[DEBUG] 2023-11-08 14:52:07,199 [qtp1625817721-242] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:07,201 [qtp1625817721-242] io.confluent.mds.request.logger log - 51 * Server has received a request on thread qtp1625817721-242null51 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull51 > User Principal: connect
51 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null51 > Cache-Control: no-cachenull51 > Connection: keep-alivenull51 > Content-Type: application/jsonnull51 > Host: kafka.confluent.svc.cluster.local:8090null51 > Pragma: no-cachenull51 > User-Agent: Java/11.0.14.1null
[DEBUG] 2023-11-08 14:52:07,201 [qtp1625817721-240] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:07,202 [qtp1625817721-242] io.confluent.mds.request.logger log - 51 * Server responded with a response on thread qtp1625817721-242null51 < 200null51 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:07,202 [qtp1625817721-240] io.confluent.mds.request.logger log - 52 * Server has received a request on thread qtp1625817721-240null52 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull52 > User Principal: connect
52 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null52 > Cache-Control: no-cachenull52 > Connection: keep-alivenull52 > Content-Type: application/jsonnull52 > Host: kafka.confluent.svc.cluster.local:8090null52 > Pragma: no-cachenull52 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:07,204 [qtp1625817721-242] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 15
[INFO] 2023-11-08 14:52:07,205 [qtp1625817721-242] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 16
[INFO] 2023-11-08 14:52:07,205 [qtp1625817721-242] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 16
[INFO] 2023-11-08 14:52:07,209 [qtp1625817721-240] io.confluent.mds.request.logger log - 52 * Server responded with a response on thread qtp1625817721-240null52 < 200null52 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:07,210 [qtp1625817721-240] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 16
[INFO] 2023-11-08 14:52:07,210 [qtp1625817721-240] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 16
[INFO] 2023-11-08 14:52:07,210 [qtp1625817721-240] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 16
[TRACE] 2023-11-08 14:52:07,333 [qtp1625817721-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 246]
[TRACE] 2023-11-08 14:52:07,334 [qtp1625817721-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 246]
[DEBUG] 2023-11-08 14:52:07,336 [qtp1625817721-246] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:07,338 [qtp1625817721-246] io.confluent.mds.request.logger log - 53 * Server has received a request on thread qtp1625817721-246null53 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull53 > User Principal: connect
53 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null53 > Cache-Control: no-cachenull53 > Connection: keep-alivenull53 > Content-Type: application/jsonnull53 > Host: kafka.confluent.svc.cluster.local:8090null53 > Pragma: no-cachenull53 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:07,339 [qtp1625817721-246] io.confluent.mds.request.logger log - 53 * Server responded with a response on thread qtp1625817721-246null53 < 200null53 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:07,340 [qtp1625817721-246] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:07,340 [qtp1625817721-246] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:07,341 [qtp1625817721-246] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:07,375 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 16 from controller 1 for 25 partitions
[INFO] 2023-11-08 14:52:07,391 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(confluent.connect-offsets-5, confluent.connect-offsets-17, confluent.connect-offsets-14, confluent.connect-offsets-20, confluent.connect-offsets-23, confluent.connect-offsets-2, confluent.connect-offsets-11, confluent.connect-offsets-8)
[INFO] 2023-11-08 14:52:07,392 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 16 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[INFO] 2023-11-08 14:52:07,398 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-14, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,399 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-14 in /mnt/data/data0/logs/confluent.connect-offsets-14 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,400 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-14 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-14
[INFO] 2023-11-08 14:52:07,400 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-14 broker=2] Log loaded for partition confluent.connect-offsets-14 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,400 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader confluent.connect-offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,405 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,406 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-2 in /mnt/data/data0/logs/confluent.connect-offsets-2 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,406 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-2 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-2
[INFO] 2023-11-08 14:52:07,406 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-2 broker=2] Log loaded for partition confluent.connect-offsets-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,406 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader confluent.connect-offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,410 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-17, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,411 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-17 in /mnt/data/data0/logs/confluent.connect-offsets-17 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,411 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-17 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-17
[INFO] 2023-11-08 14:52:07,411 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-17 broker=2] Log loaded for partition confluent.connect-offsets-17 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,412 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader confluent.connect-offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,416 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-23, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,417 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-23 in /mnt/data/data0/logs/confluent.connect-offsets-23 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,417 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-23 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-23
[INFO] 2023-11-08 14:52:07,417 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-23 broker=2] Log loaded for partition confluent.connect-offsets-23 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,417 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader confluent.connect-offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,422 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,423 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-5 in /mnt/data/data0/logs/confluent.connect-offsets-5 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,423 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-5 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-5
[INFO] 2023-11-08 14:52:07,423 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-5 broker=2] Log loaded for partition confluent.connect-offsets-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,423 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader confluent.connect-offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,428 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-20, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,429 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-20 in /mnt/data/data0/logs/confluent.connect-offsets-20 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,429 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-20 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-20
[INFO] 2023-11-08 14:52:07,429 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-20 broker=2] Log loaded for partition confluent.connect-offsets-20 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,429 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader confluent.connect-offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,434 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,435 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-11 in /mnt/data/data0/logs/confluent.connect-offsets-11 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,435 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-11 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-11
[INFO] 2023-11-08 14:52:07,435 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-11 broker=2] Log loaded for partition confluent.connect-offsets-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,435 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader confluent.connect-offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,440 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,441 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-8 in /mnt/data/data0/logs/confluent.connect-offsets-8 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,441 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-8 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-8
[INFO] 2023-11-08 14:52:07,441 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-8 broker=2] Log loaded for partition confluent.connect-offsets-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,441 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader confluent.connect-offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,446 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-12, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,447 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-12 in /mnt/data/data0/logs/confluent.connect-offsets-12 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,447 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-12 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-12
[INFO] 2023-11-08 14:52:07,447 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-12 broker=2] Log loaded for partition confluent.connect-offsets-12 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,447 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,451 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,452 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-3 in /mnt/data/data0/logs/confluent.connect-offsets-3 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,452 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-3 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-3
[INFO] 2023-11-08 14:52:07,452 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-3 broker=2] Log loaded for partition confluent.connect-offsets-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,452 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,457 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-18, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,458 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-18 in /mnt/data/data0/logs/confluent.connect-offsets-18 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,458 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-18 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-18
[INFO] 2023-11-08 14:52:07,458 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-18 broker=2] Log loaded for partition confluent.connect-offsets-18 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,458 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:07,458 [qtp1625817721-233] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 233]
[TRACE] 2023-11-08 14:52:07,461 [qtp1625817721-233] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 233]
[DEBUG] 2023-11-08 14:52:07,463 [qtp1625817721-233] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:07,466 [qtp1625817721-233] io.confluent.mds.request.logger log - 54 * Server has received a request on thread qtp1625817721-233null54 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull54 > User Principal: connect
54 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null54 > Cache-Control: no-cachenull54 > Connection: keep-alivenull54 > Content-Type: application/jsonnull54 > Host: kafka.confluent.svc.cluster.local:8090null54 > Pragma: no-cachenull54 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:07,466 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,467 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-1 in /mnt/data/data0/logs/confluent.connect-offsets-1 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,467 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-1 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-1
[INFO] 2023-11-08 14:52:07,467 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-1 broker=2] Log loaded for partition confluent.connect-offsets-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,467 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,467 [qtp1625817721-233] io.confluent.mds.request.logger log - 54 * Server responded with a response on thread qtp1625817721-233null54 < 200null54 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:07,468 [qtp1625817721-233] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:07,468 [qtp1625817721-233] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:07,468 [qtp1625817721-233] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:07,472 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-16, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,472 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-16 in /mnt/data/data0/logs/confluent.connect-offsets-16 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,472 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-16 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-16
[INFO] 2023-11-08 14:52:07,473 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-16 broker=2] Log loaded for partition confluent.connect-offsets-16 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,473 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,480 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,481 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-6 in /mnt/data/data0/logs/confluent.connect-offsets-6 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,481 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-6 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-6
[INFO] 2023-11-08 14:52:07,481 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-6 broker=2] Log loaded for partition confluent.connect-offsets-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,482 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,491 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-21, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,492 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-21 in /mnt/data/data0/logs/confluent.connect-offsets-21 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,492 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-21 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-21
[INFO] 2023-11-08 14:52:07,492 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-21 broker=2] Log loaded for partition confluent.connect-offsets-21 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,493 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,499 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,500 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-4 in /mnt/data/data0/logs/confluent.connect-offsets-4 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,500 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-4 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-4
[INFO] 2023-11-08 14:52:07,500 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-4 broker=2] Log loaded for partition confluent.connect-offsets-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,500 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,507 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,507 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-10 in /mnt/data/data0/logs/confluent.connect-offsets-10 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,508 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-10 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-10
[INFO] 2023-11-08 14:52:07,508 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-10 broker=2] Log loaded for partition confluent.connect-offsets-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,508 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,514 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-15, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,515 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-15 in /mnt/data/data0/logs/confluent.connect-offsets-15 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,515 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-15 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-15
[INFO] 2023-11-08 14:52:07,515 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-15 broker=2] Log loaded for partition confluent.connect-offsets-15 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,516 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,520 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-13, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,521 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-13 in /mnt/data/data0/logs/confluent.connect-offsets-13 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,521 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-13 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-13
[INFO] 2023-11-08 14:52:07,521 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-13 broker=2] Log loaded for partition confluent.connect-offsets-13 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,521 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,526 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-19, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,527 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-19 in /mnt/data/data0/logs/confluent.connect-offsets-19 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,527 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-19 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-19
[INFO] 2023-11-08 14:52:07,527 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-19 broker=2] Log loaded for partition confluent.connect-offsets-19 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,527 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,531 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,532 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-0 in /mnt/data/data0/logs/confluent.connect-offsets-0 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,532 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-0 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-0
[INFO] 2023-11-08 14:52:07,532 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-0 broker=2] Log loaded for partition confluent.connect-offsets-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,532 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,536 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,537 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-7 in /mnt/data/data0/logs/confluent.connect-offsets-7 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,537 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-7 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-7
[INFO] 2023-11-08 14:52:07,537 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-7 broker=2] Log loaded for partition confluent.connect-offsets-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,537 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,541 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-22, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,542 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-22 in /mnt/data/data0/logs/confluent.connect-offsets-22 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,542 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-22 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-22
[INFO] 2023-11-08 14:52:07,542 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-22 broker=2] Log loaded for partition confluent.connect-offsets-22 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,542 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,547 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,547 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-9 in /mnt/data/data0/logs/confluent.connect-offsets-9 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,547 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-9 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-9
[INFO] 2023-11-08 14:52:07,547 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-9 broker=2] Log loaded for partition confluent.connect-offsets-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,547 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,551 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-24, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,552 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-24 in /mnt/data/data0/logs/confluent.connect-offsets-24 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,552 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-24 broker=2] No checkpointed highwatermark is found for partition confluent.connect-offsets-24
[INFO] 2023-11-08 14:52:07,552 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-offsets-24 broker=2] Log loaded for partition confluent.connect-offsets-24 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,552 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower confluent.connect-offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,553 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(confluent.connect-offsets-19, confluent.connect-offsets-21, confluent.connect-offsets-22, confluent.connect-offsets-15, confluent.connect-offsets-16, confluent.connect-offsets-18, confluent.connect-offsets-24, confluent.connect-offsets-3, confluent.connect-offsets-4, confluent.connect-offsets-6, confluent.connect-offsets-0, confluent.connect-offsets-1, confluent.connect-offsets-12, confluent.connect-offsets-13, confluent.connect-offsets-7, confluent.connect-offsets-9, confluent.connect-offsets-10)
[INFO] 2023-11-08 14:52:07,553 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 16 for 17 partitions
[INFO] 2023-11-08 14:52:07,554 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(confluent.connect-offsets-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-18 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-15 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-12 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-24 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-21 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,555 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(confluent.connect-offsets-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-19 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-16 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-13 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-22 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,555 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 180ms correlationId 16 from controller 1 for 25 partitions
[INFO] 2023-11-08 14:52:07,558 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 17
[INFO] 2023-11-08 14:52:07,560 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 18 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:07,568 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-telemetry-metrics-9, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-3, _confluent-telemetry-metrics-0)
[INFO] 2023-11-08 14:52:07,568 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 18 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:07,573 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,574 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-9 in /mnt/data/data0/logs/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,575 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-9 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9
[INFO] 2023-11-08 14:52:07,575 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-9 broker=2] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,575 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-telemetry-metrics-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,580 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,581 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-6 in /mnt/data/data0/logs/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,581 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-6 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6
[INFO] 2023-11-08 14:52:07,581 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-6 broker=2] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,581 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-telemetry-metrics-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,586 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,587 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-3 in /mnt/data/data0/logs/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,587 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-3 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3
[INFO] 2023-11-08 14:52:07,587 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-3 broker=2] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,587 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-telemetry-metrics-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,591 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,592 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-0 in /mnt/data/data0/logs/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,592 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-0 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0
[INFO] 2023-11-08 14:52:07,592 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-0 broker=2] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,592 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-telemetry-metrics-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,597 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,598 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-11 in /mnt/data/data0/logs/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,598 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-11 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11
[INFO] 2023-11-08 14:52:07,598 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-11 broker=2] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,598 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-telemetry-metrics-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,602 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,603 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-10 in /mnt/data/data0/logs/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,603 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-10 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10
[INFO] 2023-11-08 14:52:07,603 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-10 broker=2] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,603 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-telemetry-metrics-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,607 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,608 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-8 in /mnt/data/data0/logs/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,608 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-8 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8
[INFO] 2023-11-08 14:52:07,608 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-8 broker=2] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,608 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-telemetry-metrics-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,612 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,613 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-7 in /mnt/data/data0/logs/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,613 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-7 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7
[INFO] 2023-11-08 14:52:07,613 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-7 broker=2] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,613 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-telemetry-metrics-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,617 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,617 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-5 in /mnt/data/data0/logs/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,618 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-5 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5
[INFO] 2023-11-08 14:52:07,618 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-5 broker=2] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,618 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-telemetry-metrics-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,622 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,622 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-4 in /mnt/data/data0/logs/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,622 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-4 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4
[INFO] 2023-11-08 14:52:07,622 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-4 broker=2] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,623 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-telemetry-metrics-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,626 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,627 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-2 in /mnt/data/data0/logs/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,627 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-2 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2
[INFO] 2023-11-08 14:52:07,627 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-2 broker=2] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,627 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-telemetry-metrics-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,631 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,631 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-1 in /mnt/data/data0/logs/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,631 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-1 broker=2] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1
[INFO] 2023-11-08 14:52:07,631 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-1 broker=2] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,631 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-telemetry-metrics-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,632 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-telemetry-metrics-4, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-10, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-1, _confluent-telemetry-metrics-2)
[INFO] 2023-11-08 14:52:07,632 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 18 for 8 partitions
[INFO] 2023-11-08 14:52:07,633 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-telemetry-metrics-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,633 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-telemetry-metrics-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,634 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 74ms correlationId 18 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:07,636 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 19
[INFO] 2023-11-08 14:52:07,689 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,689 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-19, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,689 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-telemetry-metrics-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,689 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,690 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,690 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-22, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,690 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,690 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-16, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,690 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-telemetry-metrics-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,690 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,690 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-telemetry-metrics-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,690 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,690 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,690 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,691 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,691 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,691 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,691 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-13, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,691 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,691 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,691 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,691 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,691 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-telemetry-metrics-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,691 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,693 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-telemetry-metrics-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,693 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,693 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,693 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-21, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,693 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,693 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-15, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,693 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-telemetry-metrics-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-18, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-telemetry-metrics-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-24, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,694 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,695 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,695 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,695 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-12, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,695 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,695 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,695 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-telemetry-metrics-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,695 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,834 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 20 from controller 1 for 5 partitions
[INFO] 2023-11-08 14:52:07,836 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(confluent.connect-status-2)
[INFO] 2023-11-08 14:52:07,837 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 20 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions
[INFO] 2023-11-08 14:52:07,841 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-status-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,841 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition confluent.connect-status-2 in /mnt/data/data0/logs/confluent.connect-status-2 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,842 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-status-2 broker=2] No checkpointed highwatermark is found for partition confluent.connect-status-2
[INFO] 2023-11-08 14:52:07,842 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-status-2 broker=2] Log loaded for partition confluent.connect-status-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,842 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader confluent.connect-status-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,847 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-status-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,847 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition confluent.connect-status-3 in /mnt/data/data0/logs/confluent.connect-status-3 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,847 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-status-3 broker=2] No checkpointed highwatermark is found for partition confluent.connect-status-3
[INFO] 2023-11-08 14:52:07,848 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-status-3 broker=2] Log loaded for partition confluent.connect-status-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,848 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower confluent.connect-status-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,852 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-status-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,853 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition confluent.connect-status-1 in /mnt/data/data0/logs/confluent.connect-status-1 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,853 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-status-1 broker=2] No checkpointed highwatermark is found for partition confluent.connect-status-1
[INFO] 2023-11-08 14:52:07,853 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-status-1 broker=2] Log loaded for partition confluent.connect-status-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,853 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower confluent.connect-status-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,857 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-status-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,858 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition confluent.connect-status-0 in /mnt/data/data0/logs/confluent.connect-status-0 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,858 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-status-0 broker=2] No checkpointed highwatermark is found for partition confluent.connect-status-0
[INFO] 2023-11-08 14:52:07,858 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-status-0 broker=2] Log loaded for partition confluent.connect-status-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,858 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower confluent.connect-status-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,862 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-status-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,863 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition confluent.connect-status-4 in /mnt/data/data0/logs/confluent.connect-status-4 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,863 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-status-4 broker=2] No checkpointed highwatermark is found for partition confluent.connect-status-4
[INFO] 2023-11-08 14:52:07,863 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-status-4 broker=2] Log loaded for partition confluent.connect-status-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,863 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower confluent.connect-status-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,864 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(confluent.connect-status-0, confluent.connect-status-1, confluent.connect-status-3, confluent.connect-status-4)
[INFO] 2023-11-08 14:52:07,864 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 20 for 4 partitions
[INFO] 2023-11-08 14:52:07,864 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions Map(confluent.connect-status-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-status-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,865 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions Map(confluent.connect-status-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-status-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,865 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 31ms correlationId 20 from controller 1 for 5 partitions
[INFO] 2023-11-08 14:52:07,867 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 21
[INFO] 2023-11-08 14:52:08,003 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 22 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:52:08,005 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(confluent.connect-configs-0)
[INFO] 2023-11-08 14:52:08,005 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 22 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions
[INFO] 2023-11-08 14:52:08,010 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-configs-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,011 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition confluent.connect-configs-0 in /mnt/data/data0/logs/confluent.connect-configs-0 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:08,011 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-configs-0 broker=2] No checkpointed highwatermark is found for partition confluent.connect-configs-0
[INFO] 2023-11-08 14:52:08,012 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition confluent.connect-configs-0 broker=2] Log loaded for partition confluent.connect-configs-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,012 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader confluent.connect-configs-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,013 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 10ms correlationId 22 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:52:08,015 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 23
[INFO] 2023-11-08 14:52:08,059 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent.connect-status-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,059 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-status-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,060 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition confluent.connect-status-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,060 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-status-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,097 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-status-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,097 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-status-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,098 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition confluent.connect-status-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,098 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-status-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,158 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:52:08,160 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'compression.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:08,160 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'enable.idempotence' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:08,160 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'acks' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:08,160 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'key.serializer' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:08,160 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'max.request.size' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:08,160 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'value.serializer' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:08,161 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'interceptor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:08,161 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:08,161 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'linger.ms' was supplied but isn't a known config.
[INFO] 2023-11-08 14:52:08,161 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:52:08,161 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:52:08,161 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455128161
[INFO] 2023-11-08 14:52:08,192 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 24 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,204 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8)
[INFO] 2023-11-08 14:52:08,204 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 24 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,204 [kafka-admin-client-thread | confluent-telemetry-reporter-local-producer] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered
[INFO] 2023-11-08 14:52:08,206 [kafka-admin-client-thread | confluent-telemetry-reporter-local-producer] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:52:08,206 [kafka-admin-client-thread | confluent-telemetry-reporter-local-producer] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:52:08,206 [kafka-admin-client-thread | confluent-telemetry-reporter-local-producer] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:52:08,212 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,213 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,214 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2
[INFO] 2023-11-08 14:52:08,214 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,214 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,221 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,222 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,222 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11
[INFO] 2023-11-08 14:52:08,222 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,222 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,230 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,235 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,236 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5
[INFO] 2023-11-08 14:52:08,236 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,236 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,241 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,241 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,241 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8
[INFO] 2023-11-08 14:52:08,242 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,242 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,246 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,247 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,247 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1
[INFO] 2023-11-08 14:52:08,247 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,247 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,251 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,252 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,253 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4
[INFO] 2023-11-08 14:52:08,253 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,253 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,257 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,258 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,258 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3
[INFO] 2023-11-08 14:52:08,258 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,259 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,263 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,264 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,264 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0
[INFO] 2023-11-08 14:52:08,264 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,264 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,268 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,269 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,269 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10
[INFO] 2023-11-08 14:52:08,269 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,269 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,274 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,275 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,275 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9
[INFO] 2023-11-08 14:52:08,275 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,276 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,279 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,280 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,280 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6
[INFO] 2023-11-08 14:52:08,280 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,280 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,284 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,285 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,286 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7
[INFO] 2023-11-08 14:52:08,286 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,286 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,287 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10)
[INFO] 2023-11-08 14:52:08,287 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 24 for 8 partitions
[INFO] 2023-11-08 14:52:08,288 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,289 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,290 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 98ms correlationId 24 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,293 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 25
[INFO] 2023-11-08 14:52:08,297 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,297 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,297 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,297 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,297 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,298 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,298 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,303 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,304 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,304 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,304 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,304 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,304 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,304 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,305 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,318 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 26 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,336 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8)
[INFO] 2023-11-08 14:52:08,336 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 26 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,341 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,342 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,343 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11
[INFO] 2023-11-08 14:52:08,343 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,344 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,351 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,353 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,353 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2
[INFO] 2023-11-08 14:52:08,353 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,353 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,360 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,361 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,361 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5
[INFO] 2023-11-08 14:52:08,361 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,362 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,366 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,366 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,366 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8
[INFO] 2023-11-08 14:52:08,367 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,367 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,373 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,374 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,374 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1
[INFO] 2023-11-08 14:52:08,376 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,376 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,382 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,383 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,383 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0
[INFO] 2023-11-08 14:52:08,383 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,384 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,388 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,388 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,389 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6
[INFO] 2023-11-08 14:52:08,389 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,389 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,393 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,394 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,394 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3
[INFO] 2023-11-08 14:52:08,394 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,395 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,399 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,400 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,400 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4
[INFO] 2023-11-08 14:52:08,400 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,400 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,406 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,406 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,406 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9
[INFO] 2023-11-08 14:52:08,407 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,407 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,411 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,411 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,412 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10
[INFO] 2023-11-08 14:52:08,412 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,412 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,417 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,418 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,418 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7
[INFO] 2023-11-08 14:52:08,419 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,419 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,419 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10)
[INFO] 2023-11-08 14:52:08,419 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 26 for 8 partitions
[INFO] 2023-11-08 14:52:08,421 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,421 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,422 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 104ms correlationId 26 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,424 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 27
[INFO] 2023-11-08 14:52:08,427 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 28 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,429 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,429 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,430 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,431 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,431 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,432 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,432 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,432 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,437 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4)
[INFO] 2023-11-08 14:52:08,438 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 28 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,447 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,448 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,449 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10
[INFO] 2023-11-08 14:52:08,449 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,450 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,454 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,455 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,456 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7
[INFO] 2023-11-08 14:52:08,456 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,456 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,461 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,469 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,472 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1
[INFO] 2023-11-08 14:52:08,473 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,474 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,477 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,478 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,479 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,479 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,479 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,480 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,480 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,480 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4
[INFO] 2023-11-08 14:52:08,480 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,480 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,480 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,480 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,480 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,484 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,484 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,484 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9
[INFO] 2023-11-08 14:52:08,484 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,484 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,501 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,502 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,503 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11
[INFO] 2023-11-08 14:52:08,503 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,504 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,508 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,508 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,509 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5
[INFO] 2023-11-08 14:52:08,509 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,509 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,514 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,515 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,515 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6
[INFO] 2023-11-08 14:52:08,515 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,515 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,616 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,618 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,618 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8
[INFO] 2023-11-08 14:52:08,618 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,618 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,623 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,624 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,625 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2
[INFO] 2023-11-08 14:52:08,625 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,625 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,632 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,634 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,635 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3
[INFO] 2023-11-08 14:52:08,635 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,635 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,639 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,640 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,640 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0
[INFO] 2023-11-08 14:52:08,640 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,640 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,641 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2)
[INFO] 2023-11-08 14:52:08,641 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 28 for 8 partitions
[INFO] 2023-11-08 14:52:08,642 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,642 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,643 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 216ms correlationId 28 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,646 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 29
[INFO] 2023-11-08 14:52:08,648 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 30 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:08,657 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,657 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,659 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,659 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,659 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,659 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,659 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,659 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,667 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0)
[INFO] 2023-11-08 14:52:08,668 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 30 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[INFO] 2023-11-08 14:52:08,674 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,675 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,677 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10
[INFO] 2023-11-08 14:52:08,677 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,677 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,683 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,685 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,685 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9
[INFO] 2023-11-08 14:52:08,685 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,685 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,691 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,692 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,692 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7
[INFO] 2023-11-08 14:52:08,692 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,692 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,697 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,698 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,698 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4
[INFO] 2023-11-08 14:52:08,698 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,699 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,703 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,703 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,704 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0
[INFO] 2023-11-08 14:52:08,704 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,704 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,708 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,709 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,709 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1
[INFO] 2023-11-08 14:52:08,709 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,710 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,714 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,715 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,715 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3
[INFO] 2023-11-08 14:52:08,715 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,715 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,719 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,720 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,721 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6
[INFO] 2023-11-08 14:52:08,721 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,721 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,726 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,726 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,727 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11
[INFO] 2023-11-08 14:52:08,727 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,727 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,731 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,732 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,732 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7
[INFO] 2023-11-08 14:52:08,732 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,733 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,736 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,737 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,737 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9
[INFO] 2023-11-08 14:52:08,737 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,738 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,741 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,741 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,741 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,742 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,742 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,742 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,742 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,742 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,742 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,742 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11
[INFO] 2023-11-08 14:52:08,742 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,742 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,743 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,747 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,747 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,747 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5
[INFO] 2023-11-08 14:52:08,748 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,748 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,751 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,752 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,752 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2
[INFO] 2023-11-08 14:52:08,752 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,752 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,756 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,756 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,756 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2
[INFO] 2023-11-08 14:52:08,757 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,757 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,762 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,762 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,762 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0
[INFO] 2023-11-08 14:52:08,763 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,763 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,766 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,767 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,767 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4
[INFO] 2023-11-08 14:52:08,767 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,767 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,771 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,772 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,772 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8
[INFO] 2023-11-08 14:52:08,772 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,772 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,776 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,777 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,777 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10
[INFO] 2023-11-08 14:52:08,777 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,777 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,780 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,780 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,781 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8
[INFO] 2023-11-08 14:52:08,781 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,781 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,784 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,785 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,785 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6
[INFO] 2023-11-08 14:52:08,785 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,785 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,789 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,790 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,790 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3
[INFO] 2023-11-08 14:52:08,790 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,790 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,794 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,795 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,795 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1
[INFO] 2023-11-08 14:52:08,795 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,795 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,799 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,800 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,800 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5
[INFO] 2023-11-08 14:52:08,800 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,800 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,801 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5)
[INFO] 2023-11-08 14:52:08,801 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 30 for 16 partitions
[INFO] 2023-11-08 14:52:08,803 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,804 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,805 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 157ms correlationId 30 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:08,807 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 31
[INFO] 2023-11-08 14:52:08,809 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 32 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,817 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2)
[INFO] 2023-11-08 14:52:08,817 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 32 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,822 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,823 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,823 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5
[INFO] 2023-11-08 14:52:08,824 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,824 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,828 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,829 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,829 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8
[INFO] 2023-11-08 14:52:08,829 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,829 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,833 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,834 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,834 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11
[INFO] 2023-11-08 14:52:08,834 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,834 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,839 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,839 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,839 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2
[INFO] 2023-11-08 14:52:08,839 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,839 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,843 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,844 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,844 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6
[INFO] 2023-11-08 14:52:08,844 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,844 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,849 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,850 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,850 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4
[INFO] 2023-11-08 14:52:08,850 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,850 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,855 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,856 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,857 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3
[INFO] 2023-11-08 14:52:08,857 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,857 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,861 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,862 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,862 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10
[INFO] 2023-11-08 14:52:08,862 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,862 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,866 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,866 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,866 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9
[INFO] 2023-11-08 14:52:08,866 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,866 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,870 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,871 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,871 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7
[INFO] 2023-11-08 14:52:08,871 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,871 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,874 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,875 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,875 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,875 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,875 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,875 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,875 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,875 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,875 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,876 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,876 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,876 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,876 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,876 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,876 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,876 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,878 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,879 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,879 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1
[INFO] 2023-11-08 14:52:08,879 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,879 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,883 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,884 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,884 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0
[INFO] 2023-11-08 14:52:08,884 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,884 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,885 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3)
[INFO] 2023-11-08 14:52:08,885 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 32 for 8 partitions
[INFO] 2023-11-08 14:52:08,886 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,886 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,887 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 78ms correlationId 32 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,889 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 33
[INFO] 2023-11-08 14:52:08,921 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 34 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,930 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9)
[INFO] 2023-11-08 14:52:08,931 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 34 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,940 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,941 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,944 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3
[INFO] 2023-11-08 14:52:08,944 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,944 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,954 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,955 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,955 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6
[INFO] 2023-11-08 14:52:08,955 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,957 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,962 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,962 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,963 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0
[INFO] 2023-11-08 14:52:08,963 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,963 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,967 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,968 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,968 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9
[INFO] 2023-11-08 14:52:08,969 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,969 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,975 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,976 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,976 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4
[INFO] 2023-11-08 14:52:08,976 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,976 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,982 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,984 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,984 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5
[INFO] 2023-11-08 14:52:08,984 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,984 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,991 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,992 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,993 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2
[INFO] 2023-11-08 14:52:08,993 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,993 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,997 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,998 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,998 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1
[INFO] 2023-11-08 14:52:08,998 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,998 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,015 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,016 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,017 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11
[INFO] 2023-11-08 14:52:09,017 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,017 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,022 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,022 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,022 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8
[INFO] 2023-11-08 14:52:09,023 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,023 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,026 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,027 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,027 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7
[INFO] 2023-11-08 14:52:09,028 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,028 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,031 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,041 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,041 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10
[INFO] 2023-11-08 14:52:09,041 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,042 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,042 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8)
[INFO] 2023-11-08 14:52:09,042 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 34 for 8 partitions
[INFO] 2023-11-08 14:52:09,043 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,043 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,044 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 123ms correlationId 34 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,047 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 35
[INFO] 2023-11-08 14:52:09,057 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 36 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,066 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0)
[INFO] 2023-11-08 14:52:09,066 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 36 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,071 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,072 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,072 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9
[INFO] 2023-11-08 14:52:09,073 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,073 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,077 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,078 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,078 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6
[INFO] 2023-11-08 14:52:09,078 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,078 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,082 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,083 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,083 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3
[INFO] 2023-11-08 14:52:09,083 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,083 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,087 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,087 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,088 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0
[INFO] 2023-11-08 14:52:09,088 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,088 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,092 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,092 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,092 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11
[INFO] 2023-11-08 14:52:09,092 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,093 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,096 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,096 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,096 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10
[INFO] 2023-11-08 14:52:09,097 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,097 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,100 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,101 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,101 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8
[INFO] 2023-11-08 14:52:09,101 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,101 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,105 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,105 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,105 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7
[INFO] 2023-11-08 14:52:09,106 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,106 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,109 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,110 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,110 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5
[INFO] 2023-11-08 14:52:09,110 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,110 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,126 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,127 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,127 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4
[INFO] 2023-11-08 14:52:09,127 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,128 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,131 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,132 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,132 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2
[INFO] 2023-11-08 14:52:09,132 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,133 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,139 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,139 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,139 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1
[INFO] 2023-11-08 14:52:09,140 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,140 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,140 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5)
[INFO] 2023-11-08 14:52:09,140 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 36 for 8 partitions
[INFO] 2023-11-08 14:52:09,142 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,143 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,143 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 86ms correlationId 36 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,148 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 37
[INFO] 2023-11-08 14:52:09,152 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 38 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,156 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,156 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,157 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,157 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,157 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,157 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,157 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,157 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,157 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,157 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,157 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,158 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,158 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,158 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,158 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,158 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,158 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,158 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,159 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,159 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,160 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,160 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,160 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,160 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,160 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,161 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,161 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,161 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,161 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,161 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,161 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,161 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,161 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,161 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,162 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,162 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,162 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,162 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,162 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,162 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,163 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4)
[INFO] 2023-11-08 14:52:09,163 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 38 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,167 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,168 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,168 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7
[INFO] 2023-11-08 14:52:09,168 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,169 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,172 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,173 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,173 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10
[INFO] 2023-11-08 14:52:09,173 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,173 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,180 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,180 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,181 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1
[INFO] 2023-11-08 14:52:09,181 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,181 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,185 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,186 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,186 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4
[INFO] 2023-11-08 14:52:09,186 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,186 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,190 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,190 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,191 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8
[INFO] 2023-11-08 14:52:09,191 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,191 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,195 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,195 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,195 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9
[INFO] 2023-11-08 14:52:09,195 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,195 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,199 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,199 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,200 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11
[INFO] 2023-11-08 14:52:09,200 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,200 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,204 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,204 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,205 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0
[INFO] 2023-11-08 14:52:09,205 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,205 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,209 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,209 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,210 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2
[INFO] 2023-11-08 14:52:09,210 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,210 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,214 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,215 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,215 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3
[INFO] 2023-11-08 14:52:09,215 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,215 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,219 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,219 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,220 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6
[INFO] 2023-11-08 14:52:09,220 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,220 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,224 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,224 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,224 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5
[INFO] 2023-11-08 14:52:09,225 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,225 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,225 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0)
[INFO] 2023-11-08 14:52:09,225 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 38 for 8 partitions
[INFO] 2023-11-08 14:52:09,226 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,226 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,227 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 75ms correlationId 38 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,229 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 39
[INFO] 2023-11-08 14:52:09,231 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 40 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,238 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-cluster-rekey-3, _confluent-controlcenter-7-1-0-0-cluster-rekey-0, _confluent-controlcenter-7-1-0-0-cluster-rekey-9, _confluent-controlcenter-7-1-0-0-cluster-rekey-6)
[INFO] 2023-11-08 14:52:09,239 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 40 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,244 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,245 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,247 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3
[INFO] 2023-11-08 14:52:09,247 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,247 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-cluster-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,251 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,251 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,251 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0
[INFO] 2023-11-08 14:52:09,251 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,252 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-cluster-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,264 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,265 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,265 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9
[INFO] 2023-11-08 14:52:09,265 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,265 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-cluster-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,271 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,271 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,271 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6
[INFO] 2023-11-08 14:52:09,272 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,272 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-cluster-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,275 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,276 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,276 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1
[INFO] 2023-11-08 14:52:09,276 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,276 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,280 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,280 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,280 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2
[INFO] 2023-11-08 14:52:09,281 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,281 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,284 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,285 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,285 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4
[INFO] 2023-11-08 14:52:09,285 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,285 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,289 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,290 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,290 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10
[INFO] 2023-11-08 14:52:09,290 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,291 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,294 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,295 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,295 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11
[INFO] 2023-11-08 14:52:09,295 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,296 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,304 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,304 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,305 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5
[INFO] 2023-11-08 14:52:09,305 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,305 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,309 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,309 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,310 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7
[INFO] 2023-11-08 14:52:09,310 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,310 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,313 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,313 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,313 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8
[INFO] 2023-11-08 14:52:09,314 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,314 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,314 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-cluster-rekey-1, _confluent-controlcenter-7-1-0-0-cluster-rekey-5, _confluent-controlcenter-7-1-0-0-cluster-rekey-4, _confluent-controlcenter-7-1-0-0-cluster-rekey-2, _confluent-controlcenter-7-1-0-0-cluster-rekey-8, _confluent-controlcenter-7-1-0-0-cluster-rekey-7, _confluent-controlcenter-7-1-0-0-cluster-rekey-11, _confluent-controlcenter-7-1-0-0-cluster-rekey-10)
[INFO] 2023-11-08 14:52:09,314 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 40 for 8 partitions
[INFO] 2023-11-08 14:52:09,315 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-cluster-rekey-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,315 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-cluster-rekey-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,316 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 85ms correlationId 40 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,317 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 41
[INFO] 2023-11-08 14:52:09,320 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 42 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,327 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5)
[INFO] 2023-11-08 14:52:09,328 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 42 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,332 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,332 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,334 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11
[INFO] 2023-11-08 14:52:09,334 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,335 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,338 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,339 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,339 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8
[INFO] 2023-11-08 14:52:09,339 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,339 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,343 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,343 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,343 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2
[INFO] 2023-11-08 14:52:09,343 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,344 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,347 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,348 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,348 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5
[INFO] 2023-11-08 14:52:09,348 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,348 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,351 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,352 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,352 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1
[INFO] 2023-11-08 14:52:09,352 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,352 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,355 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,356 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,356 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0
[INFO] 2023-11-08 14:52:09,356 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,356 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,362 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,362 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,363 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10
[INFO] 2023-11-08 14:52:09,363 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,363 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,367 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,367 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,367 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7
[INFO] 2023-11-08 14:52:09,367 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,367 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:52:09,370 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:52:09,371 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,371 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,371 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6
[INFO] 2023-11-08 14:52:09,372 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,372 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:52:09,372 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:09,375 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,376 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,376 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9
[INFO] 2023-11-08 14:52:09,376 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,376 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,379 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,380 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,380 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3
[INFO] 2023-11-08 14:52:09,380 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,380 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,381 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,381 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,382 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,383 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,384 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,384 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,386 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,386 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4
[INFO] 2023-11-08 14:52:09,386 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,386 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,386 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0)
[INFO] 2023-11-08 14:52:09,387 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 42 for 8 partitions
[INFO] 2023-11-08 14:52:09,387 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,388 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,388 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 68ms correlationId 42 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,391 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 43
[INFO] 2023-11-08 14:52:09,409 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 44 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,421 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9)
[INFO] 2023-11-08 14:52:09,424 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 44 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,428 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,429 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,430 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0
[INFO] 2023-11-08 14:52:09,430 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,430 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,435 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,436 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,436 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3
[INFO] 2023-11-08 14:52:09,436 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,436 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,444 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,445 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,445 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6
[INFO] 2023-11-08 14:52:09,445 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,446 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,449 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,450 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,450 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9
[INFO] 2023-11-08 14:52:09,450 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,450 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,453 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,454 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,454 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4
[INFO] 2023-11-08 14:52:09,454 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,454 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,457 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,458 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,458 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1
[INFO] 2023-11-08 14:52:09,458 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,458 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,462 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,462 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,463 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2
[INFO] 2023-11-08 14:52:09,463 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,463 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,466 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,467 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,467 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7
[INFO] 2023-11-08 14:52:09,467 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,467 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,470 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,471 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,471 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8
[INFO] 2023-11-08 14:52:09,471 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,471 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,475 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,475 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,475 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5
[INFO] 2023-11-08 14:52:09,476 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,476 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,479 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,480 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,480 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11
[INFO] 2023-11-08 14:52:09,480 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,480 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,483 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[WARN] 2023-11-08 14:52:09,483 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:09,484 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:09,484 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:09,484 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[WARN] 2023-11-08 14:52:09,484 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:09,484 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10
[INFO] 2023-11-08 14:52:09,484 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 with initial high watermark 0
[WARN] 2023-11-08 14:52:09,484 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:09,484 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:09,484 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:09,484 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:09,485 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,487 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,487 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,487 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,487 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,487 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,487 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,487 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,488 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,490 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8)
[INFO] 2023-11-08 14:52:09,490 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 44 for 8 partitions
[INFO] 2023-11-08 14:52:09,492 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,492 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,493 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 85ms correlationId 44 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,495 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 45
[INFO] 2023-11-08 14:52:09,497 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 46 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,504 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0)
[INFO] 2023-11-08 14:52:09,504 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 46 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,508 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,509 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,510 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9
[INFO] 2023-11-08 14:52:09,510 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,511 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,514 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,515 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,515 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6
[INFO] 2023-11-08 14:52:09,515 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,515 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,519 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,519 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,520 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3
[INFO] 2023-11-08 14:52:09,520 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,520 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,524 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,525 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,525 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0
[INFO] 2023-11-08 14:52:09,525 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,525 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,528 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,529 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,529 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8
[INFO] 2023-11-08 14:52:09,529 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,529 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,532 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,533 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,533 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7
[INFO] 2023-11-08 14:52:09,533 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,533 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,536 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,537 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,537 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5
[INFO] 2023-11-08 14:52:09,537 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,537 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,540 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,540 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,540 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4
[INFO] 2023-11-08 14:52:09,540 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,541 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,544 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,545 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,545 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2
[INFO] 2023-11-08 14:52:09,545 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,545 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,556 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,557 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,557 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1
[INFO] 2023-11-08 14:52:09,557 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,558 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,562 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,562 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,562 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11
[INFO] 2023-11-08 14:52:09,563 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,563 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,566 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,567 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,567 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10
[INFO] 2023-11-08 14:52:09,567 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,567 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,567 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8)
[INFO] 2023-11-08 14:52:09,568 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 46 for 8 partitions
[INFO] 2023-11-08 14:52:09,568 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,569 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,570 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 73ms correlationId 46 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,576 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 47
[INFO] 2023-11-08 14:52:09,596 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 48 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,604 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4)
[INFO] 2023-11-08 14:52:09,605 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 48 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,610 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,610 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,613 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1
[INFO] 2023-11-08 14:52:09,614 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,614 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,620 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,620 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,620 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10
[INFO] 2023-11-08 14:52:09,621 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,621 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,624 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,624 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,624 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7
[INFO] 2023-11-08 14:52:09,624 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,624 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,627 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,628 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,628 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4
[INFO] 2023-11-08 14:52:09,628 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,628 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,631 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,631 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,631 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0
[INFO] 2023-11-08 14:52:09,631 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,631 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,634 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,635 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,635 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11
[INFO] 2023-11-08 14:52:09,635 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,635 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,639 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,639 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,640 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9
[INFO] 2023-11-08 14:52:09,640 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,640 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,646 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,651 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,651 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8
[INFO] 2023-11-08 14:52:09,651 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,652 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,662 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,663 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,666 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6
[INFO] 2023-11-08 14:52:09,666 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,667 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,674 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,674 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,678 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5
[INFO] 2023-11-08 14:52:09,679 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,679 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,684 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,685 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,685 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3
[INFO] 2023-11-08 14:52:09,685 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,685 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,690 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,691 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,691 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2
[INFO] 2023-11-08 14:52:09,691 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,691 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,691 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0)
[INFO] 2023-11-08 14:52:09,691 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 48 for 8 partitions
[INFO] 2023-11-08 14:52:09,692 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,692 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,693 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 97ms correlationId 48 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,700 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 49
[INFO] 2023-11-08 14:52:09,706 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 50 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,714 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5)
[INFO] 2023-11-08 14:52:09,718 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 50 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,726 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,727 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,728 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11
[INFO] 2023-11-08 14:52:09,730 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,730 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,739 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,740 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,741 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8
[INFO] 2023-11-08 14:52:09,742 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,742 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,747 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,747 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,751 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2
[INFO] 2023-11-08 14:52:09,752 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,752 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,759 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,760 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,760 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5
[INFO] 2023-11-08 14:52:09,760 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,760 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,766 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,767 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,767 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0
[INFO] 2023-11-08 14:52:09,767 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,767 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,778 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,779 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,779 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1
[INFO] 2023-11-08 14:52:09,780 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,780 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,787 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,788 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,788 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10
[INFO] 2023-11-08 14:52:09,788 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,788 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,792 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,793 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,793 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6
[INFO] 2023-11-08 14:52:09,794 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,794 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,797 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,798 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,798 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7
[INFO] 2023-11-08 14:52:09,798 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,798 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,801 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,802 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,802 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9
[INFO] 2023-11-08 14:52:09,802 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,802 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,805 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,806 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,806 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3
[INFO] 2023-11-08 14:52:09,806 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,806 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,810 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,810 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,811 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4
[INFO] 2023-11-08 14:52:09,811 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,811 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,811 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0)
[INFO] 2023-11-08 14:52:09,811 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 50 for 8 partitions
[INFO] 2023-11-08 14:52:09,812 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,812 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,813 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 107ms correlationId 50 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,815 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 51
[INFO] 2023-11-08 14:52:09,817 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 52 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,822 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9)
[INFO] 2023-11-08 14:52:09,822 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 52 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,825 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,826 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,827 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0
[INFO] 2023-11-08 14:52:09,827 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,827 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,835 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,836 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,836 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3
[INFO] 2023-11-08 14:52:09,837 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,837 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,845 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,846 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,850 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6
[INFO] 2023-11-08 14:52:09,851 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,851 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,861 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,862 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,862 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9
[INFO] 2023-11-08 14:52:09,866 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,867 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,875 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,880 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,880 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11
[INFO] 2023-11-08 14:52:09,881 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,881 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,889 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,890 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,890 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,892 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,893 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,893 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,893 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,893 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,893 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,893 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,893 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,893 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,893 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,893 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,890 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,894 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,902 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,903 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,904 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,904 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,905 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1
[INFO] 2023-11-08 14:52:09,905 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,905 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,910 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,910 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,910 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2
[INFO] 2023-11-08 14:52:09,911 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,911 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,916 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,917 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,917 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5
[INFO] 2023-11-08 14:52:09,917 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,917 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,927 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,928 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,928 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4
[INFO] 2023-11-08 14:52:09,929 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,929 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,934 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,935 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,935 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7
[INFO] 2023-11-08 14:52:09,935 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,936 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,942 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,943 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,944 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8
[INFO] 2023-11-08 14:52:09,944 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,944 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,945 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10)
[INFO] 2023-11-08 14:52:09,945 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 52 for 8 partitions
[INFO] 2023-11-08 14:52:09,946 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,946 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,947 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 130ms correlationId 52 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,952 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 53
[INFO] 2023-11-08 14:52:09,962 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 54 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,974 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7)
[INFO] 2023-11-08 14:52:09,974 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 54 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,978 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,979 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,981 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4
[INFO] 2023-11-08 14:52:09,981 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,981 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,991 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,991 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,992 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1
[INFO] 2023-11-08 14:52:09,992 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,992 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,997 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,998 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,998 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,998 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,998 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,998 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,998 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,999 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,000 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10
[INFO] 2023-11-08 14:52:10,000 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,000 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,001 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,001 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,001 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,001 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,001 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,001 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,005 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,005 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,006 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7
[INFO] 2023-11-08 14:52:10,006 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,006 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,010 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,011 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,011 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2
[INFO] 2023-11-08 14:52:10,011 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,011 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,015 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,016 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,016 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3
[INFO] 2023-11-08 14:52:10,016 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,017 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,021 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,021 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,022 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5
[INFO] 2023-11-08 14:52:10,022 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,022 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,025 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,026 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,026 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0
[INFO] 2023-11-08 14:52:10,026 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,026 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,029 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,029 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,029 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11
[INFO] 2023-11-08 14:52:10,029 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,029 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,033 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,033 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,033 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6
[INFO] 2023-11-08 14:52:10,033 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,033 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,044 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,045 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,045 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8
[INFO] 2023-11-08 14:52:10,045 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,045 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,070 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,071 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,071 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9
[INFO] 2023-11-08 14:52:10,071 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,071 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,071 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9)
[INFO] 2023-11-08 14:52:10,072 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 54 for 8 partitions
[INFO] 2023-11-08 14:52:10,076 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,083 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,084 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 122ms correlationId 54 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:10,135 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 55
[INFO] 2023-11-08 14:52:10,139 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 56 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:10,272 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1)
[INFO] 2023-11-08 14:52:10,285 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,286 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,286 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,286 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,286 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,286 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,286 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,286 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,290 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 56 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[INFO] 2023-11-08 14:52:10,300 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,301 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,305 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10
[INFO] 2023-11-08 14:52:10,305 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,305 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,311 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,312 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,312 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5
[INFO] 2023-11-08 14:52:10,312 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,312 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,316 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,317 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,317 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2
[INFO] 2023-11-08 14:52:10,317 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,317 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,321 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,321 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,322 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11
[INFO] 2023-11-08 14:52:10,322 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,322 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,326 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,327 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,327 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1
[INFO] 2023-11-08 14:52:10,327 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,327 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,331 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,331 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,331 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8
[INFO] 2023-11-08 14:52:10,332 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,332 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,336 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,337 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,337 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7
[INFO] 2023-11-08 14:52:10,337 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,337 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,341 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,342 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,342 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4
[INFO] 2023-11-08 14:52:10,342 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,342 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,346 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,346 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,347 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3
[INFO] 2023-11-08 14:52:10,347 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,347 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,350 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,351 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,351 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8
[INFO] 2023-11-08 14:52:10,351 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,351 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,355 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,355 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,356 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1
[INFO] 2023-11-08 14:52:10,356 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,356 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,361 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,362 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,362 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3
[INFO] 2023-11-08 14:52:10,362 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,362 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,367 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,368 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,368 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10
[INFO] 2023-11-08 14:52:10,368 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,368 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,371 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,372 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,372 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5
[INFO] 2023-11-08 14:52:10,372 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,372 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,375 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,376 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,376 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4
[INFO] 2023-11-08 14:52:10,376 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,376 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,379 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,380 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,380 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11
[INFO] 2023-11-08 14:52:10,380 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,380 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,383 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,384 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,384 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6
[INFO] 2023-11-08 14:52:10,384 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,384 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,387 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,388 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,388 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9
[INFO] 2023-11-08 14:52:10,388 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,388 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,393 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,393 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,393 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0
[INFO] 2023-11-08 14:52:10,393 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,394 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,408 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,408 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,408 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2
[INFO] 2023-11-08 14:52:10,409 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,409 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,412 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,412 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,412 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,412 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,412 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,413 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,414 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,414 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,414 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0
[INFO] 2023-11-08 14:52:10,415 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,415 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,418 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,419 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,419 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7
[INFO] 2023-11-08 14:52:10,419 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,419 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,427 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,427 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,428 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6
[INFO] 2023-11-08 14:52:10,428 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,428 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,434 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,435 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,435 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9
[INFO] 2023-11-08 14:52:10,435 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,436 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,436 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9)
[INFO] 2023-11-08 14:52:10,436 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 56 for 16 partitions
[INFO] 2023-11-08 14:52:10,438 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,438 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,439 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 300ms correlationId 56 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:10,442 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 57
[INFO] 2023-11-08 14:52:10,449 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 58 from controller 1 for 36 partitions
[INFO] 2023-11-08 14:52:10,466 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1)
[INFO] 2023-11-08 14:52:10,467 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 58 from controller 1 epoch 1 as part of the become-leader transition for 12 partitions
[INFO] 2023-11-08 14:52:10,470 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,471 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,471 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2
[INFO] 2023-11-08 14:52:10,472 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,472 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,475 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,475 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,475 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11
[INFO] 2023-11-08 14:52:10,475 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,475 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,479 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,479 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,479 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4
[INFO] 2023-11-08 14:52:10,479 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,479 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,482 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,483 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,483 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8
[INFO] 2023-11-08 14:52:10,483 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,483 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,485 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,486 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,486 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11
[INFO] 2023-11-08 14:52:10,486 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,486 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,489 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,490 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,490 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10
[INFO] 2023-11-08 14:52:10,490 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,490 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,493 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,493 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,493 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5
[INFO] 2023-11-08 14:52:10,494 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,494 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,497 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,498 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,498 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8
[INFO] 2023-11-08 14:52:10,498 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,498 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,501 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,502 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,502 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5
[INFO] 2023-11-08 14:52:10,502 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,502 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,506 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,506 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,506 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1
[INFO] 2023-11-08 14:52:10,506 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,506 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,510 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,510 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,510 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2
[INFO] 2023-11-08 14:52:10,510 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,511 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,514 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,514 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,514 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7
[INFO] 2023-11-08 14:52:10,514 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,515 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,517 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,518 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,518 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7
[INFO] 2023-11-08 14:52:10,518 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,518 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,521 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,522 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,522 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2
[INFO] 2023-11-08 14:52:10,522 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,522 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,525 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,525 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,525 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6
[INFO] 2023-11-08 14:52:10,525 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,526 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,530 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,530 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,530 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9
[INFO] 2023-11-08 14:52:10,530 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,531 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,534 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,535 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,535 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6
[INFO] 2023-11-08 14:52:10,535 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,535 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,545 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,546 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,546 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3
[INFO] 2023-11-08 14:52:10,546 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,546 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,555 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,556 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,556 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1
[INFO] 2023-11-08 14:52:10,556 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,557 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,560 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,560 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,561 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0
[INFO] 2023-11-08 14:52:10,561 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,561 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,565 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,565 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,565 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5
[INFO] 2023-11-08 14:52:10,566 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,566 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,569 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,570 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,570 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4
[INFO] 2023-11-08 14:52:10,570 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,570 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,578 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,579 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,579 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9
[INFO] 2023-11-08 14:52:10,579 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,579 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,583 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,584 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,584 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9
[INFO] 2023-11-08 14:52:10,584 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,584 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,587 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,588 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,588 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0
[INFO] 2023-11-08 14:52:10,588 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,588 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,591 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,592 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,592 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4
[INFO] 2023-11-08 14:52:10,592 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,592 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,597 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,597 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,597 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1
[INFO] 2023-11-08 14:52:10,597 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,598 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,602 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,603 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,603 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8
[INFO] 2023-11-08 14:52:10,603 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,603 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,607 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,607 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,608 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0
[INFO] 2023-11-08 14:52:10,608 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,608 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,611 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,611 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,611 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3
[INFO] 2023-11-08 14:52:10,611 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,611 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,614 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,615 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,615 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10
[INFO] 2023-11-08 14:52:10,615 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,615 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,620 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,620 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,621 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7
[INFO] 2023-11-08 14:52:10,621 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,621 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,624 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,624 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,625 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3
[INFO] 2023-11-08 14:52:10,625 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,625 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,628 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,628 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,629 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10
[INFO] 2023-11-08 14:52:10,629 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,629 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,634 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,635 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,635 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6
[INFO] 2023-11-08 14:52:10,635 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,635 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,638 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,640 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,640 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11
[INFO] 2023-11-08 14:52:10,640 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,640 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,640 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9)
[INFO] 2023-11-08 14:52:10,641 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 58 for 24 partitions
[INFO] 2023-11-08 14:52:10,642 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,642 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,644 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 194ms correlationId 58 from controller 1 for 36 partitions
[INFO] 2023-11-08 14:52:10,646 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Add 36 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 59
[INFO] 2023-11-08 14:52:10,648 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 60 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:10,661 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11)
[INFO] 2023-11-08 14:52:10,661 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 60 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[INFO] 2023-11-08 14:52:10,665 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,666 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,667 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8
[INFO] 2023-11-08 14:52:10,667 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,667 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,670 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,671 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,671 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9
[INFO] 2023-11-08 14:52:10,671 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,671 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,674 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,675 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,675 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5
[INFO] 2023-11-08 14:52:10,675 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,675 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,678 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,678 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,679 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0
[INFO] 2023-11-08 14:52:10,679 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,679 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,682 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,682 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,682 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2
[INFO] 2023-11-08 14:52:10,682 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,682 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,686 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,686 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,686 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3
[INFO] 2023-11-08 14:52:10,686 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,687 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,692 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,693 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,693 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11
[INFO] 2023-11-08 14:52:10,693 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,694 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,698 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,699 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,699 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6
[INFO] 2023-11-08 14:52:10,699 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,699 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,704 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,705 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,705 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9
[INFO] 2023-11-08 14:52:10,705 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,705 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,708 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,709 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,709 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11
[INFO] 2023-11-08 14:52:10,709 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,709 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,712 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,713 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,713 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7
[INFO] 2023-11-08 14:52:10,713 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,713 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,717 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,717 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,717 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3
[INFO] 2023-11-08 14:52:10,717 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,718 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,721 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,721 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,722 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0
[INFO] 2023-11-08 14:52:10,722 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,722 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,726 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,726 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,726 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4
[INFO] 2023-11-08 14:52:10,726 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,726 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,729 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,730 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,730 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2
[INFO] 2023-11-08 14:52:10,730 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,730 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,733 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,733 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,733 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8
[INFO] 2023-11-08 14:52:10,733 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,734 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,740 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,741 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,741 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6
[INFO] 2023-11-08 14:52:10,741 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,741 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,747 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,747 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,748 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10
[INFO] 2023-11-08 14:52:10,748 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,748 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,753 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,754 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,754 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4
[INFO] 2023-11-08 14:52:10,754 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,754 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,758 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,758 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,758 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1
[INFO] 2023-11-08 14:52:10,758 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,759 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,763 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,764 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,764 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1
[INFO] 2023-11-08 14:52:10,764 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,764 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,768 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,769 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,769 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7
[INFO] 2023-11-08 14:52:10,769 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,769 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,772 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,772 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,773 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5
[INFO] 2023-11-08 14:52:10,773 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,773 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,776 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,777 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,777 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10
[INFO] 2023-11-08 14:52:10,777 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,777 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,777 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10)
[INFO] 2023-11-08 14:52:10,777 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 60 for 16 partitions
[INFO] 2023-11-08 14:52:10,778 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,779 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,780 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 132ms correlationId 60 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:10,782 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 61
[INFO] 2023-11-08 14:52:10,787 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 62 from controller 1 for 24 partitions
[WARN] 2023-11-08 14:52:10,797 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:10,798 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:10,798 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:10,798 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:10,798 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,798 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,799 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,800 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,801 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,802 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11)
[INFO] 2023-11-08 14:52:10,802 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 62 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[INFO] 2023-11-08 14:52:10,807 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,807 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,808 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11
[INFO] 2023-11-08 14:52:10,808 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,808 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,812 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,812 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,812 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8
[INFO] 2023-11-08 14:52:10,812 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,812 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,815 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,815 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,816 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0
[INFO] 2023-11-08 14:52:10,816 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,816 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,818 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,819 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,819 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9
[INFO] 2023-11-08 14:52:10,819 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,819 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,822 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,823 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,823 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5
[INFO] 2023-11-08 14:52:10,823 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,823 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,827 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,827 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,827 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2
[INFO] 2023-11-08 14:52:10,828 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,828 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,831 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,839 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,839 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6
[INFO] 2023-11-08 14:52:10,839 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,839 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,843 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,844 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,844 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3
[INFO] 2023-11-08 14:52:10,844 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,844 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,848 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,849 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,849 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10
[INFO] 2023-11-08 14:52:10,849 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,849 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,853 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,854 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,854 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6
[INFO] 2023-11-08 14:52:10,855 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,855 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,858 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,858 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,859 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10
[INFO] 2023-11-08 14:52:10,859 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,859 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,869 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,870 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,870 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4
[INFO] 2023-11-08 14:52:10,870 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,870 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,874 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,874 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,874 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0
[INFO] 2023-11-08 14:52:10,875 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,875 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,879 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,880 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,880 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8
[INFO] 2023-11-08 14:52:10,880 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,880 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,883 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,884 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,884 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2
[INFO] 2023-11-08 14:52:10,884 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,884 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,887 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,888 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,888 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4
[INFO] 2023-11-08 14:52:10,888 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,888 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,891 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,891 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,891 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9
[INFO] 2023-11-08 14:52:10,891 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,891 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,894 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,895 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,895 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7
[INFO] 2023-11-08 14:52:10,895 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,895 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,905 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,906 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,906 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11
[INFO] 2023-11-08 14:52:10,906 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,906 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,915 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,916 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,916 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5
[INFO] 2023-11-08 14:52:10,916 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,916 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,917 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,917 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,917 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,917 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,917 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,917 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,918 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,919 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,919 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,920 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,920 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,920 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,920 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,920 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,923 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,924 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,924 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7
[INFO] 2023-11-08 14:52:10,924 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,924 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,927 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,928 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,928 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1
[INFO] 2023-11-08 14:52:10,928 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,928 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,931 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,931 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,931 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1
[INFO] 2023-11-08 14:52:10,931 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,932 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,932 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10)
[INFO] 2023-11-08 14:52:10,932 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 62 for 16 partitions
[INFO] 2023-11-08 14:52:10,933 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,933 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,934 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 147ms correlationId 62 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:10,936 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 63
[INFO] 2023-11-08 14:52:10,938 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 64 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:10,944 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2)
[INFO] 2023-11-08 14:52:10,945 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 64 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:10,949 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,949 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,951 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5
[INFO] 2023-11-08 14:52:10,951 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,951 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,954 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,955 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,955 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8
[INFO] 2023-11-08 14:52:10,955 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,955 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,958 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,959 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,959 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11
[INFO] 2023-11-08 14:52:10,959 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,960 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,963 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,963 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,963 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2
[INFO] 2023-11-08 14:52:10,964 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,964 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,967 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,968 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,968 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4
[INFO] 2023-11-08 14:52:10,968 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,968 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,971 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,972 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,972 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6
[INFO] 2023-11-08 14:52:10,972 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,972 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,976 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,977 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,978 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7
[INFO] 2023-11-08 14:52:10,978 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,978 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,981 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,982 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,982 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9
[INFO] 2023-11-08 14:52:10,982 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,982 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,985 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,986 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,986 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10
[INFO] 2023-11-08 14:52:10,986 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,987 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,990 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,991 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,991 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0
[INFO] 2023-11-08 14:52:10,991 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,991 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,997 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,997 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,998 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1
[INFO] 2023-11-08 14:52:10,998 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,998 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,003 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,005 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,005 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3
[INFO] 2023-11-08 14:52:11,005 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,005 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,006 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0)
[INFO] 2023-11-08 14:52:11,006 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 64 for 8 partitions
[INFO] 2023-11-08 14:52:11,007 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,007 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,008 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 70ms correlationId 64 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,010 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 65
[INFO] 2023-11-08 14:52:11,012 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 66 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,018 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1)
[INFO] 2023-11-08 14:52:11,018 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 66 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,023 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,024 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,025 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10
[INFO] 2023-11-08 14:52:11,025 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,025 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,031 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,031 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,032 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7
[INFO] 2023-11-08 14:52:11,032 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,032 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,038 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,038 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,039 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4
[INFO] 2023-11-08 14:52:11,039 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,039 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,042 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,042 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,042 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1
[INFO] 2023-11-08 14:52:11,042 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,042 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,046 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,046 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,047 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11
[INFO] 2023-11-08 14:52:11,047 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,047 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,050 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,050 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,051 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6
[INFO] 2023-11-08 14:52:11,051 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,051 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,054 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,055 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,055 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9
[INFO] 2023-11-08 14:52:11,055 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,055 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,059 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,060 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,060 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8
[INFO] 2023-11-08 14:52:11,060 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,061 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,064 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,065 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,066 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3
[INFO] 2023-11-08 14:52:11,066 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,066 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,070 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,071 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,071 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2
[INFO] 2023-11-08 14:52:11,072 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,072 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,075 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,076 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,077 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5
[INFO] 2023-11-08 14:52:11,077 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,077 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,084 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,085 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,086 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0
[INFO] 2023-11-08 14:52:11,086 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,086 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,086 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2)
[INFO] 2023-11-08 14:52:11,087 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 66 for 8 partitions
[INFO] 2023-11-08 14:52:11,088 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,088 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,089 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 77ms correlationId 66 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,092 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 67
[INFO] 2023-11-08 14:52:11,097 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 68 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,105 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7)
[INFO] 2023-11-08 14:52:11,105 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 68 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,109 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,110 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,111 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1
[INFO] 2023-11-08 14:52:11,111 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,111 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,120 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,121 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,121 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10
[INFO] 2023-11-08 14:52:11,121 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,121 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,128 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,129 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,129 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4
[INFO] 2023-11-08 14:52:11,129 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,129 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,133 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,134 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,134 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7
[INFO] 2023-11-08 14:52:11,134 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,134 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,136 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,136 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,136 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,136 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,136 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,136 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,136 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,136 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,138 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,138 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0
[INFO] 2023-11-08 14:52:11,138 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,138 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,139 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,139 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,139 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,139 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,139 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,139 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,139 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,141 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,142 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,142 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3
[INFO] 2023-11-08 14:52:11,142 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,142 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,145 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,145 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,145 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2
[INFO] 2023-11-08 14:52:11,146 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,146 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,152 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,153 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,153 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9
[INFO] 2023-11-08 14:52:11,153 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,153 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,156 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,157 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,157 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8
[INFO] 2023-11-08 14:52:11,157 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,157 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,160 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,161 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,161 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11
[INFO] 2023-11-08 14:52:11,161 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,161 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,164 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,165 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,165 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5
[INFO] 2023-11-08 14:52:11,165 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,165 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,169 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,169 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,170 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6
[INFO] 2023-11-08 14:52:11,170 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,170 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,170 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11)
[INFO] 2023-11-08 14:52:11,170 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 68 for 8 partitions
[INFO] 2023-11-08 14:52:11,171 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,171 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,172 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 76ms correlationId 68 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,174 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 69
[INFO] 2023-11-08 14:52:11,176 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 70 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,183 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1)
[INFO] 2023-11-08 14:52:11,183 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 70 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,187 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,188 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,189 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10
[INFO] 2023-11-08 14:52:11,189 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,189 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,192 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,193 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,193 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7
[INFO] 2023-11-08 14:52:11,193 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,193 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,197 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,197 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,197 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4
[INFO] 2023-11-08 14:52:11,197 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,197 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,200 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,201 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,201 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1
[INFO] 2023-11-08 14:52:11,201 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,201 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,204 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,205 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,205 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11
[INFO] 2023-11-08 14:52:11,205 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,205 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,210 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,211 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,211 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8
[INFO] 2023-11-08 14:52:11,211 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,211 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,215 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,215 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,216 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9
[INFO] 2023-11-08 14:52:11,216 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,216 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,223 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,224 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,224 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6
[INFO] 2023-11-08 14:52:11,224 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,224 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,228 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,229 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,229 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5
[INFO] 2023-11-08 14:52:11,229 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,229 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,233 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,233 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,233 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2
[INFO] 2023-11-08 14:52:11,233 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,234 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,239 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,239 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,240 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3
[INFO] 2023-11-08 14:52:11,240 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,240 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,245 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,246 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,246 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0
[INFO] 2023-11-08 14:52:11,246 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,246 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,246 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2)
[INFO] 2023-11-08 14:52:11,247 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 70 for 8 partitions
[INFO] 2023-11-08 14:52:11,247 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,248 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,249 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 73ms correlationId 70 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,308 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 71
[INFO] 2023-11-08 14:52:11,309 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,309 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,309 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,309 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,309 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,309 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,309 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,309 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,310 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 72 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,310 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,311 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,312 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,316 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3)
[INFO] 2023-11-08 14:52:11,316 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 72 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,320 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,321 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,322 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0
[INFO] 2023-11-08 14:52:11,322 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,322 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,326 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,327 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,327 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9
[INFO] 2023-11-08 14:52:11,327 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,327 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,330 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,331 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,331 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6
[INFO] 2023-11-08 14:52:11,331 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,331 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,334 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,335 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,335 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3
[INFO] 2023-11-08 14:52:11,335 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,335 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,338 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,338 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,338 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1
[INFO] 2023-11-08 14:52:11,339 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,339 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,341 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,342 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,342 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2
[INFO] 2023-11-08 14:52:11,342 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,342 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,345 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,345 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,345 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11
[INFO] 2023-11-08 14:52:11,345 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,346 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,348 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,349 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,349 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10
[INFO] 2023-11-08 14:52:11,349 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,349 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,351 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,352 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,352 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7
[INFO] 2023-11-08 14:52:11,352 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,352 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,354 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,355 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,355 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8
[INFO] 2023-11-08 14:52:11,355 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,355 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,357 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,358 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,358 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5
[INFO] 2023-11-08 14:52:11,358 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,358 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,361 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,361 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,361 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4
[INFO] 2023-11-08 14:52:11,361 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,361 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,361 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10)
[INFO] 2023-11-08 14:52:11,361 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 72 for 8 partitions
[INFO] 2023-11-08 14:52:11,362 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,362 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,363 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 53ms correlationId 72 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,365 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 73
[INFO] 2023-11-08 14:52:11,380 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 74 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,387 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5)
[INFO] 2023-11-08 14:52:11,387 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 74 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,391 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,392 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,393 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11
[INFO] 2023-11-08 14:52:11,393 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,393 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,397 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,398 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,398 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8
[INFO] 2023-11-08 14:52:11,398 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,398 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,402 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,403 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,403 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2
[INFO] 2023-11-08 14:52:11,403 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,404 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,407 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,408 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,408 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5
[INFO] 2023-11-08 14:52:11,408 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,408 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,413 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,413 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,414 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0
[INFO] 2023-11-08 14:52:11,414 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,414 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,417 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,418 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,418 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1
[INFO] 2023-11-08 14:52:11,418 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,418 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,422 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,422 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,422 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10
[INFO] 2023-11-08 14:52:11,423 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,423 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,426 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,426 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,427 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6
[INFO] 2023-11-08 14:52:11,427 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,427 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,430 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,430 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,430 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7
[INFO] 2023-11-08 14:52:11,430 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,430 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,433 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,434 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,434 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9
[INFO] 2023-11-08 14:52:11,434 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,434 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,437 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,438 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,438 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3
[INFO] 2023-11-08 14:52:11,438 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,438 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,441 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,442 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,442 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4
[INFO] 2023-11-08 14:52:11,442 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,442 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,442 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0)
[INFO] 2023-11-08 14:52:11,442 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 74 for 8 partitions
[INFO] 2023-11-08 14:52:11,443 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,443 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,444 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 64ms correlationId 74 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,447 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 75
[INFO] 2023-11-08 14:52:11,461 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 76 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,467 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9)
[INFO] 2023-11-08 14:52:11,467 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 76 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,471 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,471 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,472 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6
[INFO] 2023-11-08 14:52:11,473 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,473 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,476 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,477 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,477 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3
[INFO] 2023-11-08 14:52:11,477 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,477 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,480 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,481 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,481 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0
[INFO] 2023-11-08 14:52:11,481 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,481 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,484 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,485 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,485 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9
[INFO] 2023-11-08 14:52:11,485 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,485 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,488 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,488 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,488 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5
[INFO] 2023-11-08 14:52:11,489 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,489 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,491 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,492 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,492 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7
[INFO] 2023-11-08 14:52:11,492 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,492 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,495 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,495 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,495 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8
[INFO] 2023-11-08 14:52:11,495 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,495 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,498 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,499 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,499 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1
[INFO] 2023-11-08 14:52:11,499 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,499 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,502 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,502 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,502 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2
[INFO] 2023-11-08 14:52:11,502 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,502 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,505 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,505 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,505 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4
[INFO] 2023-11-08 14:52:11,506 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,506 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,508 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,509 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,509 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10
[INFO] 2023-11-08 14:52:11,509 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,509 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,512 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,512 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,512 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11
[INFO] 2023-11-08 14:52:11,512 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,513 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,513 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7)
[INFO] 2023-11-08 14:52:11,513 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 76 for 8 partitions
[INFO] 2023-11-08 14:52:11,514 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,514 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,515 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 54ms correlationId 76 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,516 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 77
[INFO] 2023-11-08 14:52:11,543 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 78 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,654 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,654 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,654 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,654 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,654 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,654 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,654 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,654 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,654 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,654 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,655 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,656 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,656 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,664 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9)
[INFO] 2023-11-08 14:52:11,665 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 78 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,669 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,670 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,671 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6
[INFO] 2023-11-08 14:52:11,671 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,671 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,674 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,675 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,675 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3
[INFO] 2023-11-08 14:52:11,675 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,675 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,678 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,679 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,680 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0
[INFO] 2023-11-08 14:52:11,680 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,680 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,683 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,684 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,684 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9
[INFO] 2023-11-08 14:52:11,684 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,684 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,688 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,689 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,689 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7
[INFO] 2023-11-08 14:52:11,689 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,689 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,692 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,693 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,693 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8
[INFO] 2023-11-08 14:52:11,693 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,693 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,696 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,697 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,697 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5
[INFO] 2023-11-08 14:52:11,697 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,697 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,700 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,701 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,701 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4
[INFO] 2023-11-08 14:52:11,701 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,701 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,704 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,705 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,705 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1
[INFO] 2023-11-08 14:52:11,705 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,705 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,708 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,708 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,709 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2
[INFO] 2023-11-08 14:52:11,709 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,709 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,711 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,712 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,712 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11
[INFO] 2023-11-08 14:52:11,712 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,712 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,715 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,715 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,715 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10
[INFO] 2023-11-08 14:52:11,715 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,715 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,715 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8)
[INFO] 2023-11-08 14:52:11,715 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 78 for 8 partitions
[INFO] 2023-11-08 14:52:11,716 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,716 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,717 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 174ms correlationId 78 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,719 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 79
[INFO] 2023-11-08 14:52:11,721 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 80 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:11,730 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6)
[INFO] 2023-11-08 14:52:11,731 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 80 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[INFO] 2023-11-08 14:52:11,734 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,735 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,735 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8
[INFO] 2023-11-08 14:52:11,736 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,736 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,739 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,739 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,739 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6
[INFO] 2023-11-08 14:52:11,739 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,739 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,742 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,743 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,743 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9
[INFO] 2023-11-08 14:52:11,743 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,743 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,745 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,746 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,746 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11
[INFO] 2023-11-08 14:52:11,746 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,746 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,749 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,749 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,750 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2
[INFO] 2023-11-08 14:52:11,750 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,750 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,753 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,754 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,754 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0
[INFO] 2023-11-08 14:52:11,754 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,754 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,758 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,758 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,758 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3
[INFO] 2023-11-08 14:52:11,758 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,758 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,762 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,762 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,763 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5
[INFO] 2023-11-08 14:52:11,763 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,763 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,766 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,767 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,767 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7
[INFO] 2023-11-08 14:52:11,767 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,767 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,770 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,771 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,771 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10
[INFO] 2023-11-08 14:52:11,771 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,771 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,774 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,775 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,775 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11
[INFO] 2023-11-08 14:52:11,775 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,775 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,778 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,779 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,779 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0
[INFO] 2023-11-08 14:52:11,779 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,779 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,783 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,783 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,783 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4
[INFO] 2023-11-08 14:52:11,784 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,784 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,786 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,787 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,787 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2
[INFO] 2023-11-08 14:52:11,787 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,787 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,798 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,799 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,799 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6
[INFO] 2023-11-08 14:52:11,799 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,799 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,803 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,804 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,804 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4
[INFO] 2023-11-08 14:52:11,804 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,804 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,808 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,809 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,809 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9
[INFO] 2023-11-08 14:52:11,809 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,809 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,812 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,813 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,813 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8
[INFO] 2023-11-08 14:52:11,813 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,813 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,816 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,817 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,817 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10
[INFO] 2023-11-08 14:52:11,817 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,817 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,820 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,820 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,821 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1
[INFO] 2023-11-08 14:52:11,821 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,821 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,824 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,824 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,824 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1
[INFO] 2023-11-08 14:52:11,824 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,824 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,827 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,828 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,828 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3
[INFO] 2023-11-08 14:52:11,828 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,828 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,831 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,831 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,832 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5
[INFO] 2023-11-08 14:52:11,832 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,832 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,835 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,836 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,836 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7
[INFO] 2023-11-08 14:52:11,836 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,836 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,836 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3)
[INFO] 2023-11-08 14:52:11,836 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 80 for 16 partitions
[INFO] 2023-11-08 14:52:11,837 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,838 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,839 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 118ms correlationId 80 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:11,841 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=2] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 81
[INFO] 2023-11-08 14:52:11,843 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 82 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,849 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3)
[INFO] 2023-11-08 14:52:11,849 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 82 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,856 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,856 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,857 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6
[INFO] 2023-11-08 14:52:11,857 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,858 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,861 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,861 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,861 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9
[INFO] 2023-11-08 14:52:11,861 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,862 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,865 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,865 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,865 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0
[INFO] 2023-11-08 14:52:11,865 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,865 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,868 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,869 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,869 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3
[INFO] 2023-11-08 14:52:11,869 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,869 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,872 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,872 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,872 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5
[INFO] 2023-11-08 14:52:11,872 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,872 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,875 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,875 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,876 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8
[INFO] 2023-11-08 14:52:11,876 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,876 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,878 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,878 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,879 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7
[INFO] 2023-11-08 14:52:11,879 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,879 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,881 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,882 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,882 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10
[INFO] 2023-11-08 14:52:11,882 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,882 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,885 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,885 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,885 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11
[INFO] 2023-11-08 14:52:11,885 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,885 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,888 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,888 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,889 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2
[INFO] 2023-11-08 14:52:11,889 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,889 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,892 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,892 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,892 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1
[INFO] 2023-11-08 14:52:11,892 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,892 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,895 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,896 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,896 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4
[INFO] 2023-11-08 14:52:11,896 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,896 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,896 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1)
[INFO] 2023-11-08 14:52:11,896 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 82 for 8 partitions
[INFO] 2023-11-08 14:52:11,897 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,897 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,898 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 55ms correlationId 82 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,900 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 83
[INFO] 2023-11-08 14:52:11,902 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 84 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,906 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2)
[INFO] 2023-11-08 14:52:11,907 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 84 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[WARN] 2023-11-08 14:52:11,908 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,908 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,908 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,908 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,908 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,908 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,909 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,909 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:11,909 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,909 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,909 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,909 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,909 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,909 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,909 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,909 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,910 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,910 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,910 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,910 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,910 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,910 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,910 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,910 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,910 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,910 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,911 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,912 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,913 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,913 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,913 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,914 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5
[INFO] 2023-11-08 14:52:11,914 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,914 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,918 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,918 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,919 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8
[INFO] 2023-11-08 14:52:11,919 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,919 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,922 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,923 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,923 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11
[INFO] 2023-11-08 14:52:11,923 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,923 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,927 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,927 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,927 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2
[INFO] 2023-11-08 14:52:11,927 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,928 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,931 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,931 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,931 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6
[INFO] 2023-11-08 14:52:11,931 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,931 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,935 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,935 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,935 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7
[INFO] 2023-11-08 14:52:11,935 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,935 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,938 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,939 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,939 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10
[INFO] 2023-11-08 14:52:11,939 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,939 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,943 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,943 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,943 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9
[INFO] 2023-11-08 14:52:11,943 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,943 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,946 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,947 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,947 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0
[INFO] 2023-11-08 14:52:11,947 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,947 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,950 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,951 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,951 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1
[INFO] 2023-11-08 14:52:11,951 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,951 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,954 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,955 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,955 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4
[INFO] 2023-11-08 14:52:11,955 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,955 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,958 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,959 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,959 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3
[INFO] 2023-11-08 14:52:11,959 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,959 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,959 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1)
[INFO] 2023-11-08 14:52:11,959 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 84 for 8 partitions
[INFO] 2023-11-08 14:52:11,960 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,960 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,961 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 59ms correlationId 84 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,963 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 85
[INFO] 2023-11-08 14:52:11,987 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 86 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,992 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6)
[INFO] 2023-11-08 14:52:11,992 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 86 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,995 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,996 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:11,996 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3
[INFO] 2023-11-08 14:52:11,997 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,997 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,000 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,000 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,000 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0
[INFO] 2023-11-08 14:52:12,000 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,000 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,003 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,004 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,004 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9
[INFO] 2023-11-08 14:52:12,004 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,004 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,007 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,008 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,008 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6
[INFO] 2023-11-08 14:52:12,008 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,008 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,011 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,012 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,012 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1
[INFO] 2023-11-08 14:52:12,012 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,012 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,015 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,016 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,016 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2
[INFO] 2023-11-08 14:52:12,016 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,016 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,019 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,019 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,019 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4
[INFO] 2023-11-08 14:52:12,019 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,019 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,022 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,023 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,023 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10
[INFO] 2023-11-08 14:52:12,023 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,023 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,026 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,026 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,026 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11
[INFO] 2023-11-08 14:52:12,027 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,027 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,030 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,030 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,030 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5
[INFO] 2023-11-08 14:52:12,031 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,031 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,033 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,034 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,034 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7
[INFO] 2023-11-08 14:52:12,034 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,034 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,038 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,038 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,038 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8
[INFO] 2023-11-08 14:52:12,038 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,038 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,039 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10)
[INFO] 2023-11-08 14:52:12,039 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 86 for 8 partitions
[INFO] 2023-11-08 14:52:12,039 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,040 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,041 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 54ms correlationId 86 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,042 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 87
[INFO] 2023-11-08 14:52:12,067 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 88 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,072 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7)
[INFO] 2023-11-08 14:52:12,072 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 88 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,075 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,076 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,076 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4
[INFO] 2023-11-08 14:52:12,076 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,076 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,079 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,080 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,080 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1
[INFO] 2023-11-08 14:52:12,080 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,080 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,083 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,083 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,083 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10
[INFO] 2023-11-08 14:52:12,083 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,083 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,086 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,087 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,087 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7
[INFO] 2023-11-08 14:52:12,087 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,087 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,090 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,090 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,091 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5
[INFO] 2023-11-08 14:52:12,091 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,091 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,093 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,094 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,094 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2
[INFO] 2023-11-08 14:52:12,094 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,094 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,096 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,097 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,097 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3
[INFO] 2023-11-08 14:52:12,097 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,097 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,100 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,100 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,101 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0
[INFO] 2023-11-08 14:52:12,101 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,101 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,103 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,104 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,104 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11
[INFO] 2023-11-08 14:52:12,104 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,104 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,106 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,107 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,107 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8
[INFO] 2023-11-08 14:52:12,107 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,107 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,121 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,121 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,121 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9
[INFO] 2023-11-08 14:52:12,122 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,122 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,125 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,125 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,126 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6
[INFO] 2023-11-08 14:52:12,126 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,126 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,126 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11)
[INFO] 2023-11-08 14:52:12,126 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 88 for 8 partitions
[INFO] 2023-11-08 14:52:12,127 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,127 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,128 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 60ms correlationId 88 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,129 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 89
[INFO] 2023-11-08 14:52:12,146 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 90 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,150 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10)
[INFO] 2023-11-08 14:52:12,150 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 90 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,153 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,154 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,155 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1
[INFO] 2023-11-08 14:52:12,155 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,156 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,158 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,159 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,159 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4
[INFO] 2023-11-08 14:52:12,159 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,159 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,161 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,162 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,162 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,163 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7
[INFO] 2023-11-08 14:52:12,162 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,163 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,163 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,163 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,163 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,163 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,163 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,163 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,163 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,163 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,163 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,163 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,163 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,164 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,166 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,167 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,167 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10
[INFO] 2023-11-08 14:52:12,167 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,167 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,171 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,171 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,171 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0
[INFO] 2023-11-08 14:52:12,171 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,171 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,174 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,175 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,175 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2
[INFO] 2023-11-08 14:52:12,175 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,175 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,179 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,179 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,179 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3
[INFO] 2023-11-08 14:52:12,179 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,179 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,182 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,183 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,183 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5
[INFO] 2023-11-08 14:52:12,183 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,183 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,186 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,187 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,187 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6
[INFO] 2023-11-08 14:52:12,187 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,187 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,189 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,190 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,190 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8
[INFO] 2023-11-08 14:52:12,190 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,190 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,192 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,193 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,193 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9
[INFO] 2023-11-08 14:52:12,193 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,193 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,195 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,196 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,196 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11
[INFO] 2023-11-08 14:52:12,196 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,196 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,196 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8)
[INFO] 2023-11-08 14:52:12,196 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 90 for 8 partitions
[INFO] 2023-11-08 14:52:12,197 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,197 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,198 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 52ms correlationId 90 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,199 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 91
[INFO] 2023-11-08 14:52:12,232 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 92 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,236 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10)
[INFO] 2023-11-08 14:52:12,236 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 92 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,239 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,240 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,241 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4
[INFO] 2023-11-08 14:52:12,241 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,241 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,243 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,244 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,244 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1
[INFO] 2023-11-08 14:52:12,244 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,244 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,247 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,247 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,247 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7
[INFO] 2023-11-08 14:52:12,247 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,247 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,250 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,250 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,251 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10
[INFO] 2023-11-08 14:52:12,251 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,251 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,253 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,254 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,254 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3
[INFO] 2023-11-08 14:52:12,254 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,254 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,256 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,256 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,257 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6
[INFO] 2023-11-08 14:52:12,257 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,257 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,259 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,259 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,260 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5
[INFO] 2023-11-08 14:52:12,260 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,260 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,262 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,262 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,263 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0
[INFO] 2023-11-08 14:52:12,263 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,263 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,265 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,265 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,266 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2
[INFO] 2023-11-08 14:52:12,266 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,266 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,268 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,268 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,269 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11
[INFO] 2023-11-08 14:52:12,269 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,269 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,271 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,271 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,271 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8
[INFO] 2023-11-08 14:52:12,272 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,272 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,274 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,274 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,275 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9
[INFO] 2023-11-08 14:52:12,275 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,275 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,275 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9)
[INFO] 2023-11-08 14:52:12,275 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 92 for 8 partitions
[INFO] 2023-11-08 14:52:12,276 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,276 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,277 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 45ms correlationId 92 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,279 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 93
[INFO] 2023-11-08 14:52:12,321 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 94 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,325 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-monitoring-7, _confluent-monitoring-10, _confluent-monitoring-4, _confluent-monitoring-1)
[INFO] 2023-11-08 14:52:12,325 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 94 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,328 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,329 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-7 in /mnt/data/data0/logs/_confluent-monitoring-7 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,330 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-7 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-7
[INFO] 2023-11-08 14:52:12,330 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-7 broker=2] Log loaded for partition _confluent-monitoring-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,330 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-monitoring-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,332 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,333 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-10 in /mnt/data/data0/logs/_confluent-monitoring-10 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,333 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-10 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-10
[INFO] 2023-11-08 14:52:12,333 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-10 broker=2] Log loaded for partition _confluent-monitoring-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,333 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-monitoring-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,335 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,336 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-4 in /mnt/data/data0/logs/_confluent-monitoring-4 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,336 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-4 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-4
[INFO] 2023-11-08 14:52:12,336 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-4 broker=2] Log loaded for partition _confluent-monitoring-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,336 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-monitoring-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,338 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,339 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-1 in /mnt/data/data0/logs/_confluent-monitoring-1 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,339 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-1 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-1
[INFO] 2023-11-08 14:52:12,339 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-1 broker=2] Log loaded for partition _confluent-monitoring-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,339 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-monitoring-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,341 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,342 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-8 in /mnt/data/data0/logs/_confluent-monitoring-8 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,342 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-8 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-8
[INFO] 2023-11-08 14:52:12,342 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-8 broker=2] Log loaded for partition _confluent-monitoring-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,342 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-monitoring-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,344 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,345 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-9 in /mnt/data/data0/logs/_confluent-monitoring-9 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,345 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-9 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-9
[INFO] 2023-11-08 14:52:12,345 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-9 broker=2] Log loaded for partition _confluent-monitoring-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,345 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-monitoring-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,347 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,347 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-3 in /mnt/data/data0/logs/_confluent-monitoring-3 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,347 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-3 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-3
[INFO] 2023-11-08 14:52:12,348 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-3 broker=2] Log loaded for partition _confluent-monitoring-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,348 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-monitoring-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,350 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,350 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-5 in /mnt/data/data0/logs/_confluent-monitoring-5 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,350 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-5 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-5
[INFO] 2023-11-08 14:52:12,350 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-5 broker=2] Log loaded for partition _confluent-monitoring-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,350 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-monitoring-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,352 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,352 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-6 in /mnt/data/data0/logs/_confluent-monitoring-6 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,353 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-6 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-6
[INFO] 2023-11-08 14:52:12,353 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-6 broker=2] Log loaded for partition _confluent-monitoring-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,353 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-monitoring-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,355 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,355 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-0 in /mnt/data/data0/logs/_confluent-monitoring-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,355 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-0 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-0
[INFO] 2023-11-08 14:52:12,356 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-0 broker=2] Log loaded for partition _confluent-monitoring-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,356 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-monitoring-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,358 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,358 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-2 in /mnt/data/data0/logs/_confluent-monitoring-2 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,358 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-2 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-2
[INFO] 2023-11-08 14:52:12,358 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-2 broker=2] Log loaded for partition _confluent-monitoring-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,358 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-monitoring-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,360 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,360 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-monitoring-11 in /mnt/data/data0/logs/_confluent-monitoring-11 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,361 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-11 broker=2] No checkpointed highwatermark is found for partition _confluent-monitoring-11
[INFO] 2023-11-08 14:52:12,361 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-monitoring-11 broker=2] Log loaded for partition _confluent-monitoring-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,361 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-monitoring-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,361 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-monitoring-11, _confluent-monitoring-9, _confluent-monitoring-8, _confluent-monitoring-3, _confluent-monitoring-2, _confluent-monitoring-0, _confluent-monitoring-6, _confluent-monitoring-5)
[INFO] 2023-11-08 14:52:12,361 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 94 for 8 partitions
[INFO] 2023-11-08 14:52:12,361 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-monitoring-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,362 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-monitoring-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,363 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 42ms correlationId 94 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,364 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 95
[DEBUG] 2023-11-08 14:52:12,371 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:12,372 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:12,413 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 96 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,419 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-monitoring-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,419 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,419 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,419 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,419 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,419 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,419 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,419 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,419 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-monitoring-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-monitoring-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,420 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-monitoring-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,421 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,422 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10)
[INFO] 2023-11-08 14:52:12,422 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 96 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,426 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,426 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,427 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7
[INFO] 2023-11-08 14:52:12,427 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,427 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,430 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,430 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,430 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4
[INFO] 2023-11-08 14:52:12,430 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,430 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,433 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,433 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,433 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1
[INFO] 2023-11-08 14:52:12,433 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,433 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,436 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,436 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,437 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10
[INFO] 2023-11-08 14:52:12,437 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,437 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,439 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,440 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,440 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6
[INFO] 2023-11-08 14:52:12,440 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,440 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,443 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,443 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,443 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8
[INFO] 2023-11-08 14:52:12,443 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,443 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,445 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,446 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,446 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9
[INFO] 2023-11-08 14:52:12,446 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,446 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,449 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,449 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,449 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2
[INFO] 2023-11-08 14:52:12,449 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,449 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,451 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,452 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,452 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3
[INFO] 2023-11-08 14:52:12,452 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,452 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,454 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,455 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,455 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5
[INFO] 2023-11-08 14:52:12,455 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,455 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,457 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,457 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,457 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0
[INFO] 2023-11-08 14:52:12,457 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,457 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,459 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,460 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,460 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11
[INFO] 2023-11-08 14:52:12,460 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,460 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,460 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5)
[INFO] 2023-11-08 14:52:12,460 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 96 for 8 partitions
[INFO] 2023-11-08 14:52:12,461 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,461 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,462 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 49ms correlationId 96 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,502 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 97
[TRACE] 2023-11-08 14:52:12,519 [qtp1625817721-233] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 233]
[TRACE] 2023-11-08 14:52:12,520 [qtp1625817721-233] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 233]
[DEBUG] 2023-11-08 14:52:12,522 [qtp1625817721-233] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[INFO] 2023-11-08 14:52:12,524 [qtp1625817721-233] io.confluent.mds.request.logger log - 55 * Server has received a request on thread qtp1625817721-233null55 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull55 > User Principal: ksql
55 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null55 > Cache-Control: no-cachenull55 > Connection: keep-alivenull55 > Content-Type: application/jsonnull55 > Host: kafka.confluent.svc.cluster.local:8090null55 > Pragma: no-cachenull55 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:12,526 [qtp1625817721-233] io.confluent.mds.request.logger log - 55 * Server responded with a response on thread qtp1625817721-233null55 < 200null55 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:12,527 [qtp1625817721-233] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:12 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:12,527 [qtp1625817721-233] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:12 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:12,527 [qtp1625817721-233] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:12 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:12,588 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 98 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,592 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3)
[INFO] 2023-11-08 14:52:12,592 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 98 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,596 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,596 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,597 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6
[INFO] 2023-11-08 14:52:12,597 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,598 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,600 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,601 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,601 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9
[INFO] 2023-11-08 14:52:12,601 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,601 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,604 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,604 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,604 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0
[INFO] 2023-11-08 14:52:12,604 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,604 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,607 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,607 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,608 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3
[INFO] 2023-11-08 14:52:12,608 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,608 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,610 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,611 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,611 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4
[INFO] 2023-11-08 14:52:12,611 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,611 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,613 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,614 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,614 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5
[INFO] 2023-11-08 14:52:12,614 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,614 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,616 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,617 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,617 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7
[INFO] 2023-11-08 14:52:12,617 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,617 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,619 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,620 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,620 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8
[INFO] 2023-11-08 14:52:12,620 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,620 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,622 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,623 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,623 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10
[INFO] 2023-11-08 14:52:12,623 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,623 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,626 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,626 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,626 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11
[INFO] 2023-11-08 14:52:12,626 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,626 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,629 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,630 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,630 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1
[INFO] 2023-11-08 14:52:12,630 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,630 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,633 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,634 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,634 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2
[INFO] 2023-11-08 14:52:12,634 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,634 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,634 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1)
[INFO] 2023-11-08 14:52:12,634 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 98 for 8 partitions
[INFO] 2023-11-08 14:52:12,635 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,635 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,636 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 48ms correlationId 98 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,639 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 99
[INFO] 2023-11-08 14:52:12,669 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 100 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,675 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2)
[INFO] 2023-11-08 14:52:12,675 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 100 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,679 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,679 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,681 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11
[INFO] 2023-11-08 14:52:12,681 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,681 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,683 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,684 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,684 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8
[INFO] 2023-11-08 14:52:12,684 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,684 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,687 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,687 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,687 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5
[INFO] 2023-11-08 14:52:12,687 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,687 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,690 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,690 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,690 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2
[INFO] 2023-11-08 14:52:12,690 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,690 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,693 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,693 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,693 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10
[INFO] 2023-11-08 14:52:12,694 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,694 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,700 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,700 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,700 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9
[INFO] 2023-11-08 14:52:12,701 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,701 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:12,701 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,704 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,705 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,705 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6
[INFO] 2023-11-08 14:52:12,705 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,705 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,708 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,709 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_partition_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 3600000)
[INFO] 2023-11-08 14:52:12,709 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,709 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7
[INFO] 2023-11-08 14:52:12,709 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,710 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,711 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,712 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,712 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,712 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,713 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,713 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,713 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,713 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,713 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,713 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,713 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4
[INFO] 2023-11-08 14:52:12,713 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,713 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,713 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,713 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,713 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,714 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,715 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,715 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,715 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,715 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,716 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,716 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,717 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3
[INFO] 2023-11-08 14:52:12,717 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,717 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,719 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,719 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,719 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0
[INFO] 2023-11-08 14:52:12,719 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,719 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,722 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,722 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,722 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1
[INFO] 2023-11-08 14:52:12,722 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,722 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,723 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1)
[INFO] 2023-11-08 14:52:12,723 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 100 for 8 partitions
[INFO] 2023-11-08 14:52:12,723 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,723 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,725 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 55ms correlationId 100 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,726 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 101
[INFO] 2023-11-08 14:52:12,758 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 102 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,762 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3)
[INFO] 2023-11-08 14:52:12,762 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 102 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,765 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,766 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,767 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6
[INFO] 2023-11-08 14:52:12,767 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,767 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-monitoring-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-monitoring-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-monitoring-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-monitoring-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,771 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,772 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,772 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9
[INFO] 2023-11-08 14:52:12,772 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,772 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,773 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_broker_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 12000000)
[INFO] 2023-11-08 14:52:12,774 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,775 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,775 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,776 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0
[INFO] 2023-11-08 14:52:12,778 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,778 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,782 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,782 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,782 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3
[INFO] 2023-11-08 14:52:12,782 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,783 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,785 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,786 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,786 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8
[INFO] 2023-11-08 14:52:12,786 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,786 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,789 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,789 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,789 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,789 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,789 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,789 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,789 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,789 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,790 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,790 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,790 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7
[INFO] 2023-11-08 14:52:12,790 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,790 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,790 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,790 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,790 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,791 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,791 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,791 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,791 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,791 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,791 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,791 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,792 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,792 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,792 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,792 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,792 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,792 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,794 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,794 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,794 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5
[INFO] 2023-11-08 14:52:12,794 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,794 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,797 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,797 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,798 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11
[INFO] 2023-11-08 14:52:12,798 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,798 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,801 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,801 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,802 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10
[INFO] 2023-11-08 14:52:12,802 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,802 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,806 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,807 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,807 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4
[INFO] 2023-11-08 14:52:12,807 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,807 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,810 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,811 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,811 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2
[INFO] 2023-11-08 14:52:12,811 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,811 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,814 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,815 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,815 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1
[INFO] 2023-11-08 14:52:12,816 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,816 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,816 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1)
[INFO] 2023-11-08 14:52:12,816 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 102 for 8 partitions
[INFO] 2023-11-08 14:52:12,817 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,817 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,818 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 61ms correlationId 102 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,820 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 103
[TRACE] 2023-11-08 14:52:12,821 [qtp1625817721-233] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 233]
[TRACE] 2023-11-08 14:52:12,822 [qtp1625817721-233] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user ksql with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=ksql,dc=test,dc=com [Thread 233]
[DEBUG] 2023-11-08 14:52:12,824 [qtp1625817721-233] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for ksql
[INFO] 2023-11-08 14:52:12,828 [qtp1625817721-233] io.confluent.mds.request.logger log - 56 * Server has received a request on thread qtp1625817721-233null56 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull56 > User Principal: ksql
56 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null56 > Cache-Control: no-cachenull56 > Connection: keep-alivenull56 > Content-Type: application/jsonnull56 > Host: kafka.confluent.svc.cluster.local:8090null56 > Pragma: no-cachenull56 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:12,829 [qtp1625817721-233] io.confluent.mds.request.logger log - 56 * Server responded with a response on thread qtp1625817721-233null56 < 200null56 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:12,831 [qtp1625817721-233] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:12 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:52:12,831 [qtp1625817721-233] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:12 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:52:12,831 [qtp1625817721-233] io.confluent.rest-utils.requests write - 10.40.0.16 - ksql [08/Nov/2023:14:52:12 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:52:12,846 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 104 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,852 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2)
[INFO] 2023-11-08 14:52:12,853 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 104 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,856 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,857 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,858 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11
[INFO] 2023-11-08 14:52:12,858 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,858 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,862 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,863 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,863 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8
[INFO] 2023-11-08 14:52:12,863 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,863 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,866 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,867 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,867 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5
[INFO] 2023-11-08 14:52:12,867 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,867 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,870 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,870 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,870 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2
[INFO] 2023-11-08 14:52:12,870 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,871 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Leader _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,0,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,874 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,874 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,874 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7
[INFO] 2023-11-08 14:52:12,874 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,874 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,877 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,878 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,878 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10
[INFO] 2023-11-08 14:52:12,878 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,878 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,881 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,882 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,882 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9
[INFO] 2023-11-08 14:52:12,882 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,882 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,885 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,885 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,885 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4
[INFO] 2023-11-08 14:52:12,885 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,885 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,888 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,888 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,888 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3
[INFO] 2023-11-08 14:52:12,888 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,888 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,891 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,891 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,891 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6
[INFO] 2023-11-08 14:52:12,892 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,892 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,894 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,895 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,895 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0
[INFO] 2023-11-08 14:52:12,895 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,895 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,897 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,994 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,995 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 broker=2] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1
[INFO] 2023-11-08 14:52:12,995 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 broker=2] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,995 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,996 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1)
[INFO] 2023-11-08 14:52:12,996 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 104 for 8 partitions
[INFO] 2023-11-08 14:52:12,997 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-0-Default to broker 0 for partitions HashMap(_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 -> InitialFetchState(None,BrokerEndPoint(id=0, host=kafka-0.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,997 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,998 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 152ms correlationId 104 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:13,000 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=2] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 105
[INFO] 2023-11-08 14:52:13,002 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Handling LeaderAndIsr request correlationId 106 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:52:13,003 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 2] Removed fetcher for partitions Set(_confluent_balancer_api_state-0)
[INFO] 2023-11-08 14:52:13,003 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Stopped fetchers as part of LeaderAndIsr request correlationId 106 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions
[INFO] 2023-11-08 14:52:13,007 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_api_state-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:13,007 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent_balancer_api_state-0 in /mnt/data/data0/logs/_confluent_balancer_api_state-0 with properties {cleanup.policy=compact, retention.ms=-1}
[INFO] 2023-11-08 14:52:13,008 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent_balancer_api_state-0 broker=2] No checkpointed highwatermark is found for partition _confluent_balancer_api_state-0
[INFO] 2023-11-08 14:52:13,008 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent_balancer_api_state-0 broker=2] Log loaded for partition _confluent_balancer_api_state-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:13,008 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Leader _confluent_balancer_api_state-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 2,1,0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:13,009 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=2] Finished LeaderAndIsr request in 7ms correlationId 106 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:52:13,011 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=2] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 107
[INFO] 2023-11-08 14:52:13,091 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,091 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,091 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[WARN] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,298 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,299 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,299 [ReplicaFetcherThread-0-0-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=2, leaderId=0, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,299 [ReplicaFetcherThread-0-0-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:14,082 [data-plane-kafka-request-handler-1] kafka.server.ZkAdminManager info - [Admin Manager on Broker 2]: User:operator is updating topic _confluent_balancer_api_state with new configuration : cleanup.policy -> compact,retention.ms -> -1
[INFO] 2023-11-08 14:52:14,095 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:14,097 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_api_state with config: HashMap(cleanup.policy -> compact, retention.ms -> -1)
[INFO] 2023-11-08 14:52:14,098 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,108 [data-plane-kafka-network-thread-2-ListenerName(REPLICATION)-SASL_SSL-11] kafka.request.logger updateRequestMetrics - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":44,"requestApiVersion":1,"correlationId":5,"clientId":"kafka-cruise-control","requestApiKeyName":"INCREMENTAL_ALTER_CONFIGS"},"request":{"resources":[{"resourceType":2,"resourceName":"_confluent_balancer_api_state","configs":[{"name":"cleanup.policy","configOperation":0,"value":"compact"},{"name":"retention.ms","configOperation":0,"value":"-1"}]}],"validateOnly":false},"response":{"throttleTimeMs":0,"responses":[{"errorCode":0,"errorMessage":null,"resourceType":2,"resourceName":"_confluent_balancer_api_state"}]},"connection":"10.40.0.15:9072-10.40.1.7:44242-12","totalTimeMs":158.654,"requestQueueTimeMs":2.14,"localTimeMs":153.403,"remoteTimeMs":2.958,"throttleTimeMs":0,"responseQueueTimeMs":0.06,"sendTimeMs":0.09,"securityProtocol":"SASL_SSL","principal":{"class":"KafkaPrincipal","type":"User","name":"operator","tokenAuthenticated":false},"listener":"REPLICATION","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.1.0-ce"},"isDisconnectedClient":false}
[INFO] 2023-11-08 14:52:14,694 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_partition_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 3600000)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,698 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,699 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,751 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:14,754 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_broker_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 12000000)
[INFO] 2023-11-08 14:52:14,754 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,757 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,810 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:14,813 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_partition_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 3600000)
[INFO] 2023-11-08 14:52:14,813 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,862 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:14,864 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_broker_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 12000000)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[DEBUG] 2023-11-08 14:52:15,371 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:15,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:15,963 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,963 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:3 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:4 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:4 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,965 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:5 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,967 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:6 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,968 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,969 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:7 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:18,372 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:18,375 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:52:18,827 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:52:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:52:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:52:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:52:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:52:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455138842
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,845 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:52:18,846 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455138846
[INFO] 2023-11-08 14:52:18,856 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:52:18,868 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:52:18,869 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:52:18,869 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:52:18,869 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:52:18,869 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:52:18,869 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:52:18,870 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:52:18,870 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:52:18,870 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:52:18,870 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:52:18,870 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:52:18,870 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:52:18,870 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:52:21,373 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:21,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:24,374 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:24,375 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:24,454 [qtp1625817721-174] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 174]
[TRACE] 2023-11-08 14:52:24,456 [qtp1625817721-174] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 174]
[DEBUG] 2023-11-08 14:52:24,458 [qtp1625817721-174] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:52:24,459 [qtp1625817721-174] io.confluent.mds.request.logger log - 57 * Server has received a request on thread qtp1625817721-174null57 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull57 > User Principal: c3
57 > authorization: Basic YzM6YzMtc2VjcmV0null57 > host: kafka.confluent.svc.cluster.local:8090null57 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:52:24,459 [qtp1625817721-174] io.confluent.mds.request.logger log - 57 * Server responded with a response on thread qtp1625817721-174null57 < 200null57 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:52:24,461 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:24 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[INFO] 2023-11-08 14:52:24,461 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:24 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[INFO] 2023-11-08 14:52:24,461 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:24 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[DEBUG] 2023-11-08 14:52:27,375 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:27,376 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:28,225 [qtp1625817721-174] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 174]
[TRACE] 2023-11-08 14:52:28,227 [qtp1625817721-174] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 174]
[DEBUG] 2023-11-08 14:52:28,232 [qtp1625817721-174] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:52:28,234 [qtp1625817721-174] io.confluent.mds.request.logger log - 58 * Server has received a request on thread qtp1625817721-174null58 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull58 > User Principal: c3
58 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null58 > Cache-Control: no-cachenull58 > Connection: keep-alivenull58 > Content-Type: application/jsonnull58 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null58 > Pragma: no-cachenull58 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:28,239 [qtp1625817721-174] io.confluent.mds.request.logger log - 58 * Server responded with a response on thread qtp1625817721-174null58 < 200null58 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:28,241 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:52:28 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 16
[INFO] 2023-11-08 14:52:28,241 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:52:28 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 16
[INFO] 2023-11-08 14:52:28,241 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:52:28 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 16
[TRACE] 2023-11-08 14:52:28,290 [qtp1625817721-247] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 247]
[TRACE] 2023-11-08 14:52:28,292 [qtp1625817721-247] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 247]
[DEBUG] 2023-11-08 14:52:28,295 [qtp1625817721-247] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:52:28,296 [qtp1625817721-247] io.confluent.mds.request.logger log - 59 * Server has received a request on thread qtp1625817721-247null59 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull59 > User Principal: c3
59 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null59 > Cache-Control: no-cachenull59 > Connection: keep-alivenull59 > Content-Type: application/jsonnull59 > Host: kafka.confluent.svc.cluster.local:8090null59 > Pragma: no-cachenull59 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:28,300 [qtp1625817721-247] io.confluent.mds.request.logger log - 59 * Server responded with a response on thread qtp1625817721-247null59 < 200null59 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:28,301 [qtp1625817721-247] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:52:28 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:52:28,301 [qtp1625817721-247] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:52:28 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:52:28,301 [qtp1625817721-247] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:52:28 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 11
[DEBUG] 2023-11-08 14:52:30,375 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:30,376 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:33,375 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:33,376 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:36,375 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:36,376 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:39,376 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:39,377 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:40,612 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[TRACE] 2023-11-08 14:52:40,614 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 179]
[DEBUG] 2023-11-08 14:52:40,628 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:52:40,629 [qtp1625817721-179] io.confluent.mds.request.logger log - 60 * Server has received a request on thread qtp1625817721-179null60 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull60 > User Principal: c3
60 > authorization: Basic YzM6YzMtc2VjcmV0null60 > host: kafka.confluent.svc.cluster.local:8090null60 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:52:40,630 [qtp1625817721-179] io.confluent.mds.request.logger log - 60 * Server responded with a response on thread qtp1625817721-179null60 < 200null60 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:52:40,631 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:40 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 20
[INFO] 2023-11-08 14:52:40,631 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:40 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 20
[INFO] 2023-11-08 14:52:40,631 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:40 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 20
[DEBUG] 2023-11-08 14:52:42,377 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:42,378 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:45,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:45,378 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:46,664 [qtp1625817721-156] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 156]
[TRACE] 2023-11-08 14:52:46,666 [qtp1625817721-156] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 156]
[DEBUG] 2023-11-08 14:52:46,668 [qtp1625817721-156] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:46,669 [qtp1625817721-156] io.confluent.mds.request.logger log - 61 * Server has received a request on thread qtp1625817721-156null61 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull61 > User Principal: connect
61 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null61 > Cache-Control: no-cachenull61 > Connection: keep-alivenull61 > Content-Type: application/jsonnull61 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null61 > Pragma: no-cachenull61 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:46,672 [qtp1625817721-156] io.confluent.mds.request.logger log - 61 * Server responded with a response on thread qtp1625817721-156null61 < 200null61 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:46,674 [qtp1625817721-156] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:46 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:46,674 [qtp1625817721-156] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:46 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:46,674 [qtp1625817721-156] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:46 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[TRACE] 2023-11-08 14:52:46,888 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[TRACE] 2023-11-08 14:52:46,889 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 179]
[DEBUG] 2023-11-08 14:52:46,891 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:46,893 [qtp1625817721-179] io.confluent.mds.request.logger log - 62 * Server has received a request on thread qtp1625817721-179null62 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull62 > User Principal: connect
62 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null62 > Cache-Control: no-cachenull62 > Connection: keep-alivenull62 > Content-Type: application/jsonnull62 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null62 > Pragma: no-cachenull62 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:46,896 [qtp1625817721-179] io.confluent.mds.request.logger log - 62 * Server responded with a response on thread qtp1625817721-179null62 < 200null62 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:46,897 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:46 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:46,897 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:46 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:46,897 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:46 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[TRACE] 2023-11-08 14:52:48,034 [qtp1625817721-254] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 254]
[TRACE] 2023-11-08 14:52:48,036 [qtp1625817721-254] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 254]
[DEBUG] 2023-11-08 14:52:48,038 [qtp1625817721-254] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:52:48,039 [qtp1625817721-254] io.confluent.mds.request.logger log - 63 * Server has received a request on thread qtp1625817721-254null63 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull63 > User Principal: c3
63 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null63 > Cache-Control: no-cachenull63 > Connection: keep-alivenull63 > Content-Type: application/jsonnull63 > Host: kafka.confluent.svc.cluster.local:8090null63 > Pragma: no-cachenull63 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:48,043 [qtp1625817721-254] io.confluent.mds.request.logger log - 63 * Server responded with a response on thread qtp1625817721-254null63 < 200null63 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:48,044 [qtp1625817721-254] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:52:48 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:48,044 [qtp1625817721-254] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:52:48 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:48,044 [qtp1625817721-254] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:52:48 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[DEBUG] 2023-11-08 14:52:48,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:48,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:48,626 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:52:48,627 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:52:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:52:48,629 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,630 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:52:48,630 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:52:48,630 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:52:48,630 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455168630
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,633 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,634 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:52:48,635 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455168635
[INFO] 2023-11-08 14:52:48,643 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:52:48,657 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:52:48,657 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:52:48,658 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:52:48,658 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:52:48,658 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:52:48,658 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:52:48,658 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:52:48,659 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:52:48,659 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:52:48,659 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:52:48,659 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:52:48,659 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:52:48,659 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:52:51,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:51,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:54,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:54,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:57,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:57,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:00,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:00,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:03,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:03,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:06,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:06,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:09,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:09,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:12,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:12,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:15,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:15,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:15,963 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,963 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:4 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:5 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:5 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,965 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:7 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:8 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:6 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[TRACE] 2023-11-08 14:53:17,917 [qtp1625817721-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 181]
[TRACE] 2023-11-08 14:53:17,918 [qtp1625817721-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 181]
[DEBUG] 2023-11-08 14:53:17,934 [qtp1625817721-181] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:53:17,935 [qtp1625817721-181] io.confluent.mds.request.logger log - 64 * Server has received a request on thread qtp1625817721-181null64 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull64 > User Principal: c3
64 > authorization: Basic YzM6YzMtc2VjcmV0null64 > host: kafka.confluent.svc.cluster.local:8090null64 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:53:17,936 [qtp1625817721-181] io.confluent.mds.request.logger log - 64 * Server responded with a response on thread qtp1625817721-181null64 < 200null64 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:53:17,937 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:53:17 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 21
[INFO] 2023-11-08 14:53:17,937 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:53:17 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 21
[INFO] 2023-11-08 14:53:17,937 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:53:17 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 21
[DEBUG] 2023-11-08 14:53:18,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:18,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:53:18,690 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:53:18,690 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:53:18,691 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:53:18,703 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,704 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:53:18,704 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:53:18,704 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:53:18,704 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455198704
[WARN] 2023-11-08 14:53:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:53:18,708 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455198708
[INFO] 2023-11-08 14:53:18,717 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:53:18,731 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:53:18,731 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:53:18,732 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:53:18,732 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:53:18,732 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:53:18,732 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:53:18,733 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:53:18,733 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:53:18,733 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:53:18,733 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:53:18,733 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:53:18,733 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:53:18,733 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:53:21,379 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:21,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:24,379 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:24,380 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:53:26,986 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:53:26 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 11
[INFO] 2023-11-08 14:53:26,986 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:53:26 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 11
[INFO] 2023-11-08 14:53:26,986 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:53:26 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 11
[DEBUG] 2023-11-08 14:53:27,380 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:27,381 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:30,380 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:30,381 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:53:30,993 [qtp1625817721-250] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 250]
[TRACE] 2023-11-08 14:53:30,995 [qtp1625817721-250] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 250]
[DEBUG] 2023-11-08 14:53:30,997 [qtp1625817721-250] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:53:30,998 [qtp1625817721-250] io.confluent.mds.request.logger log - 65 * Server has received a request on thread qtp1625817721-250null65 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull65 > User Principal: c3
65 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null65 > Cache-Control: no-cachenull65 > Connection: keep-alivenull65 > Content-Type: application/jsonnull65 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null65 > Pragma: no-cachenull65 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:53:31,002 [qtp1625817721-250] io.confluent.mds.request.logger log - 65 * Server responded with a response on thread qtp1625817721-250null65 < 200null65 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:53:31,003 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:53:30 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:53:31,003 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:53:30 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:53:31,003 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:53:30 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[DEBUG] 2023-11-08 14:53:33,380 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:33,381 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:53:34,937 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 188]
[TRACE] 2023-11-08 14:53:34,938 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 188]
[DEBUG] 2023-11-08 14:53:34,940 [qtp1625817721-188] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:53:34,941 [qtp1625817721-188] io.confluent.mds.request.logger log - 66 * Server has received a request on thread qtp1625817721-188null66 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull66 > User Principal: c3
66 > authorization: Basic YzM6YzMtc2VjcmV0null66 > host: kafka.confluent.svc.cluster.local:8090null66 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:53:34,942 [qtp1625817721-188] io.confluent.mds.request.logger log - 66 * Server responded with a response on thread qtp1625817721-188null66 < 200null66 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:53:34,943 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:53:34 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:53:34,943 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:53:34 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:53:34,943 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:53:34 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[DEBUG] 2023-11-08 14:53:36,380 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:36,381 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:39,380 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:39,381 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:42,381 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:42,381 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:53:44,814 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:53:44 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[INFO] 2023-11-08 14:53:44,814 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:53:44 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[INFO] 2023-11-08 14:53:44,814 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:53:44 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[DEBUG] 2023-11-08 14:53:45,382 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:45,382 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:53:46,307 [pool-18-thread-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - all topics exist
[INFO] 2023-11-08 14:53:46,307 [pool-18-thread-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - Event logger has metadata for all topics
[DEBUG] 2023-11-08 14:53:48,382 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:48,383 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:53:48,751 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:53:48,752 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:53:48,752 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:53:48,766 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455228767
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,770 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,771 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,772 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,772 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,772 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,772 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,772 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,772 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:53:48,772 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:53:48,772 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:53:48,772 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455228772
[INFO] 2023-11-08 14:53:48,781 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:53:48,798 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:53:48,798 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:53:48,799 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:53:48,799 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:53:48,799 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:53:48,799 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:53:48,799 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:53:48,800 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:53:48,800 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:53:48,800 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:53:48,800 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:53:48,800 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:53:48,800 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:53:51,383 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:51,383 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:54,382 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:54,383 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:53:56,751 [qtp1625817721-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 192]
[TRACE] 2023-11-08 14:53:56,753 [qtp1625817721-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 192]
[DEBUG] 2023-11-08 14:53:56,773 [qtp1625817721-192] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:53:56,775 [qtp1625817721-192] io.confluent.mds.request.logger log - 67 * Server has received a request on thread qtp1625817721-192null67 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull67 > User Principal: c3
67 > authorization: Basic YzM6YzMtc2VjcmV0null67 > host: kafka.confluent.svc.cluster.local:8090null67 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:53:56,775 [qtp1625817721-192] io.confluent.mds.request.logger log - 67 * Server responded with a response on thread qtp1625817721-192null67 < 200null67 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:53:56,777 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:53:56 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 27
[INFO] 2023-11-08 14:53:56,777 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:53:56 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 27
[INFO] 2023-11-08 14:53:56,777 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:53:56 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 27
[DEBUG] 2023-11-08 14:53:57,382 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:57,383 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:00,383 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:00,383 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:03,383 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:03,384 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:54:06,119 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:06 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:54:06,119 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:06 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:54:06,119 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:06 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[DEBUG] 2023-11-08 14:54:06,383 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:06,384 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:54:08,632 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[TRACE] 2023-11-08 14:54:08,634 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 179]
[DEBUG] 2023-11-08 14:54:08,636 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:54:08,637 [qtp1625817721-179] io.confluent.mds.request.logger log - 68 * Server has received a request on thread qtp1625817721-179null68 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull68 > User Principal: c3
68 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null68 > Cache-Control: no-cachenull68 > Connection: keep-alivenull68 > Content-Type: application/jsonnull68 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null68 > Pragma: no-cachenull68 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:54:08,641 [qtp1625817721-179] io.confluent.mds.request.logger log - 68 * Server responded with a response on thread qtp1625817721-179null68 < 200null68 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:54:08,642 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:54:08 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:54:08,643 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:54:08 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:54:08,643 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:54:08 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 12
[DEBUG] 2023-11-08 14:54:09,383 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:09,384 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:54:10,184 [qtp1625817721-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 246]
[TRACE] 2023-11-08 14:54:10,186 [qtp1625817721-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 246]
[DEBUG] 2023-11-08 14:54:10,188 [qtp1625817721-246] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:54:10,189 [qtp1625817721-246] io.confluent.mds.request.logger log - 69 * Server has received a request on thread qtp1625817721-246null69 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull69 > User Principal: c3
69 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null69 > Cache-Control: no-cachenull69 > Connection: keep-alivenull69 > Content-Type: application/jsonnull69 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null69 > Pragma: no-cachenull69 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:54:10,193 [qtp1625817721-246] io.confluent.mds.request.logger log - 69 * Server responded with a response on thread qtp1625817721-246null69 < 200null69 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:54:10,194 [qtp1625817721-246] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:54:10 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:54:10,194 [qtp1625817721-246] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:54:10 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:54:10,194 [qtp1625817721-246] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:54:10 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[DEBUG] 2023-11-08 14:54:12,384 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:12,384 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:54:14,003 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 179]
[TRACE] 2023-11-08 14:54:14,005 [qtp1625817721-179] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 179]
[DEBUG] 2023-11-08 14:54:14,006 [qtp1625817721-179] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:54:14,008 [qtp1625817721-179] io.confluent.mds.request.logger log - 70 * Server has received a request on thread qtp1625817721-179null70 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull70 > User Principal: c3
70 > authorization: Basic YzM6YzMtc2VjcmV0null70 > host: kafka.confluent.svc.cluster.local:8090null70 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:54:14,008 [qtp1625817721-179] io.confluent.mds.request.logger log - 70 * Server responded with a response on thread qtp1625817721-179null70 < 200null70 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:54:14,010 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:54:14 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[INFO] 2023-11-08 14:54:14,010 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:54:14 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[INFO] 2023-11-08 14:54:14,010 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:54:14 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[DEBUG] 2023-11-08 14:54:15,384 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:15,385 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:15,963 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:5 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:6 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:6 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:8 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:9 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,967 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:7 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:18,384 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:18,385 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:54:18,810 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:54:18,810 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:54:18,811 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:54:18,823 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,824 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:54:18,824 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:54:18,824 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:54:18,824 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455258824
[WARN] 2023-11-08 14:54:18,826 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:54:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:54:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:54:18,828 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455258828
[INFO] 2023-11-08 14:54:18,837 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:54:18,850 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:54:18,850 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:54:18,851 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:54:18,851 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:54:18,851 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:54:18,851 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:54:18,852 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:54:18,852 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:54:18,852 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:54:18,852 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:54:18,852 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:54:18,852 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:54:18,852 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:54:21,384 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:21,385 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:24,384 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:24,385 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:54:27,242 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:27 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:54:27,242 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:27 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:54:27,242 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:27 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[DEBUG] 2023-11-08 14:54:27,384 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:27,385 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:30,384 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:30,385 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:54:30,753 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 188]
[TRACE] 2023-11-08 14:54:30,755 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 188]
[DEBUG] 2023-11-08 14:54:30,768 [qtp1625817721-188] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:54:30,769 [qtp1625817721-188] io.confluent.mds.request.logger log - 71 * Server has received a request on thread qtp1625817721-188null71 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull71 > User Principal: c3
71 > authorization: Basic YzM6YzMtc2VjcmV0null71 > host: kafka.confluent.svc.cluster.local:8090null71 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:54:30,770 [qtp1625817721-188] io.confluent.mds.request.logger log - 71 * Server responded with a response on thread qtp1625817721-188null71 < 200null71 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:54:30,771 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:54:30 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 18
[INFO] 2023-11-08 14:54:30,771 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:54:30 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 18
[INFO] 2023-11-08 14:54:30,771 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:54:30 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 18
[TRACE] 2023-11-08 14:54:31,377 [qtp1625817721-176] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 176]
[TRACE] 2023-11-08 14:54:31,379 [qtp1625817721-176] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 176]
[DEBUG] 2023-11-08 14:54:31,381 [qtp1625817721-176] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:54:31,382 [qtp1625817721-176] io.confluent.mds.request.logger log - 72 * Server has received a request on thread qtp1625817721-176null72 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull72 > User Principal: c3
72 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null72 > Cache-Control: no-cachenull72 > Connection: keep-alivenull72 > Content-Type: application/jsonnull72 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null72 > Pragma: no-cachenull72 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:54:31,386 [qtp1625817721-176] io.confluent.mds.request.logger log - 72 * Server responded with a response on thread qtp1625817721-176null72 < 200null72 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:54:31,387 [qtp1625817721-176] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:54:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:54:31,387 [qtp1625817721-176] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:54:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:54:31,387 [qtp1625817721-176] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:54:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:54:31,440 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:31 +0000] "GET /security/1.0/authenticate?renewer=kafka HTTP/1.1" 401 148 "https://localhost:9021/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36" 16
[INFO] 2023-11-08 14:54:31,441 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:31 +0000] "GET /security/1.0/authenticate?renewer=kafka HTTP/1.1" 401 148 "https://localhost:9021/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36" 17
[INFO] 2023-11-08 14:54:31,441 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:31 +0000] "GET /security/1.0/authenticate?renewer=kafka HTTP/1.1" 401 148 "https://localhost:9021/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36" 17
[DEBUG] 2023-11-08 14:54:33,385 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:33,386 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:36,385 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:36,386 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:39,386 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:39,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:42,386 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:42,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:45,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:45,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:48,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:48,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:54:48,865 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:54:48,865 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:54:48,866 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:54:48,880 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,880 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:54:48,881 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:54:48,881 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:54:48,881 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455288880
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,883 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:54:48,884 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455288884
[INFO] 2023-11-08 14:54:48,893 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:54:48,916 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:54:48,917 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:54:48,917 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:54:48,917 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:54:48,917 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:54:48,917 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:54:48,918 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:54:48,918 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:54:48,919 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:54:48,919 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:54:48,919 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:54:48,919 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:54:48,919 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:54:51,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:51,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:54,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:54,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:55,477 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:7 key RoleBindingKey{principal=User:testadmin, role='ClusterAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[]) oldValue null
[TRACE] 2023-11-08 14:54:55,553 [qtp1625817721-243] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 243]
[TRACE] 2023-11-08 14:54:55,555 [qtp1625817721-243] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 243]
[DEBUG] 2023-11-08 14:54:55,557 [qtp1625817721-243] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[DEBUG] 2023-11-08 14:54:55,558 [qtp1625817721-243] io.confluent.rbacapi.rest.MdsWriterProxyServlet extractProxyUri - Forwarding request to leader https://kafka-0.kafka.confluent.svc.cluster.local:8090
[DEBUG] 2023-11-08 14:54:55,558 [qtp1625817721-243] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 service - 1767038220 rewriting: https://kafka.confluent.svc.cluster.local:8090/security/leader/security/1.0/principals/User:testadmin/roles/SystemAdmin -> https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:testadmin/roles/SystemAdmin
[DEBUG] 2023-11-08 14:54:55,558 [qtp1625817721-243] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 sendProxyRequest - 1767038220 proxying to upstream:
POST /security/leader/security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Host: kafka.confluent.svc.cluster.local:8090
Content-Length: 112
Content-Type: application/json

HttpRequest[POST /security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1]@65659461
Authorization: Basic a2Fma2E6a2Fma2Etc2VjcmV0
Accept: application/json
User-Agent: confluent-operator
Content-Length: 112
Content-Type: application/json
Via: 1.1 kafka-2
X-Forwarded-For: 10.40.2.7
X-Forwarded-Proto: https
X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090
X-Forwarded-Server: 10.40.0.15
[DEBUG] 2023-11-08 14:54:55,559 [qtp1625817721-243] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 1767038220 asynchronous read 112 bytes on HttpInputOverHTTP@386f5dab[c=112,q=0,[0]=null,s=ASYNC]
[DEBUG] 2023-11-08 14:54:55,559 [qtp1625817721-243] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 1767038220 proxying content to upstream: 112 bytes
[DEBUG] 2023-11-08 14:54:55,569 [qtp1625817721-247] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 1767038220 asynchronous read -1 bytes on HttpInputOverHTTP@386f5dab[c=112,q=0,[0]=null,s=AEOF]
[DEBUG] 2023-11-08 14:54:55,569 [qtp1625817721-246] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onAllDataRead - 1767038220 proxying content to upstream completed
[DEBUG] 2023-11-08 14:54:55,569 [qtp1625817721-247] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 process - 1767038220 asynchronous read complete on HttpInputOverHTTP@386f5dab[c=112,q=0,[0]=null,s=EOF]
[DEBUG] 2023-11-08 14:54:55,581 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:10 key RoleBindingKey{principal=User:testadmin, role='SystemAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, schema-registry-cluster=id_schemaregistry_confluent}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:54:55,582 [qtp1625817721-237] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onServerResponseHeaders - 1767038220 proxying to downstream:
HttpResponse[HTTP/1.1 204 No Content]@5ef8c4f1
Date: Wed, 08 Nov 2023 14:54:55 GMT

[DEBUG] 2023-11-08 14:54:55,582 [qtp1625817721-237] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onProxyResponseSuccess - 1767038220 proxying successful
[INFO] 2023-11-08 14:54:55,583 [qtp1625817721-237] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 30
[INFO] 2023-11-08 14:54:55,583 [qtp1625817721-237] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 30
[INFO] 2023-11-08 14:54:55,583 [qtp1625817721-237] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 30
[DEBUG] 2023-11-08 14:54:55,583 [qtp1625817721-237] io.confluent.rbacapi.rest.MdsWriterProxyServlet.b056ac5 onComplete - 1767038220 proxying complete
[DEBUG] 2023-11-08 14:54:55,669 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:8 key RoleBindingKey{principal=User:testadmin, role='SystemAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, connect-cluster=confluent.connect}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:54:55,809 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:9 key RoleBindingKey{principal=User:testadmin, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, ksql-cluster=confluent.ksqldb_}')'} newValue RoleBindingValue(resources=[KsqlCluster:LITERAL:ksql-cluster]) oldValue null
[DEBUG] 2023-11-08 14:54:57,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:57,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:00,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:00,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:03,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:03,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:06,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:06,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:09,389 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:09,389 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:12,390 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:12,390 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:15,390 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:15,391 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:15,965 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:6 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:10 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:7 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,967 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:9 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,968 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,968 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:11 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,968 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,968 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:8 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:18,390 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:18,391 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:55:18,922 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:55:18,923 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:55:18,923 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:55:18,938 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,940 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:55:18,940 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:55:18,940 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:55:18,940 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455318940
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,942 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,943 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:55:18,944 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455318944
[INFO] 2023-11-08 14:55:18,953 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:55:18,966 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:55:18,967 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:55:18,967 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:55:18,967 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:55:18,967 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:55:18,967 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:55:18,968 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:55:18,968 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:55:18,968 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:55:18,968 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:55:18,968 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:55:18,968 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:55:18,968 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:55:21,390 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:21,391 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:24,410 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:24,411 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:55:24,420 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 188]
[TRACE] 2023-11-08 14:55:24,422 [qtp1625817721-188] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 188]
[DEBUG] 2023-11-08 14:55:24,435 [qtp1625817721-188] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:55:24,436 [qtp1625817721-188] io.confluent.mds.request.logger log - 73 * Server has received a request on thread qtp1625817721-188null73 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull73 > User Principal: c3
73 > authorization: Basic YzM6YzMtc2VjcmV0null73 > host: kafka.confluent.svc.cluster.local:8090null73 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:55:24,437 [qtp1625817721-188] io.confluent.mds.request.logger log - 73 * Server responded with a response on thread qtp1625817721-188null73 < 200null73 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:55:24,438 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:24 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 18
[INFO] 2023-11-08 14:55:24,439 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:24 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 19
[INFO] 2023-11-08 14:55:24,439 [qtp1625817721-188] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:24 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 19
[DEBUG] 2023-11-08 14:55:27,410 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:27,411 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:55:28,506 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:28 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:55:28,507 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:28 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[INFO] 2023-11-08 14:55:28,507 [qtp1625817721-181] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:28 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[DEBUG] 2023-11-08 14:55:30,410 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:30,411 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:33,410 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:33,411 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:55:34,818 [qtp1625817721-239] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 239]
[TRACE] 2023-11-08 14:55:34,819 [qtp1625817721-239] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 239]
[DEBUG] 2023-11-08 14:55:34,821 [qtp1625817721-239] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:55:34,822 [qtp1625817721-239] io.confluent.mds.request.logger log - 74 * Server has received a request on thread qtp1625817721-239null74 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull74 > User Principal: kafka
74 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null74 > Cache-Control: no-cachenull74 > Connection: keep-alivenull74 > Content-Type: application/jsonnull74 > Host: kafka.confluent.svc.cluster.local:8090null74 > Pragma: no-cachenull74 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:34,825 [qtp1625817721-239] io.confluent.mds.request.logger log - 74 * Server responded with a response on thread qtp1625817721-239null74 < 200null74 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:55:34,826 [qtp1625817721-239] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 639 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:55:34,827 [qtp1625817721-239] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 639 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:55:34,827 [qtp1625817721-239] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 639 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:55:34,849 [qtp1625817721-244] io.confluent.kafkarest.security.config.ConfluentSecureKafkaRestConfig logAll - ConfluentSecureKafkaRestConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	advertised.listeners = []
	api.endpoints.allowlist = []
	api.endpoints.blocklist = []
	api.v2.enable = false
	api.v3.enable = true
	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
	api.v3.produce.rate.limit.enabled = false
	api.v3.produce.rate.limit.grace.period.ms = 30000
	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
	api.v3.produce.rate.limit.max.requests.per.sec = 10000
	api.v3.produce.response.thread.pool.size = 16
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	bootstrap.servers = kafka.confluent.svc.cluster.local:9073
	client.init.timeout.ms = 60000
	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	client.sasl.kerberos.min.time.before.relogin = 60000
	client.sasl.kerberos.service.name = 
	client.sasl.kerberos.ticket.renew.jitter = 0.05
	client.sasl.kerberos.ticket.renew.window.factor = 0.8
	client.sasl.mechanism = OAUTHBEARER
	client.security.protocol = SASL_SSL
	client.ssl.cipher.suites = 
	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
	client.ssl.endpoint.identification.algorithm = 
	client.ssl.key.password = [hidden]
	client.ssl.keymanager.algorithm = SunX509
	client.ssl.keystore.location = 
	client.ssl.keystore.password = [hidden]
	client.ssl.keystore.type = JKS
	client.ssl.protocol = TLS
	client.ssl.provider = 
	client.ssl.trustmanager.algorithm = PKIX
	client.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	client.ssl.truststore.password = [hidden]
	client.ssl.truststore.type = JKS
	client.timeout.ms = 500
	client.zk.session.timeout.ms = 30000
	compression.enable = true
	confluent.license = 
	confluent.license.topic = _confluent-license
	confluent.metadata.bootstrap.server.urls = https://kafka.confluent.svc.cluster.local:8090
	confluent.resource.name.authority = 
	confluent.rest.auth.propagate.method = SSL
	confluent.rest.auth.ssl.principal.mapping.rules = DEFAULT
	consumer.instance.timeout.ms = 300000
	consumer.iterator.backoff.ms = 50
	consumer.iterator.timeout.ms = 1
	consumer.request.max.bytes = 67108864
	consumer.request.timeout.ms = 1000
	consumer.threads = 50
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	fetch.min.bytes = -1
	host.name = 
	http2.enabled = true
	id = 
	idle.timeout.ms = 30000
	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension, io.confluent.kafkarest.security.KafkaRestSecurityResourceExtension]
	listener.protocol.map = []
	listeners = []
	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.jmx.prefix = kafka.rest
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8082
	producer.threads = 5
	proxy.protocol.enabled = false
	rate.limit.backend = guava
	rate.limit.costs = 
	rate.limit.default.cost = 1
	rate.limit.enable = false
	rate.limit.permits.per.sec = 50
	rate.limit.timeout.ms = 0
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
	rest.servlet.initializor.classes = [io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler]
	schema.registry.url = http://localhost:8081
	shutdown.graceful.ms = 1000
	simpleconsumer.pool.size.max = 25
	simpleconsumer.pool.timeout.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []
	zookeeper.connect = 

[INFO] 2023-11-08 14:55:34,861 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9073]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = class io.confluent.kafka.clients.plugins.auth.token.TokenBearerLoginCallbackHandler
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = OAUTHBEARER
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:55:34,862 [qtp1625817721-244] io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig logAll - EventEmitterConfig values: 

[INFO] 2023-11-08 14:55:34,862 [qtp1625817721-244] io.confluent.telemetry.ConfluentTelemetryConfig logAll - ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

[INFO] 2023-11-08 14:55:34,863 [qtp1625817721-244] io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig logAll - VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

[INFO] 2023-11-08 14:55:34,863 [qtp1625817721-244] io.confluent.telemetry.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[WARN] 2023-11-08 14:55:34,863 [qtp1625817721-244] io.confluent.telemetry.ConfluentTelemetryConfig <init> - no telemetry exporters are enabled
[WARN] 2023-11-08 14:55:34,863 [qtp1625817721-244] io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade withLabel - Ignoring redefinition of existing telemetry label kafka_rest.version
[INFO] 2023-11-08 14:55:34,864 [qtp1625817721-244] io.confluent.telemetry.ConfluentTelemetryConfig logAll - ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

[INFO] 2023-11-08 14:55:34,864 [qtp1625817721-244] io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig logAll - VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

[INFO] 2023-11-08 14:55:34,864 [qtp1625817721-244] io.confluent.telemetry.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[WARN] 2023-11-08 14:55:34,865 [qtp1625817721-244] io.confluent.telemetry.ConfluentTelemetryConfig <init> - no telemetry exporters are enabled
[INFO] 2023-11-08 14:55:34,865 [qtp1625817721-244] io.confluent.telemetry.reporter.TelemetryReporter initEventLogger - Initializing the event logger
[INFO] 2023-11-08 14:55:34,865 [qtp1625817721-244] io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig logAll - EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter

[INFO] 2023-11-08 14:55:34,865 [qtp1625817721-244] io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[INFO] 2023-11-08 14:55:34,868 [qtp1625817721-244] io.confluent.telemetry.reporter.TelemetryReporter startMetricCollectorTask - Starting Confluent telemetry reporter with an interval of 60000 ms for resource = (type = kafka_rest)
[INFO] 2023-11-08 14:55:34,871 [qtp1625817721-244] io.confluent.security.auth.client.RestClientConfig logAll - RestClientConfig values: 
	confluent.metadata.basic.auth.credentials.path = null
	confluent.metadata.basic.auth.credentials.provider = USER_INFO
	confluent.metadata.basic.auth.user.info = [hidden]
	confluent.metadata.bootstrap.server.urls = [https://kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.enable.server.urls.refresh = true
	confluent.metadata.http.auth.credentials.provider = BEARER
	confluent.metadata.http.request.timeout.ms = 10000
	confluent.metadata.request.timeout.ms = 30000
	confluent.metadata.server.urls.fail.on.401 = false
	confluent.metadata.server.urls.max.age.ms = 600000
	confluent.metadata.server.urls.max.retries = 5
	confluent.metadata.token.auth.credential = [hidden]

[INFO] 2023-11-08 14:55:34,873 [qtp1625817721-244] io.confluent.security.auth.client.RestClientConfig$SslClientConfig logAll - SslClientConfig values: 
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[DEBUG] 2023-11-08 14:55:34,884 [qtp1625817721-242] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:34,885 [qtp1625817721-242] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@672854df
[DEBUG] 2023-11-08 14:55:34,886 [qtp1625817721-243] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:34,886 [qtp1625817721-243] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@48ab4ba3
[INFO] 2023-11-08 14:55:34,886 [qtp1625817721-242] io.confluent.mds.request.logger log - 75 * Server has received a request on thread qtp1625817721-242null75 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull75 > User Principal: kafka
75 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null75 > Cache-Control: no-cachenull75 > Connection: keep-alivenull75 > Content-Type: application/jsonnull75 > Host: kafka.confluent.svc.cluster.local:8090null75 > Pragma: no-cachenull75 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:34,887 [qtp1625817721-243] io.confluent.mds.request.logger log - 76 * Server has received a request on thread qtp1625817721-243null76 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull76 > User Principal: kafka
76 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null76 > Cache-Control: no-cachenull76 > Connection: keep-alivenull76 > Content-Type: application/jsonnull76 > Host: kafka.confluent.svc.cluster.local:8090null76 > Pragma: no-cachenull76 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:34,887 [qtp1625817721-242] io.confluent.mds.request.logger log - 75 * Server responded with a response on thread qtp1625817721-242null75 < 200null75 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[DEBUG] 2023-11-08 14:55:34,888 [pool-44-thread-1] io.confluent.security.auth.client.rest.RestClient run - Successfully fetched MDS URLs ([https://kafka-1.kafka.confluent.svc.cluster.local:8090, https://kafka-2.kafka.confluent.svc.cluster.local:8090, https://kafka-0.kafka.confluent.svc.cluster.local:8090, https://kafka.confluent.svc.cluster.local:8090])
[INFO] 2023-11-08 14:55:34,888 [qtp1625817721-242] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 5
[INFO] 2023-11-08 14:55:34,888 [qtp1625817721-242] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 5
[INFO] 2023-11-08 14:55:34,888 [qtp1625817721-242] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 5
[INFO] 2023-11-08 14:55:34,889 [qtp1625817721-243] io.confluent.mds.request.logger log - 76 * Server responded with a response on thread qtp1625817721-243null76 < 200null76 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:55:34,890 [qtp1625817721-243] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 639 "-" "Java/11.0.14.1" 5
[INFO] 2023-11-08 14:55:34,891 [qtp1625817721-243] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 639 "-" "Java/11.0.14.1" 6
[INFO] 2023-11-08 14:55:34,891 [qtp1625817721-243] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 639 "-" "Java/11.0.14.1" 6
[INFO] 2023-11-08 14:55:34,891 [qtp1625817721-244] io.confluent.security.auth.client.RestClientConfig logAll - RestClientConfig values: 
	confluent.metadata.basic.auth.credentials.path = null
	confluent.metadata.basic.auth.credentials.provider = USER_INFO
	confluent.metadata.basic.auth.user.info = [hidden]
	confluent.metadata.bootstrap.server.urls = [https://kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.enable.server.urls.refresh = true
	confluent.metadata.http.auth.credentials.provider = BEARER
	confluent.metadata.http.request.timeout.ms = 10000
	confluent.metadata.request.timeout.ms = 30000
	confluent.metadata.server.urls.fail.on.401 = false
	confluent.metadata.server.urls.max.age.ms = 600000
	confluent.metadata.server.urls.max.retries = 5
	confluent.metadata.token.auth.credential = [hidden]

[INFO] 2023-11-08 14:55:34,892 [qtp1625817721-244] io.confluent.security.auth.client.RestClientConfig$SslClientConfig logAll - SslClientConfig values: 
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:55:34,893 [qtp1625817721-244] org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin login - Successfully logged in.
[INFO] 2023-11-08 14:55:34,895 [kafka-expiring-relogin-thread-kafka] org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin run - [Principal=:kafka]: Expiring credential re-login thread started.
[INFO] 2023-11-08 14:55:34,895 [kafka-expiring-relogin-thread-kafka] org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin refreshMs - [Principal=kafka]: Expiring credential valid from Wed Nov 08 14:55:34 GMT 2023 to Wed Nov 08 15:55:34 GMT 2023
[INFO] 2023-11-08 14:55:34,895 [kafka-expiring-relogin-thread-kafka] org.apache.kafka.common.security.oauthbearer.internals.expiring.ExpiringCredentialRefreshingLogin run - [Principal=:kafka]: Expiring credential re-login sleeping until: Wed Nov 08 15:44:38 GMT 2023
[WARN] 2023-11-08 14:55:34,896 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:34,896 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:34,896 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:34,896 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:34,896 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:34,896 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:34,896 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:34,896 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:34,896 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:34,897 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'bearer.auth.credentials.source' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:34,897 [qtp1625817721-244] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[INFO] 2023-11-08 14:55:34,897 [qtp1625817721-244] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:55:34,897 [qtp1625817721-244] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:55:34,897 [qtp1625817721-244] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455334897
[DEBUG] 2023-11-08 14:55:34,907 [qtp1625817721-157] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:34,908 [qtp1625817721-157] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@25720469
[INFO] 2023-11-08 14:55:34,909 [qtp1625817721-157] io.confluent.mds.request.logger log - 77 * Server has received a request on thread qtp1625817721-157null77 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull77 > User Principal: kafka
77 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null77 > Cache-Control: no-cachenull77 > Connection: keep-alivenull77 > Content-Type: application/jsonnull77 > Host: kafka.confluent.svc.cluster.local:8090null77 > Pragma: no-cachenull77 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:34,910 [qtp1625817721-157] io.confluent.mds.request.logger log - 77 * Server responded with a response on thread qtp1625817721-157null77 < 200null77 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[DEBUG] 2023-11-08 14:55:34,911 [pool-46-thread-1] io.confluent.security.auth.client.rest.RestClient run - Successfully fetched MDS URLs ([https://kafka-1.kafka.confluent.svc.cluster.local:8090, https://kafka-2.kafka.confluent.svc.cluster.local:8090, https://kafka-0.kafka.confluent.svc.cluster.local:8090, https://kafka.confluent.svc.cluster.local:8090])
[INFO] 2023-11-08 14:55:34,912 [qtp1625817721-157] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 5
[INFO] 2023-11-08 14:55:34,912 [qtp1625817721-157] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 5
[INFO] 2023-11-08 14:55:34,912 [qtp1625817721-157] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 5
[INFO] 2023-11-08 14:55:35,000 [kafka-admin-client-thread | adminclient-5] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:55:34 +0000] "GET /kafka/v3/clusters HTTP/1.1" 200 1271 "-" "confluent-operator" 192
[INFO] 2023-11-08 14:55:35,000 [kafka-admin-client-thread | adminclient-5] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:55:34 +0000] "GET /kafka/v3/clusters HTTP/1.1" 200 1271 "-" "confluent-operator" 192
[INFO] 2023-11-08 14:55:35,001 [kafka-admin-client-thread | adminclient-5] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:55:34 +0000] "GET /kafka/v3/clusters HTTP/1.1" 200 1271 "-" "confluent-operator" 193
[DEBUG] 2023-11-08 14:55:36,410 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:36,411 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:39,410 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:39,411 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:42,410 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:42,411 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:55:43,115 [qtp1625817721-175] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 175]
[TRACE] 2023-11-08 14:55:43,117 [qtp1625817721-175] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 175]
[DEBUG] 2023-11-08 14:55:43,119 [qtp1625817721-175] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:55:43,120 [qtp1625817721-175] io.confluent.mds.request.logger log - 78 * Server has received a request on thread qtp1625817721-175null78 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull78 > User Principal: c3
78 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null78 > Cache-Control: no-cachenull78 > Connection: keep-alivenull78 > Content-Type: application/jsonnull78 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null78 > Pragma: no-cachenull78 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:43,123 [qtp1625817721-175] io.confluent.mds.request.logger log - 78 * Server responded with a response on thread qtp1625817721-175null78 < 200null78 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:55:43,124 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:55:43 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:55:43,124 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:55:43 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:55:43,124 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:55:43 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[TRACE] 2023-11-08 14:55:43,721 [qtp1625817721-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 192]
[TRACE] 2023-11-08 14:55:43,722 [qtp1625817721-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 192]
[DEBUG] 2023-11-08 14:55:43,724 [qtp1625817721-192] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:55:43,725 [qtp1625817721-192] io.confluent.mds.request.logger log - 79 * Server has received a request on thread qtp1625817721-192null79 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull79 > User Principal: c3
79 > authorization: Basic YzM6YzMtc2VjcmV0null79 > host: kafka.confluent.svc.cluster.local:8090null79 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:55:43,726 [qtp1625817721-192] io.confluent.mds.request.logger log - 79 * Server responded with a response on thread qtp1625817721-192null79 < 200null79 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:55:43,727 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:43 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:55:43,727 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:43 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:55:43,728 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:43 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[DEBUG] 2023-11-08 14:55:45,411 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:45,411 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:55:46,223 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:46 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[INFO] 2023-11-08 14:55:46,223 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:46 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[INFO] 2023-11-08 14:55:46,223 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:46 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[DEBUG] 2023-11-08 14:55:48,412 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:48,412 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:55:49,004 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:55:49,005 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:55:49,006 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:55:49,020 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,021 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:55:49,021 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:55:49,021 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:55:49,021 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455349021
[WARN] 2023-11-08 14:55:49,023 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,023 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,023 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,023 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,023 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,023 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,023 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,023 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,023 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,023 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,024 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,025 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:55:49,025 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:55:49,025 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:55:49,025 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455349025
[INFO] 2023-11-08 14:55:49,033 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:55:49,050 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:55:49,051 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:55:49,052 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:55:49,052 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:55:49,052 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:55:49,052 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:55:49,053 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:55:49,053 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:55:49,053 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:55:49,053 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:55:49,053 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:55:49,053 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:55:49,053 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:55:51,413 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:51,413 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:54,414 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:54,414 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:57,413 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:57,414 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:55:59,541 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 200]
[TRACE] 2023-11-08 14:55:59,543 [qtp1625817721-200] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 200]
[DEBUG] 2023-11-08 14:55:59,564 [qtp1625817721-200] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:55:59,565 [qtp1625817721-200] io.confluent.mds.request.logger log - 80 * Server has received a request on thread qtp1625817721-200null80 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull80 > User Principal: c3
80 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null80 > Cache-Control: no-cachenull80 > Connection: keep-alivenull80 > Content-Type: application/jsonnull80 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null80 > Pragma: no-cachenull80 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:59,569 [qtp1625817721-200] io.confluent.mds.request.logger log - 80 * Server responded with a response on thread qtp1625817721-200null80 < 200null80 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:55:59,570 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:55:59 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 30
[INFO] 2023-11-08 14:55:59,570 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:55:59 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 30
[INFO] 2023-11-08 14:55:59,570 [qtp1625817721-200] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:55:59 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 30
[DEBUG] 2023-11-08 14:56:00,413 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:00,414 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:56:02,450 [qtp1625817721-237] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:56:02 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:56:02,450 [qtp1625817721-237] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:56:02 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:56:02,450 [qtp1625817721-237] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:56:02 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[DEBUG] 2023-11-08 14:56:03,414 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:03,414 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:56:06,414 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:06,415 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:56:06,920 [qtp1625817721-249] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 249]
[TRACE] 2023-11-08 14:56:06,922 [qtp1625817721-249] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 249]
[DEBUG] 2023-11-08 14:56:06,923 [qtp1625817721-249] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:56:06,924 [qtp1625817721-249] io.confluent.mds.request.logger log - 81 * Server has received a request on thread qtp1625817721-249null81 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull81 > User Principal: c3
81 > authorization: Basic YzM6YzMtc2VjcmV0null81 > host: kafka.confluent.svc.cluster.local:8090null81 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:56:06,925 [qtp1625817721-249] io.confluent.mds.request.logger log - 81 * Server responded with a response on thread qtp1625817721-249null81 < 200null81 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:56:06,926 [qtp1625817721-249] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:56:06 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:56:06,926 [qtp1625817721-249] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:56:06 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:56:06,926 [qtp1625817721-249] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:56:06 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[DEBUG] 2023-11-08 14:56:09,414 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:09,415 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:56:12,414 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:12,415 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:56:15,414 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:15,415 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:56:15,967 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:11 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,969 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,969 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:7 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,969 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,969 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:8 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,970 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,970 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:10 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,970 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,970 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:9 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,971 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:56:15,971 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:12 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[INFO] 2023-11-08 14:56:16,307 [pool-18-thread-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - all topics exist
[INFO] 2023-11-08 14:56:16,307 [pool-18-thread-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - Event logger has metadata for all topics
[INFO] 2023-11-08 14:56:16,321 [kafka-admin-client-thread | adminclient-1] org.apache.kafka.clients.NetworkClient handleDisconnections - [AdminClient clientId=adminclient-1] Node -1 disconnected.
[INFO] 2023-11-08 14:56:16,362 [kafka-admin-client-thread | _confluent-metadata-admin-2] org.apache.kafka.clients.NetworkClient handleDisconnections - [AdminClient clientId=_confluent-metadata-admin-2] Node -1 disconnected.
[INFO] 2023-11-08 14:56:16,376 [kafka-admin-client-thread | _confluent-metadata-admin-2] org.apache.kafka.clients.NetworkClient handleDisconnections - [AdminClient clientId=_confluent-metadata-admin-2] Node -1 disconnected.
[DEBUG] 2023-11-08 14:56:18,414 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:18,415 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:56:19,057 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:56:19,057 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:56:19,058 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:56:19,070 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,071 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:56:19,071 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:56:19,071 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:56:19,071 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455379071
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,073 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:56:19,074 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455379074
[INFO] 2023-11-08 14:56:19,084 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:56:19,097 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:56:19,098 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:56:19,098 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:56:19,098 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:56:19,098 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:56:19,098 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:56:19,099 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:56:19,099 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:56:19,099 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:56:19,099 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:56:19,099 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:56:19,099 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:56:19,099 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[INFO] 2023-11-08 14:56:20,228 [qtp1625817721-252] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:56:20 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 4
[INFO] 2023-11-08 14:56:20,228 [qtp1625817721-252] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:56:20 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 4
[INFO] 2023-11-08 14:56:20,228 [qtp1625817721-252] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:56:20 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 4
[DEBUG] 2023-11-08 14:56:21,415 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:21,415 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:56:24,415 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:24,416 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:56:27,415 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:27,416 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:56:29,360 [qtp1625817721-252] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 252]
[TRACE] 2023-11-08 14:56:29,361 [qtp1625817721-252] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 252]
[DEBUG] 2023-11-08 14:56:29,363 [qtp1625817721-252] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:56:29,365 [qtp1625817721-252] io.confluent.mds.request.logger log - 82 * Server has received a request on thread qtp1625817721-252null82 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull82 > User Principal: c3
82 > authorization: Basic YzM6YzMtc2VjcmV0null82 > host: kafka.confluent.svc.cluster.local:8090null82 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:56:29,365 [qtp1625817721-252] io.confluent.mds.request.logger log - 82 * Server responded with a response on thread qtp1625817721-252null82 < 200null82 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:56:29,367 [qtp1625817721-252] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:56:29 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[INFO] 2023-11-08 14:56:29,367 [qtp1625817721-252] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:56:29 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[INFO] 2023-11-08 14:56:29,367 [qtp1625817721-252] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:56:29 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[DEBUG] 2023-11-08 14:56:30,415 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:30,416 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:56:31,932 [qtp1625817721-250] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 250]
[TRACE] 2023-11-08 14:56:31,933 [qtp1625817721-250] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 250]
[DEBUG] 2023-11-08 14:56:31,950 [qtp1625817721-250] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:56:31,951 [qtp1625817721-250] io.confluent.mds.request.logger log - 83 * Server has received a request on thread qtp1625817721-250null83 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull83 > User Principal: c3
83 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null83 > Cache-Control: no-cachenull83 > Connection: keep-alivenull83 > Content-Type: application/jsonnull83 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null83 > Pragma: no-cachenull83 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:56:31,954 [qtp1625817721-250] io.confluent.mds.request.logger log - 83 * Server responded with a response on thread qtp1625817721-250null83 < 200null83 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:56:31,955 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:56:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 24
[INFO] 2023-11-08 14:56:31,956 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:56:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 25
[INFO] 2023-11-08 14:56:31,956 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:56:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 25
[DEBUG] 2023-11-08 14:56:33,415 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:33,416 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:56:36,415 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:36,416 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:56:39,415 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:39,416 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:56:40,396 [qtp1625817721-241] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 241]
[TRACE] 2023-11-08 14:56:40,398 [qtp1625817721-241] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 241]
[DEBUG] 2023-11-08 14:56:40,400 [qtp1625817721-241] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:56:40,401 [qtp1625817721-241] io.confluent.mds.request.logger log - 84 * Server has received a request on thread qtp1625817721-241null84 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull84 > User Principal: c3
84 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null84 > Cache-Control: no-cachenull84 > Connection: keep-alivenull84 > Content-Type: application/jsonnull84 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null84 > Pragma: no-cachenull84 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:56:40,404 [qtp1625817721-241] io.confluent.mds.request.logger log - 84 * Server responded with a response on thread qtp1625817721-241null84 < 200null84 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:56:40,405 [qtp1625817721-241] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:56:40 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:56:40,405 [qtp1625817721-241] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:56:40 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:56:40,405 [qtp1625817721-241] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:56:40 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[DEBUG] 2023-11-08 14:56:42,415 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:42,416 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:56:43,691 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:56:43 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:56:43,691 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:56:43 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:56:43,691 [qtp1625817721-180] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:56:43 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[DEBUG] 2023-11-08 14:56:45,416 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:45,416 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:56:47,624 [qtp1625817721-193] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 193]
[TRACE] 2023-11-08 14:56:47,626 [qtp1625817721-193] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 193]
[DEBUG] 2023-11-08 14:56:47,628 [qtp1625817721-193] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:56:47,629 [qtp1625817721-193] io.confluent.mds.request.logger log - 85 * Server has received a request on thread qtp1625817721-193null85 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull85 > User Principal: c3
85 > authorization: Basic YzM6YzMtc2VjcmV0null85 > host: kafka.confluent.svc.cluster.local:8090null85 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:56:47,630 [qtp1625817721-193] io.confluent.mds.request.logger log - 85 * Server responded with a response on thread qtp1625817721-193null85 < 200null85 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:56:47,631 [qtp1625817721-193] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:56:47 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:56:47,631 [qtp1625817721-193] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:56:47 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:56:47,631 [qtp1625817721-193] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:56:47 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[DEBUG] 2023-11-08 14:56:48,416 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:48,417 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:56:48,621 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:56:48,622 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:56:48,622 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:56:48,624 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,625 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:56:48,625 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:56:48,625 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:56:48,625 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455408625
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,627 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:56:48,628 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455408628
[INFO] 2023-11-08 14:56:48,639 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:56:48,650 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:56:48,651 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:56:48,651 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:56:48,651 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:56:48,651 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:56:48,651 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:56:48,652 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:56:48,652 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:56:48,652 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:56:48,652 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:56:48,652 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:56:48,652 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:56:48,652 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:56:51,417 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:51,417 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:56:54,417 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:54,418 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:56:57,417 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:56:57,418 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:00,417 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:00,417 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:57:01,955 [qtp1625817721-249] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:01 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:57:01,955 [qtp1625817721-249] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:01 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:57:01,955 [qtp1625817721-249] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:01 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[DEBUG] 2023-11-08 14:57:03,417 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:03,418 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:57:05,071 [qtp1625817721-179] io.confluent.mds.request.logger log - 86 * Server has received a request on thread qtp1625817721-179null86 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull86 > User Principal: 
86 > Accept-Encoding: gzipnull86 > Host: kafka.confluent.svc.cluster.local:8090null86 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:57:05,072 [qtp1625817721-192] io.confluent.mds.request.logger log - 88 * Server has received a request on thread qtp1625817721-192null88 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull88 > User Principal: 
88 > Accept-Encoding: gzipnull88 > Host: kafka.confluent.svc.cluster.local:8090null88 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:57:05,072 [qtp1625817721-249] io.confluent.mds.request.logger log - 87 * Server has received a request on thread qtp1625817721-249null87 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull87 > User Principal: 
87 > Accept-Encoding: gzipnull87 > Host: kafka.confluent.svc.cluster.local:8090null87 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:57:05,073 [qtp1625817721-157] io.confluent.mds.request.logger log - 89 * Server has received a request on thread qtp1625817721-157null89 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull89 > User Principal: 
89 > Accept-Encoding: gzipnull89 > Host: kafka.confluent.svc.cluster.local:8090null89 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:57:05,076 [qtp1625817721-179] io.confluent.mds.request.logger log - 86 * Server responded with a response on thread qtp1625817721-179null86 < 200null86 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:57:05,076 [qtp1625817721-249] io.confluent.mds.request.logger log - 87 * Server responded with a response on thread qtp1625817721-249null87 < 200null87 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:57:05,076 [qtp1625817721-192] io.confluent.mds.request.logger log - 88 * Server responded with a response on thread qtp1625817721-192null88 < 200null88 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:57:05,076 [qtp1625817721-157] io.confluent.mds.request.logger log - 89 * Server responded with a response on thread qtp1625817721-157null89 < 200null89 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:57:05,079 [qtp1625817721-249] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 8
[INFO] 2023-11-08 14:57:05,079 [qtp1625817721-157] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 6
[INFO] 2023-11-08 14:57:05,079 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 12
[INFO] 2023-11-08 14:57:05,079 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 8
[INFO] 2023-11-08 14:57:05,080 [qtp1625817721-250] io.confluent.mds.request.logger log - 90 * Server has received a request on thread qtp1625817721-250null90 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull90 > User Principal: 
90 > Accept-Encoding: gzipnull90 > Host: kafka.confluent.svc.cluster.local:8090null90 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:57:05,080 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 13
[INFO] 2023-11-08 14:57:05,080 [qtp1625817721-157] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 7
[INFO] 2023-11-08 14:57:05,079 [qtp1625817721-249] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 8
[INFO] 2023-11-08 14:57:05,080 [qtp1625817721-157] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 7
[INFO] 2023-11-08 14:57:05,080 [qtp1625817721-179] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 13
[INFO] 2023-11-08 14:57:05,080 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:57:05,081 [qtp1625817721-250] io.confluent.mds.request.logger log - 90 * Server responded with a response on thread qtp1625817721-250null90 < 200null90 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:57:05,080 [qtp1625817721-249] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:57:05,081 [qtp1625817721-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 10
[INFO] 2023-11-08 14:57:05,081 [qtp1625817721-175] io.confluent.mds.request.logger log - 91 * Server has received a request on thread qtp1625817721-175null91 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull91 > User Principal: 
91 > Accept-Encoding: gzipnull91 > Host: kafka.confluent.svc.cluster.local:8090null91 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:57:05,082 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:57:05,082 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:57:05,082 [qtp1625817721-250] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:57:05,082 [qtp1625817721-175] io.confluent.mds.request.logger log - 91 * Server responded with a response on thread qtp1625817721-175null91 < 200null91 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:57:05,083 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:57:05,083 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:57:05,083 [qtp1625817721-175] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:57:05 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[DEBUG] 2023-11-08 14:57:05,092 [qtp1625817721-244] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:57:05,092 [qtp1625817721-174] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:57:05,092 [qtp1625817721-195] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:57:05,092 [qtp1625817721-243] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:57:05,092 [qtp1625817721-244] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@69c5b1e3
[DEBUG] 2023-11-08 14:57:05,092 [qtp1625817721-252] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:57:05,092 [qtp1625817721-193] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:57:05,093 [qtp1625817721-243] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@7e611917
[DEBUG] 2023-11-08 14:57:05,093 [qtp1625817721-174] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@385f56b0
[DEBUG] 2023-11-08 14:57:05,093 [qtp1625817721-195] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@1980462d
[DEBUG] 2023-11-08 14:57:05,093 [qtp1625817721-193] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@2b7093f6
[DEBUG] 2023-11-08 14:57:05,093 [qtp1625817721-252] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@512f2956
[INFO] 2023-11-08 14:57:05,094 [qtp1625817721-243] io.confluent.mds.request.logger log - 93 * Server has received a request on thread qtp1625817721-243null93 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull93 > User Principal: testadmin
93 > Accept-Encoding: gzipnull93 > Connection: closenull93 > Content-Length: 115null93 > Content-Type: application/jsonnull93 > Host: kafka.confluent.svc.cluster.local:8090null93 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:57:05,094 [qtp1625817721-244] io.confluent.mds.request.logger log - 92 * Server has received a request on thread qtp1625817721-244null92 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull92 > User Principal: testadmin
92 > Accept-Encoding: gzipnull92 > Connection: closenull92 > Content-Length: 134null92 > Content-Type: application/jsonnull92 > Host: kafka.confluent.svc.cluster.local:8090null92 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":["confluent.ksqldb_"]}]

[INFO] 2023-11-08 14:57:05,094 [qtp1625817721-195] io.confluent.mds.request.logger log - 94 * Server has received a request on thread qtp1625817721-195null94 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull94 > User Principal: testadmin
94 > Accept-Encoding: gzipnull94 > Connection: closenull94 > Content-Length: 134null94 > Content-Type: application/jsonnull94 > Host: kafka.confluent.svc.cluster.local:8090null94 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":["confluent.connect"],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:57:05,094 [qtp1625817721-252] io.confluent.mds.request.logger log - 97 * Server has received a request on thread qtp1625817721-252null97 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull97 > User Principal: testadmin
97 > Accept-Encoding: gzipnull97 > Connection: closenull97 > Content-Length: 115null97 > Content-Type: application/jsonnull97 > Host: kafka.confluent.svc.cluster.local:8090null97 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:57:05,094 [qtp1625817721-193] io.confluent.mds.request.logger log - 96 * Server has received a request on thread qtp1625817721-193null96 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull96 > User Principal: testadmin
96 > Accept-Encoding: gzipnull96 > Connection: closenull96 > Content-Length: 144null96 > Content-Type: application/jsonnull96 > Host: kafka.confluent.svc.cluster.local:8090null96 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":["id_schemaregistry_confluent"],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:57:05,094 [qtp1625817721-174] io.confluent.mds.request.logger log - 95 * Server has received a request on thread qtp1625817721-174null95 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull95 > User Principal: testadmin
95 > Accept-Encoding: gzipnull95 > Connection: closenull95 > Content-Length: 115null95 > Content-Type: application/jsonnull95 > Host: kafka.confluent.svc.cluster.local:8090null95 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:57:05,137 [qtp1625817721-252] io.confluent.mds.request.logger log - 97 * Server responded with a response on thread qtp1625817721-252null97 < 200null97 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:57:05,137 [qtp1625817721-244] io.confluent.mds.request.logger log - 92 * Server responded with a response on thread qtp1625817721-244null92 < 200null92 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[{"id":"confluent.ksqldb_","visible":true,"clusterName":null}]}]

[INFO] 2023-11-08 14:57:05,137 [qtp1625817721-193] io.confluent.mds.request.logger log - 96 * Server responded with a response on thread qtp1625817721-193null96 < 200null96 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[{"id":"id_schemaregistry_confluent","visible":true,"clusterName":null}],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:57:05,137 [qtp1625817721-195] io.confluent.mds.request.logger log - 94 * Server responded with a response on thread qtp1625817721-195null94 < 200null94 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[{"id":"confluent.connect","visible":true,"clusterName":null}],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:57:05,137 [qtp1625817721-243] io.confluent.mds.request.logger log - 93 * Server responded with a response on thread qtp1625817721-243null93 < 200null93 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:57:05,137 [qtp1625817721-174] io.confluent.mds.request.logger log - 95 * Server responded with a response on thread qtp1625817721-174null95 < 200null95 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:57:05,138 [qtp1625817721-252] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 48
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-195] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 156 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,138 [qtp1625817721-193] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 161 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,138 [qtp1625817721-244] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 159 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-193] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 161 "-" "Jetty/9.4.44.v20210927" 50
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-195] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 156 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-243] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-252] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-243] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-252] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-195] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 156 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-193] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 161 "-" "Jetty/9.4.44.v20210927" 50
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-244] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 159 "-" "Jetty/9.4.44.v20210927" 50
[INFO] 2023-11-08 14:57:05,140 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 50
[INFO] 2023-11-08 14:57:05,139 [qtp1625817721-243] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 49
[INFO] 2023-11-08 14:57:05,140 [qtp1625817721-174] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 50
[INFO] 2023-11-08 14:57:05,140 [qtp1625817721-244] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:57:05 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 159 "-" "Jetty/9.4.44.v20210927" 51
[DEBUG] 2023-11-08 14:57:06,417 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:06,418 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:57:06,820 [qtp1625817721-157] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 157]
[TRACE] 2023-11-08 14:57:06,822 [qtp1625817721-157] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 157]
[DEBUG] 2023-11-08 14:57:06,836 [qtp1625817721-157] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:57:06,837 [qtp1625817721-157] io.confluent.mds.request.logger log - 98 * Server has received a request on thread qtp1625817721-157null98 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull98 > User Principal: c3
98 > authorization: Basic YzM6YzMtc2VjcmV0null98 > host: kafka.confluent.svc.cluster.local:8090null98 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:57:06,838 [qtp1625817721-157] io.confluent.mds.request.logger log - 98 * Server responded with a response on thread qtp1625817721-157null98 < 200null98 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:57:06,839 [qtp1625817721-157] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:57:06 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 20
[INFO] 2023-11-08 14:57:06,840 [qtp1625817721-157] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:57:06 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 21
[INFO] 2023-11-08 14:57:06,840 [qtp1625817721-157] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:57:06 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 21
[DEBUG] 2023-11-08 14:57:09,417 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:09,418 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:12,418 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:12,418 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:15,418 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:15,419 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:15,968 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,968 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:8 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,969 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,969 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:12 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,969 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,969 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:9 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,971 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,971 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:11 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,971 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,971 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:13 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,971 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:15,972 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:10 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:57:18,419 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:18,419 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:57:18,687 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:57:18,688 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:57:18,688 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:57:18,703 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,703 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:57:18,704 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:57:18,704 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:57:18,704 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455438704
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,706 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:57:18,707 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455438707
[INFO] 2023-11-08 14:57:18,717 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:57:18,728 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:57:18,728 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:57:18,728 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:57:18,728 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:57:18,728 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:57:18,728 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:57:18,729 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:57:18,729 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:57:18,729 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:57:18,729 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:57:18,729 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:57:18,729 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:57:18,730 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:57:21,419 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:21,420 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:24,420 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:24,420 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:27,421 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:27,422 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:30,422 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:30,422 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:33,423 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:33,423 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:57:34,784 [qtp1625817721-239] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 239]
[TRACE] 2023-11-08 14:57:34,786 [qtp1625817721-239] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 239]
[DEBUG] 2023-11-08 14:57:34,788 [qtp1625817721-239] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:57:34,789 [qtp1625817721-239] io.confluent.mds.request.logger log - 99 * Server has received a request on thread qtp1625817721-239null99 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull99 > User Principal: c3
99 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null99 > Cache-Control: no-cachenull99 > Connection: keep-alivenull99 > Content-Type: application/jsonnull99 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null99 > Pragma: no-cachenull99 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:57:34,793 [qtp1625817721-239] io.confluent.mds.request.logger log - 99 * Server responded with a response on thread qtp1625817721-239null99 < 200null99 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:57:34,793 [qtp1625817721-239] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:57:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:57:34,794 [qtp1625817721-239] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:57:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:57:34,794 [qtp1625817721-239] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:57:34 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 11
[DEBUG] 2023-11-08 14:57:36,424 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:36,424 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:39,425 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:39,425 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:42,424 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:42,425 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:45,424 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:45,425 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:48,425 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:48,426 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:57:48,738 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-2.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:57:48,738 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:57:48,739 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:57:48,754 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,754 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:57:48,754 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:57:48,754 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:57:48,754 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455468754
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,757 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:57:48,758 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455468758
[INFO] 2023-11-08 14:57:48,767 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:57:48,780 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:57:48,780 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:57:48,781 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:57:48,781 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:57:48,781 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:57:48,781 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:57:48,781 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:57:48,782 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:57:48,782 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:57:48,782 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:57:48,782 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:57:48,782 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:57:48,782 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:57:51,425 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:51,426 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:54,425 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:54,426 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:57:57,425 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:57:57,426 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:57:58,584 [qtp1625817721-245] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 245]
[TRACE] 2023-11-08 14:57:58,586 [qtp1625817721-245] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 245]
[DEBUG] 2023-11-08 14:57:58,603 [qtp1625817721-245] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:57:58,604 [qtp1625817721-245] io.confluent.mds.request.logger log - 100 * Server has received a request on thread qtp1625817721-245null100 > GET https://kafka-2.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull100 > User Principal: c3
100 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null100 > Cache-Control: no-cachenull100 > Connection: keep-alivenull100 > Content-Type: application/jsonnull100 > Host: kafka-2.kafka.confluent.svc.cluster.local:8090null100 > Pragma: no-cachenull100 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:57:58,608 [qtp1625817721-245] io.confluent.mds.request.logger log - 100 * Server responded with a response on thread qtp1625817721-245null100 < 200null100 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:57:58,609 [qtp1625817721-245] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:57:58 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 26
[INFO] 2023-11-08 14:57:58,609 [qtp1625817721-245] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:57:58 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 26
[INFO] 2023-11-08 14:57:58,609 [qtp1625817721-245] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:57:58 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 26
[DEBUG] 2023-11-08 14:58:00,426 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:58:00,426 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:58:03,426 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:58:03,427 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:58:06,426 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:58:06,427 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:58:09,427 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:58:09,427 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-2, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
