+ /mnt/config/kafka/bin/run
===> User: uid=1001 gid=0(root) groups=0(root),1001
===> Load kafka operator scripts from path /mnt/config/kafka/bin
===> Configure log4j
===> Configure disk-usage agent
===> Configure jolokia agent
===> Configure jmx prometheus agent
===> Configure JVM config
===> Launching kafka
[Global flags]
      int AVX3Threshold                            = 4096                              {ARCH diagnostic} {default}
     bool AbortVMOnCompilationFailure              = false                                  {diagnostic} {default}
    ccstr AbortVMOnException                       =                                        {diagnostic} {default}
    ccstr AbortVMOnExceptionMessage                =                                        {diagnostic} {default}
     bool AbortVMOnSafepointTimeout                = false                                  {diagnostic} {default}
     bool AbortVMOnVMOperationTimeout              = false                                  {diagnostic} {default}
     intx AbortVMOnVMOperationTimeoutDelay         = 1000                                   {diagnostic} {default}
      int ActiveProcessorCount                     = -1                                        {product} {default}
    uintx AdaptiveSizeDecrementScaleFactor         = 4                                         {product} {default}
    uintx AdaptiveSizeMajorGCDecayTimeScale        = 10                                        {product} {default}
    uintx AdaptiveSizePolicyCollectionCostMargin   = 50                                        {product} {default}
    uintx AdaptiveSizePolicyInitializingSteps      = 20                                        {product} {default}
    uintx AdaptiveSizePolicyOutputInterval         = 0                                         {product} {default}
    uintx AdaptiveSizePolicyWeight                 = 10                                        {product} {default}
    uintx AdaptiveSizeThroughPutPolicy             = 0                                         {product} {default}
    uintx AdaptiveTimeWeight                       = 25                                        {product} {default}
     bool AggressiveHeap                           = false                                     {product} {default}
     bool AggressiveOpts                           = false                                     {product} {default}
     intx AliasLevel                               = 3                                      {C2 product} {default}
     bool AlignVector                              = false                                  {C2 product} {default}
    ccstr AllocateHeapAt                           =                                           {product} {default}
     intx AllocateInstancePrefetchLines            = 1                                         {product} {default}
     intx AllocatePrefetchDistance                 = 192                                       {product} {default}
     intx AllocatePrefetchInstr                    = 3                                         {product} {default}
     intx AllocatePrefetchLines                    = 4                                         {product} {default}
     intx AllocatePrefetchStepSize                 = 64                                        {product} {default}
     intx AllocatePrefetchStyle                    = 1                                         {product} {default}
     bool AllowJNIEnvProxy                         = false                                     {product} {default}
     bool AllowNonVirtualCalls                     = false                                     {product} {default}
     bool AllowParallelDefineClass                 = false                                     {product} {default}
     bool AllowUserSignalHandlers                  = false                                     {product} {default}
     bool AllowVectorizeOnDemand                   = true                                   {C2 product} {default}
     bool AlwaysActAsServerClassMachine            = false                                     {product} {default}
     bool AlwaysCompileLoopMethods                 = false                                     {product} {default}
     bool AlwaysLockClassLoader                    = false                                     {product} {default}
     bool AlwaysPreTouch                           = false                                     {product} {default}
     bool AlwaysRestoreFPU                         = false                                     {product} {default}
     bool AlwaysTenure                             = false                                     {product} {default}
     intx ArrayCopyLoadStoreMaxElem                = 8                                      {C2 product} {default}
     bool AssertOnSuspendWaitFailure               = false                                     {product} {default}
     bool AssumeMP                                 = true                                      {product} {default}
     intx AutoBoxCacheMax                          = 128                                    {C2 product} {default}
     intx BCEATraceLevel                           = 0                                         {product} {default}
     bool BackgroundCompilation                    = true                                   {pd product} {default}
   size_t BaseFootPrintEstimate                    = 268435456                                 {product} {default}
     intx BiasedLockingBulkRebiasThreshold         = 20                                        {product} {default}
     intx BiasedLockingBulkRevokeThreshold         = 40                                        {product} {default}
     intx BiasedLockingDecayTime                   = 25000                                     {product} {default}
     intx BiasedLockingStartupDelay                = 0                                         {product} {default}
     bool BindCMSThreadToCPU                       = false                                  {diagnostic} {default}
     bool BindGCTaskThreadsToCPUs                  = false                                     {product} {default}
     bool BlockLayoutByFrequency                   = true                                   {C2 product} {default}
     intx BlockLayoutMinDiamondPercentage          = 20                                     {C2 product} {default}
     bool BlockLayoutRotateLoops                   = true                                   {C2 product} {default}
     bool BlockOffsetArrayUseUnallocatedBlock      = false                                  {diagnostic} {default}
     bool BranchOnRegister                         = false                                  {C2 product} {default}
     bool BytecodeVerificationLocal                = false                                     {product} {default}
     bool BytecodeVerificationRemote               = true                                      {product} {default}
     bool C1OptimizeVirtualCallProfiling           = true                                   {C1 product} {default}
     bool C1ProfileBranches                        = true                                   {C1 product} {default}
     bool C1ProfileCalls                           = true                                   {C1 product} {default}
     bool C1ProfileCheckcasts                      = true                                   {C1 product} {default}
     bool C1ProfileInlinedCalls                    = true                                   {C1 product} {default}
     bool C1ProfileVirtualCalls                    = true                                   {C1 product} {default}
     bool C1UpdateMethodData                       = true                                   {C1 product} {default}
     intx CICompilerCount                          = 12                                        {product} {ergonomic}
     bool CICompilerCountPerCPU                    = true                                      {product} {default}
     bool CIPrintCompileQueue                      = false                                  {diagnostic} {default}
     bool CITime                                   = false                                     {product} {default}
     bool CMSAbortSemantics                        = false                                     {product} {default}
    uintx CMSAbortablePrecleanMinWorkPerIteration  = 100                                       {product} {default}
     intx CMSAbortablePrecleanWaitMillis           = 100                                    {manageable} {default}
   size_t CMSBitMapYieldQuantum                    = 10485760                                  {product} {default}
    uintx CMSBootstrapOccupancy                    = 50                                        {product} {default}
     bool CMSClassUnloadingEnabled                 = true                                      {product} {default}
    uintx CMSClassUnloadingMaxInterval             = 0                                         {product} {default}
     bool CMSCleanOnEnter                          = true                                      {product} {default}
   size_t CMSConcMarkMultiple                      = 32                                        {product} {default}
     bool CMSConcurrentMTEnabled                   = true                                      {product} {default}
    uintx CMSCoordinatorYieldSleepCount            = 10                                        {product} {default}
     bool CMSEdenChunksRecordAlways                = true                                      {product} {default}
    uintx CMSExpAvgFactor                          = 50                                        {product} {default}
     bool CMSExtrapolateSweep                      = false                                     {product} {default}
    uintx CMSIncrementalSafetyFactor               = 10                                        {product} {default}
    uintx CMSIndexedFreeListReplenish              = 4                                         {product} {default}
     intx CMSInitiatingOccupancyFraction           = -1                                        {product} {default}
    uintx CMSIsTooFullPercentage                   = 98                                        {product} {default}
   double CMSLargeCoalSurplusPercent               = 0.950000                                  {product} {default}
   double CMSLargeSplitSurplusPercent              = 1.000000                                  {product} {default}
     bool CMSLoopWarn                              = false                                     {product} {default}
    uintx CMSMaxAbortablePrecleanLoops             = 0                                         {product} {default}
     intx CMSMaxAbortablePrecleanTime              = 5000                                      {product} {default}
   size_t CMSOldPLABMax                            = 1024                                      {product} {default}
   size_t CMSOldPLABMin                            = 16                                        {product} {default}
    uintx CMSOldPLABNumRefills                     = 4                                         {product} {default}
    uintx CMSOldPLABReactivityFactor               = 2                                         {product} {default}
     bool CMSOldPLABResizeQuicker                  = false                                     {product} {default}
    uintx CMSOldPLABToleranceFactor                = 4                                         {product} {default}
     bool CMSPLABRecordAlways                      = true                                      {product} {default}
     bool CMSParallelInitialMarkEnabled            = true                                      {product} {default}
     bool CMSParallelRemarkEnabled                 = true                                      {product} {default}
     bool CMSParallelSurvivorRemarkEnabled         = true                                      {product} {default}
    uintx CMSPrecleanDenominator                   = 3                                         {product} {default}
    uintx CMSPrecleanIter                          = 3                                         {product} {default}
    uintx CMSPrecleanNumerator                     = 2                                         {product} {default}
     bool CMSPrecleanRefLists1                     = true                                      {product} {default}
     bool CMSPrecleanRefLists2                     = false                                     {product} {default}
     bool CMSPrecleanSurvivors1                    = false                                     {product} {default}
     bool CMSPrecleanSurvivors2                    = true                                      {product} {default}
    uintx CMSPrecleanThreshold                     = 1000                                      {product} {default}
     bool CMSPrecleaningEnabled                    = true                                      {product} {default}
     bool CMSPrintChunksInDump                     = false                                     {product} {default}
     bool CMSPrintObjectsInDump                    = false                                     {product} {default}
    uintx CMSRemarkVerifyVariant                   = 1                                         {product} {default}
     bool CMSReplenishIntermediate                 = true                                      {product} {default}
   size_t CMSRescanMultiple                        = 32                                        {product} {default}
    uintx CMSSamplingGrain                         = 16384                                     {product} {default}
     bool CMSScavengeBeforeRemark                  = false                                     {product} {default}
    uintx CMSScheduleRemarkEdenPenetration         = 50                                        {product} {default}
   size_t CMSScheduleRemarkEdenSizeThreshold       = 2097152                                   {product} {default}
    uintx CMSScheduleRemarkSamplingRatio           = 5                                         {product} {default}
   double CMSSmallCoalSurplusPercent               = 1.050000                                  {product} {default}
   double CMSSmallSplitSurplusPercent              = 1.100000                                  {product} {default}
     bool CMSSplitIndexedFreeListBlocks            = true                                      {product} {default}
     intx CMSTriggerInterval                       = -1                                     {manageable} {default}
    uintx CMSTriggerRatio                          = 80                                        {product} {default}
     intx CMSWaitDuration                          = 2000                                   {manageable} {default}
    uintx CMSWorkQueueDrainThreshold               = 10                                        {product} {default}
     bool CMSYield                                 = true                                      {product} {default}
    uintx CMSYieldSleepCount                       = 0                                         {product} {default}
   size_t CMSYoungGenPerWorker                     = 67108864                               {pd product} {default}
    uintx CMS_FLSPadding                           = 1                                         {product} {default}
    uintx CMS_FLSWeight                            = 75                                        {product} {default}
    uintx CMS_SweepPadding                         = 1                                         {product} {default}
    uintx CMS_SweepTimerThresholdMillis            = 10                                        {product} {default}
    uintx CMS_SweepWeight                          = 75                                        {product} {default}
    uintx CPUForCMSThread                          = 0                                      {diagnostic} {default}
     bool CalculateClassFingerprint                = false                                     {product} {default}
     bool CheckIntrinsics                          = true                                   {diagnostic} {default}
     bool CheckJNICalls                            = false                                     {product} {default}
     bool ClassUnloading                           = true                                      {product} {default}
     bool ClassUnloadingWithConcurrentMark         = true                                      {product} {default}
     bool ClipInlining                             = true                                      {product} {default}
    uintx CodeCacheExpansionSize                   = 65536                                  {pd product} {default}
    uintx CodeCacheMinBlockLength                  = 6                                   {pd diagnostic} {default}
     bool CompactFields                            = true                                      {product} {default}
     bool CompactStrings                           = true                                   {pd product} {default}
     intx CompilationPolicyChoice                  = 2                                         {product} {default}
ccstrlist CompileCommand                           =                                           {product} {default}
    ccstr CompileCommandFile                       =                                           {product} {default}
ccstrlist CompileOnly                              =                                           {product} {default}
     intx CompileThreshold                         = 10000                                  {pd product} {default}
   double CompileThresholdScaling                  = 1.000000                                  {product} {default}
    ccstr CompilerDirectivesFile                   =                                        {diagnostic} {default}
     bool CompilerDirectivesIgnoreCompileCommands  = false                                  {diagnostic} {default}
      int CompilerDirectivesLimit                  = 50                                     {diagnostic} {default}
     bool CompilerDirectivesPrint                  = false                                  {diagnostic} {default}
     bool CompilerThreadHintNoPreempt              = false                                     {product} {default}
     intx CompilerThreadPriority                   = -1                                        {product} {default}
     intx CompilerThreadStackSize                  = 1024                                   {pd product} {default}
   size_t CompressedClassSpaceSize                 = 1073741824                                {product} {default}
     uint ConcGCThreads                            = 1                                         {product} {command line}
     intx ConditionalMoveLimit                     = 3                                   {C2 pd product} {default}
     intx ContendedPaddingWidth                    = 128                                       {product} {default}
     bool CrashOnOutOfMemoryError                  = false                                     {product} {default}
     bool CreateCoredumpOnCrash                    = true                                      {product} {default}
     bool CriticalJNINatives                       = true                                      {product} {default}
     bool DTraceAllocProbes                        = false                                     {product} {default}
     bool DTraceMethodProbes                       = false                                     {product} {default}
     bool DTraceMonitorProbes                      = false                                     {product} {default}
     bool DebugInlinedCalls                        = true                                {C2 diagnostic} {default}
     bool DebugNonSafepoints                       = false                                  {diagnostic} {default}
     bool Debugging                                = false                                     {product} {default}
     bool DeferInitialCardMark                     = false                                  {diagnostic} {default}
     bool DeoptimizeRandom                         = false                                     {product} {default}
     bool DisableAttachMechanism                   = false                                     {product} {default}
     bool DisableExplicitGC                        = false                                     {product} {default}
ccstrlist DisableIntrinsic                         =                                        {diagnostic} {default}
     bool DisplayVMOutput                          = true                                   {diagnostic} {default}
     bool DisplayVMOutputToStderr                  = false                                     {product} {default}
     bool DisplayVMOutputToStdout                  = false                                     {product} {default}
     bool DoEscapeAnalysis                         = true                                   {C2 product} {default}
     bool DoReserveCopyInSuperWord                 = true                                   {C2 product} {default}
     intx DominatorSearchLimit                     = 1000                                {C2 diagnostic} {default}
     bool DontCompileHugeMethods                   = true                                      {product} {default}
     bool DontYieldALot                            = false                                  {pd product} {default}
    ccstr DumpLoadedClassList                      =                                           {product} {default}
     bool DumpPrivateMappingsInCore                = true                                   {diagnostic} {default}
     bool DumpReplayDataOnError                    = true                                      {product} {default}
     bool DumpSharedMappingsInCore                 = true                                   {diagnostic} {default}
     bool DumpSharedSpaces                         = false                                     {product} {default}
     bool DynamicallyResizeSystemDictionaries      = true                                   {diagnostic} {default}
     bool EagerXrunInit                            = false                                     {product} {default}
     intx EliminateAllocationArraySizeLimit        = 64                                     {C2 product} {default}
     bool EliminateAllocations                     = true                                   {C2 product} {default}
     bool EliminateAutoBox                         = true                                   {C2 product} {default}
     bool EliminateLocks                           = true                                   {C2 product} {default}
     bool EliminateNestedLocks                     = true                                   {C2 product} {default}
     bool EnableContended                          = true                                      {product} {default}
     bool EnableDynamicAgentLoading                = true                                      {product} {default}
     bool EnableThreadSMRExtraValidityChecks       = true                                   {diagnostic} {default}
     bool EnableThreadSMRStatistics                = false                                  {diagnostic} {default}
   size_t ErgoHeapSizeLimit                        = 0                                         {product} {default}
    ccstr ErrorFile                                =                                           {product} {default}
     bool ErrorFileToStderr                        = false                                     {product} {default}
     bool ErrorFileToStdout                        = false                                     {product} {default}
 uint64_t ErrorLogTimeout                          = 120                                       {product} {default}
    ccstr ErrorReportServer                        =                                           {product} {default}
   double EscapeAnalysisTimeout                    = 20.000000                              {C2 product} {default}
     bool EstimateArgEscape                        = true                                      {product} {default}
     bool ExecutingUnitTests                       = false                                     {product} {default}
     bool ExitOnOutOfMemoryError                   = false                                     {product} {default}
     bool ExplicitGCInvokesConcurrent              = true                                      {product} {command line}
     bool ExtendedDTraceProbes                     = false                                     {product} {default}
     bool ExtensiveErrorReports                    = false                                     {product} {default}
    ccstr ExtraSharedClassListFile                 =                                           {product} {default}
     bool FLSAlwaysCoalesceLarge                   = false                                     {product} {default}
    uintx FLSCoalescePolicy                        = 2                                         {product} {default}
   double FLSLargestBlockCoalesceProximity         = 0.990000                                  {product} {default}
     bool FLSVerifyAllHeapReferences               = false                                  {diagnostic} {default}
     bool FLSVerifyIndexTable                      = false                                  {diagnostic} {default}
     bool FLSVerifyLists                           = false                                  {diagnostic} {default}
     bool FailOverToOldVerifier                    = true                                      {product} {default}
     intx FieldsAllocationStyle                    = 1                                         {product} {default}
     bool FilterSpuriousWakeups                    = true                                      {product} {default}
     bool FlightRecorder                           = false                                     {product} {default}
    ccstr FlightRecorderOptions                    =                                           {product} {default}
     bool FoldStableValues                         = true                                   {diagnostic} {default}
     bool ForceDynamicNumberOfGCThreads            = false                                  {diagnostic} {default}
     bool ForceNUMA                                = false                                     {product} {default}
     bool ForceTimeHighResolution                  = false                                     {product} {default}
     bool ForceUnreachable                         = false                                  {diagnostic} {default}
     intx FreqInlineSize                           = 325                                    {pd product} {default}
   double G1ConcMarkStepDurationMillis             = 10.000000                                 {product} {default}
    uintx G1ConcRSHotCardLimit                     = 4                                         {product} {default}
   size_t G1ConcRSLogCacheSize                     = 10                                        {product} {default}
   size_t G1ConcRefinementGreenZone                = 0                                         {product} {default}
   size_t G1ConcRefinementRedZone                  = 0                                         {product} {default}
    uintx G1ConcRefinementServiceIntervalMillis    = 300                                       {product} {default}
     uint G1ConcRefinementThreads                  = 1                                         {product} {ergonomic}
   size_t G1ConcRefinementThresholdStep            = 2                                         {product} {default}
   size_t G1ConcRefinementYellowZone               = 0                                         {product} {default}
    uintx G1ConfidencePercent                      = 50                                        {product} {default}
   size_t G1HeapRegionSize                         = 16777216                                  {product} {command line}
    uintx G1HeapWastePercent                       = 5                                         {product} {default}
    uintx G1MixedGCCountTarget                     = 8                                         {product} {default}
     intx G1RSetRegionEntries                      = 1280                                      {product} {default}
   size_t G1RSetScanBlockSize                      = 64                                        {product} {default}
     intx G1RSetSparseRegionEntries                = 20                                        {product} {default}
     intx G1RSetUpdatingPauseTimePercent           = 10                                        {product} {default}
     uint G1RefProcDrainInterval                   = 1000                                      {product} {default}
    uintx G1ReservePercent                         = 10                                        {product} {default}
    uintx G1SATBBufferEnqueueingThresholdPercent   = 60                                        {product} {default}
   size_t G1SATBBufferSize                         = 1024                                      {product} {default}
     intx G1SummarizeRSetStatsPeriod               = 0                                      {diagnostic} {default}
   size_t G1UpdateBufferSize                       = 256                                       {product} {default}
     bool G1UseAdaptiveConcRefinement              = true                                      {product} {default}
     bool G1UseAdaptiveIHOP                        = true                                      {product} {default}
     bool G1VerifyHeapRegionCodeRoots              = false                                  {diagnostic} {default}
     bool G1VerifyRSetsDuringFullGC                = false                                  {diagnostic} {default}
    uintx GCDrainStackTargetSize                   = 64                                        {product} {ergonomic}
    uintx GCHeapFreeLimit                          = 2                                         {product} {default}
    uintx GCLockerEdenExpansionPercent             = 5                                         {product} {default}
     bool GCLockerInvokesConcurrent                = false                                     {product} {default}
    uintx GCLockerRetryAllocationCount             = 2                                      {diagnostic} {default}
     bool GCParallelVerificationEnabled            = true                                   {diagnostic} {default}
    uintx GCPauseIntervalMillis                    = 21                                        {product} {default}
     uint GCTaskTimeStampEntries                   = 200                                       {product} {default}
    uintx GCTimeLimit                              = 98                                        {product} {default}
    uintx GCTimeRatio                              = 12                                        {product} {default}
     intx GuaranteedSafepointInterval              = 1000                                   {diagnostic} {default}
     uint HandshakeTimeout                         = 0                                      {diagnostic} {default}
   size_t HeapBaseMinAddress                       = 2147483648                             {pd product} {default}
     bool HeapDumpAfterFullGC                      = false                                  {manageable} {default}
     bool HeapDumpBeforeFullGC                     = false                                  {manageable} {default}
     bool HeapDumpOnOutOfMemoryError               = false                                  {manageable} {default}
    ccstr HeapDumpPath                             =                                        {manageable} {default}
    uintx HeapFirstMaximumCompactionCount          = 3                                         {product} {default}
    uintx HeapMaximumCompactionInterval            = 20                                        {product} {default}
    uintx HeapSearchSteps                          = 3                                         {product} {default}
   size_t HeapSizePerGCThread                      = 43620760                                  {product} {default}
     intx HotMethodDetectionLimit                  = 100000                                 {diagnostic} {default}
     bool IdealizeClearArrayNode                   = true                             {C2 pd diagnostic} {default}
     bool IgnoreEmptyClassPaths                    = false                                     {product} {default}
     bool IgnoreUnrecognizedVMOptions              = false                                     {product} {default}
     bool IgnoreUnverifiableClassesDuringDump      = true                                   {diagnostic} {default}
     bool ImplicitNullChecks                       = true                                {pd diagnostic} {default}
    uintx IncreaseFirstTierCompileThresholdAt      = 50                                        {product} {default}
     bool IncrementalInline                        = true                                   {C2 product} {default}
     intx InitArrayShortSize                       = 64                                  {pd diagnostic} {default}
   size_t InitialBootClassLoaderMetaspaceSize      = 4194304                                   {product} {default}
    uintx InitialCodeCacheSize                     = 2555904                                {pd product} {default}
   size_t InitialHeapSize                          = 1056964608                                {product} {ergonomic}
    uintx InitialRAMFraction                       = 64                                        {product} {default}
   double InitialRAMPercentage                     = 1.562500                                  {product} {default}
    uintx InitialSurvivorRatio                     = 8                                         {product} {default}
    uintx InitialTenuringThreshold                 = 7                                         {product} {default}
    uintx InitiatingHeapOccupancyPercent           = 35                                        {product} {command line}
     bool InjectGCWorkerCreationFailure            = false                                  {diagnostic} {default}
     bool Inline                                   = true                                      {product} {default}
     bool InlineArrayCopy                          = true                                   {diagnostic} {default}
     bool InlineClassNatives                       = true                                   {diagnostic} {default}
    ccstr InlineDataFile                           =                                           {product} {default}
     intx InlineFrequencyCount                     = 100                                 {pd diagnostic} {default}
     bool InlineMathNatives                        = true                                   {diagnostic} {default}
     bool InlineNIOCheckIndex                      = true                                {C1 diagnostic} {default}
     bool InlineNatives                            = true                                   {diagnostic} {default}
     bool InlineObjectCopy                         = true                                {C2 diagnostic} {default}
     bool InlineObjectHash                         = true                                   {diagnostic} {default}
     bool InlineReflectionGetCallerClass           = true                                {C2 diagnostic} {default}
     intx InlineSmallCode                          = 2000                                   {pd product} {default}
     bool InlineSynchronizedMethods                = true                                   {C1 product} {default}
     bool InlineThreadNatives                      = true                                   {diagnostic} {default}
     bool InlineUnsafeOps                          = true                                   {diagnostic} {default}
     bool InsertMemBarAfterArraycopy               = true                                   {C2 product} {default}
     intx InteriorEntryAlignment                   = 16                                  {C2 pd product} {default}
     intx InterpreterProfilePercentage             = 33                                        {product} {default}
     bool JavaMonitorsInStackTrace                 = true                                      {product} {default}
     intx JavaPriority10_To_OSPriority             = -1                                        {product} {default}
     intx JavaPriority1_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority2_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority3_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority4_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority5_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority6_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority7_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority8_To_OSPriority              = -1                                        {product} {default}
     intx JavaPriority9_To_OSPriority              = -1                                        {product} {default}
     bool LIRFillDelaySlots                        = false                               {C1 pd product} {default}
   size_t LargePageHeapSizeThreshold               = 134217728                                 {product} {default}
   size_t LargePageSizeInBytes                     = 0                                         {product} {default}
     intx LiveNodeCountInliningCutoff              = 40000                                  {C2 product} {default}
     bool LoadExecStackDllInVMThread               = true                                      {product} {default}
     bool LogCompilation                           = false                                  {diagnostic} {default}
     bool LogEvents                                = true                                   {diagnostic} {default}
    uintx LogEventsBufferEntries                   = 20                                     {diagnostic} {default}
    ccstr LogFile                                  =                                        {diagnostic} {default}
     bool LogTouchedMethods                        = false                                  {diagnostic} {default}
     bool LogVMOutput                              = false                                  {diagnostic} {default}
     intx LoopMaxUnroll                            = 16                                     {C2 product} {default}
     intx LoopOptsCount                            = 43                                     {C2 product} {default}
     intx LoopPercentProfileLimit                  = 10                                  {C2 pd product} {default}
    uintx LoopStripMiningIter                      = 1000                                   {C2 product} {default}
    uintx LoopStripMiningIterShortLoop             = 100                                    {C2 product} {default}
     intx LoopUnrollLimit                          = 60                                  {C2 pd product} {default}
     intx LoopUnrollMin                            = 4                                      {C2 product} {default}
     bool LoopUnswitching                          = true                                   {C2 product} {default}
    uintx MallocMaxTestWords                       = 0                                      {diagnostic} {default}
     bool ManagementServer                         = true                                      {product} {command line}
   size_t MarkStackSize                            = 4194304                                   {product} {ergonomic}
   size_t MarkStackSizeMax                         = 16777216                                  {product} {default}
     uint MarkSweepAlwaysCompactCount              = 4                                         {product} {default}
    uintx MarkSweepDeadRatio                       = 5                                         {product} {default}
     intx MaxBCEAEstimateLevel                     = 5                                         {product} {default}
     intx MaxBCEAEstimateSize                      = 150                                       {product} {default}
 uint64_t MaxDirectMemorySize                      = 0                                         {product} {default}
     bool MaxFDLimit                               = true                                      {product} {default}
    uintx MaxGCMinorPauseMillis                    = 18446744073709551615                      {product} {default}
    uintx MaxGCPauseMillis                         = 20                                        {product} {command line}
    uintx MaxHeapFreeRatio                         = 70                                     {manageable} {default}
   size_t MaxHeapSize                              = 16861102080                               {product} {ergonomic}
     intx MaxInlineLevel                           = 15                                        {product} {default}
     intx MaxInlineSize                            = 35                                        {product} {default}
     intx MaxJNILocalCapacity                      = 65536                                     {product} {default}
     intx MaxJavaStackTraceDepth                   = 1024                                      {product} {default}
     intx MaxJumpTableSize                         = 65000                                  {C2 product} {default}
     intx MaxJumpTableSparseness                   = 5                                      {C2 product} {default}
     intx MaxLabelRootDepth                        = 1100                                   {C2 product} {default}
     intx MaxLoopPad                               = 11                                     {C2 product} {default}
   size_t MaxMetaspaceExpansion                    = 5451776                                   {product} {default}
    uintx MaxMetaspaceFreeRatio                    = 80                                        {product} {command line}
   size_t MaxMetaspaceSize                         = 18446744073709547520                      {product} {default}
   size_t MaxNewSize                               = 10116661248                               {product} {ergonomic}
     intx MaxNodeLimit                             = 80000                                  {C2 product} {default}
 uint64_t MaxRAM                                   = 137438953472                           {pd product} {default}
    uintx MaxRAMFraction                           = 4                                         {product} {default}
   double MaxRAMPercentage                         = 25.000000                                 {product} {default}
     intx MaxRecursiveInlineLevel                  = 1                                         {product} {default}
    uintx MaxTenuringThreshold                     = 15                                        {product} {default}
     intx MaxTrivialSize                           = 6                                         {product} {default}
     intx MaxVectorSize                            = 64                                     {C2 product} {default}
   size_t MetaspaceSize                            = 100663296                              {pd product} {command line}
     bool MethodFlushing                           = true                                      {product} {default}
   size_t MinHeapDeltaBytes                        = 16777216                                  {product} {ergonomic}
    uintx MinHeapFreeRatio                         = 40                                     {manageable} {default}
     intx MinInliningThreshold                     = 250                                       {product} {default}
     intx MinJumpTableSize                         = 10                                  {C2 pd product} {default}
   size_t MinMetaspaceExpansion                    = 339968                                    {product} {default}
    uintx MinMetaspaceFreeRatio                    = 50                                        {product} {command line}
     intx MinPassesBeforeFlush                     = 10                                     {diagnostic} {default}
    uintx MinRAMFraction                           = 2                                         {product} {default}
   double MinRAMPercentage                         = 50.000000                                 {product} {default}
    uintx MinSurvivorRatio                         = 3                                         {product} {default}
   size_t MinTLABSize                              = 2048                                      {product} {default}
     intx MonitorBound                             = 0                                         {product} {default}
     bool MonitorInUseLists                        = true                                      {product} {default}
     intx MultiArrayExpandLimit                    = 6                                      {C2 product} {default}
    uintx NUMAChunkResizeWeight                    = 20                                        {product} {default}
   size_t NUMAInterleaveGranularity                = 2097152                                   {product} {default}
    uintx NUMAPageScanRate                         = 256                                       {product} {default}
   size_t NUMASpaceResizeRate                      = 1073741824                                {product} {default}
     bool NUMAStats                                = false                                     {product} {default}
    ccstr NativeMemoryTracking                     = off                                       {product} {default}
     bool NeedsDeoptSuspend                        = false                                  {pd product} {default}
     bool NeverActAsServerClassMachine             = false                                  {pd product} {default}
     bool NeverTenure                              = false                                     {product} {default}
    uintx NewRatio                                 = 2                                         {product} {default}
   size_t NewSize                                  = 1363144                                   {product} {default}
   size_t NewSizeThreadIncrease                    = 5320                                   {pd product} {default}
     intx NmethodSweepActivity                     = 10                                        {product} {default}
     intx NodeLimitFudgeFactor                     = 2000                                   {C2 product} {default}
    uintx NonNMethodCodeHeapSize                   = 7594288                                {pd product} {ergonomic}
    uintx NonProfiledCodeHeapSize                  = 122031976                              {pd product} {ergonomic}
     intx NumberOfLoopInstrToAlign                 = 4                                      {C2 product} {default}
     intx ObjectAlignmentInBytes                   = 8                                    {lp64_product} {default}
   size_t OldPLABSize                              = 1024                                      {product} {default}
    uintx OldPLABWeight                            = 50                                        {product} {default}
   size_t OldSize                                  = 5452592                                   {product} {default}
     bool OmitStackTraceInFastThrow                = true                                      {product} {default}
ccstrlist OnError                                  =                                           {product} {default}
ccstrlist OnOutOfMemoryError                       =                                           {product} {default}
     intx OnStackReplacePercentage                 = 140                                    {pd product} {default}
     bool OptimizeExpensiveOps                     = true                                {C2 diagnostic} {default}
     bool OptimizeFill                             = true                                   {C2 product} {default}
     bool OptimizePtrCompare                       = true                                   {C2 product} {default}
     bool OptimizeStringConcat                     = true                                   {C2 product} {default}
     bool OptoBundling                             = false                               {C2 pd product} {default}
     intx OptoLoopAlignment                        = 16                                     {pd product} {default}
     bool OptoRegScheduling                        = true                                {C2 pd product} {default}
     bool OptoScheduling                           = false                               {C2 pd product} {default}
     bool OverrideVMProperties                     = false                                     {product} {default}
    uintx PLABWeight                               = 75                                        {product} {default}
     bool PSChunkLargeArrays                       = true                                      {product} {default}
      int ParGCArrayScanChunk                      = 50                                        {product} {default}
     intx ParGCCardsPerStrideChunk                 = 256                                    {diagnostic} {default}
    uintx ParGCDesiredObjsFromOverflowList         = 20                                        {product} {default}
    uintx ParGCStridesPerThread                    = 2                                      {diagnostic} {default}
     bool ParGCTrimOverflow                        = true                                      {product} {default}
     bool ParGCUseLocalOverflow                    = false                                     {product} {default}
    uintx ParallelGCBufferWastePct                 = 10                                        {product} {default}
     uint ParallelGCThreads                        = 1                                         {product} {command line}
   size_t ParallelOldDeadWoodLimiterMean           = 50                                        {product} {default}
   size_t ParallelOldDeadWoodLimiterStdDev         = 80                                        {product} {default}
     bool ParallelRefProcBalancingEnabled          = true                                      {product} {default}
     bool ParallelRefProcEnabled                   = false                                     {product} {default}
     bool PartialPeelAtUnsignedTests               = true                                   {C2 product} {default}
     bool PartialPeelLoop                          = true                                   {C2 product} {default}
     intx PartialPeelNewPhiDelta                   = 0                                      {C2 product} {default}
     bool PauseAtExit                              = false                                  {diagnostic} {default}
     bool PauseAtStartup                           = false                                  {diagnostic} {default}
    ccstr PauseAtStartupFile                       =                                        {diagnostic} {default}
    uintx PausePadding                             = 1                                         {product} {default}
     intx PerBytecodeRecompilationCutoff           = 200                                       {product} {default}
     intx PerBytecodeTrapLimit                     = 4                                         {product} {default}
     intx PerMethodRecompilationCutoff             = 400                                       {product} {default}
     intx PerMethodTrapLimit                       = 100                                       {product} {default}
     bool PerfAllowAtExitRegistration              = false                                     {product} {default}
     bool PerfBypassFileSystemCheck                = false                                     {product} {default}
     intx PerfDataMemorySize                       = 32768                                     {product} {default}
     intx PerfDataSamplingInterval                 = 50                                        {product} {default}
    ccstr PerfDataSaveFile                         =                                           {product} {default}
     bool PerfDataSaveToFile                       = false                                     {product} {default}
     bool PerfDisableSharedMem                     = true                                      {product} {default}
     intx PerfMaxStringConstLength                 = 1024                                      {product} {default}
   size_t PreTouchParallelChunkSize                = 1073741824                                {product} {default}
     bool PreferContainerQuotaForCPUCount          = true                                      {product} {default}
     bool PreferInterpreterNativeStubs             = false                                  {pd product} {default}
     intx PrefetchCopyIntervalInBytes              = 576                                       {product} {default}
     intx PrefetchFieldsAhead                      = 1                                         {product} {default}
     intx PrefetchScanIntervalInBytes              = 576                                       {product} {default}
     bool PreserveAllAnnotations                   = false                                     {product} {default}
     bool PreserveFramePointer                     = false                                  {pd product} {default}
   size_t PretenureSizeThreshold                   = 0                                         {product} {default}
     bool PrintAdapterHandlers                     = false                                  {diagnostic} {default}
     bool PrintAssembly                            = false                                  {diagnostic} {default}
    ccstr PrintAssemblyOptions                     =                                        {diagnostic} {default}
     bool PrintBiasedLockingStatistics             = false                                  {diagnostic} {default}
     bool PrintClassHistogram                      = false                                  {manageable} {default}
     bool PrintCodeCache                           = false                                     {product} {default}
     bool PrintCodeCacheOnCompilation              = false                                     {product} {default}
     bool PrintCommandLineFlags                    = false                                     {product} {default}
     bool PrintCompilation                         = false                                     {product} {default}
     bool PrintCompilation2                        = false                                  {diagnostic} {default}
     bool PrintConcurrentLocks                     = false                                  {manageable} {default}
     bool PrintExtendedThreadInfo                  = false                                     {product} {default}
     bool PrintFlagsFinal                          = true                                      {product} {command line}
     bool PrintFlagsInitial                        = false                                     {product} {default}
     bool PrintFlagsRanges                         = false                                     {product} {default}
     bool PrintGC                                  = false                                     {product} {default}
     bool PrintGCDetails                           = false                                     {product} {default}
     bool PrintHeapAtSIGBREAK                      = true                                      {product} {default}
     bool PrintInlining                            = false                                  {diagnostic} {default}
     bool PrintInterpreter                         = false                                  {diagnostic} {default}
     bool PrintIntrinsics                          = false                               {C2 diagnostic} {default}
     bool PrintJNIResolving                        = false                                     {product} {default}
     bool PrintMetaspaceStatisticsAtExit           = false                                  {diagnostic} {default}
     bool PrintMethodData                          = false                                  {diagnostic} {default}
     bool PrintMethodFlushingStatistics            = false                                  {diagnostic} {default}
     bool PrintMethodHandleStubs                   = false                                  {diagnostic} {default}
     bool PrintNMTStatistics                       = false                                  {diagnostic} {default}
     bool PrintNMethods                            = false                                  {diagnostic} {default}
     bool PrintNativeNMethods                      = false                                  {diagnostic} {default}
     bool PrintOptoAssembly                        = false                               {C2 diagnostic} {default}
     bool PrintPreciseBiasedLockingStatistics      = false                               {C2 diagnostic} {default}
     bool PrintPreciseRTMLockingStatistics         = false                               {C2 diagnostic} {default}
     bool PrintSafepointStatistics                 = false                                     {product} {default}
     intx PrintSafepointStatisticsCount            = 300                                       {product} {default}
     intx PrintSafepointStatisticsTimeout          = -1                                        {product} {default}
     bool PrintSharedArchiveAndExit                = false                                     {product} {default}
     bool PrintSharedDictionary                    = false                                     {product} {default}
     bool PrintSignatureHandlers                   = false                                  {diagnostic} {default}
     bool PrintStringTableStatistics               = false                                     {product} {default}
     bool PrintStubCode                            = false                                  {diagnostic} {default}
     bool PrintTieredEvents                        = false                                     {product} {default}
     bool PrintTouchedMethodsAtExit                = false                                  {diagnostic} {default}
     bool PrintVMOptions                           = false                                     {product} {default}
     bool PrintVMQWaitTime                         = false                                     {product} {default}
     bool PrintWarnings                            = true                                      {product} {default}
    uintx ProcessDistributionStride                = 4                                         {product} {default}
     bool ProfileDynamicTypes                      = true                                {C2 diagnostic} {default}
     bool ProfileInterpreter                       = true                                   {pd product} {default}
     bool ProfileIntervals                         = false                                     {product} {default}
     intx ProfileIntervalsTicks                    = 100                                       {product} {default}
     intx ProfileMaturityPercentage                = 20                                        {product} {default}
     bool ProfileVM                                = false                                     {product} {default}
    uintx ProfiledCodeHeapSize                     = 122031976                              {pd product} {ergonomic}
     intx ProfilerNumberOfCompiledMethods          = 25                                     {diagnostic} {default}
     intx ProfilerNumberOfInterpretedMethods       = 25                                     {diagnostic} {default}
     intx ProfilerNumberOfRuntimeStubNodes         = 25                                     {diagnostic} {default}
     intx ProfilerNumberOfStubMethods              = 25                                     {diagnostic} {default}
     bool ProfilerPrintByteCodeStatistics          = false                                     {product} {default}
     bool ProfilerRecordPC                         = false                                     {product} {default}
    uintx PromotedPadding                          = 3                                         {product} {default}
    uintx QueuedAllocationWarningCount             = 0                                         {product} {default}
      int RTMRetryCount                            = 5                                    {ARCH product} {default}
     bool RangeCheckElimination                    = true                                      {product} {default}
     bool ReassociateInvariants                    = true                                   {C2 product} {default}
     bool ReduceBulkZeroing                        = true                                   {C2 product} {default}
     bool ReduceFieldZeroing                       = true                                   {C2 product} {default}
     bool ReduceInitialCardMarks                   = true                                   {C2 product} {default}
     bool ReduceNumberOfCompilerThreads            = true                                   {diagnostic} {default}
     bool ReduceSignalUsage                        = false                                     {product} {default}
     intx RefDiscoveryPolicy                       = 0                                         {product} {default}
     bool RegisterFinalizersAtInit                 = true                                      {product} {default}
     bool RelaxAccessControlCheck                  = false                                     {product} {default}
    ccstr ReplayDataFile                           =                                           {product} {default}
     bool RequireSharedSpaces                      = false                                     {product} {default}
    uintx ReservedCodeCacheSize                    = 251658240                              {pd product} {ergonomic}
     bool ResizeOldPLAB                            = true                                      {product} {default}
     bool ResizePLAB                               = true                                      {product} {default}
     bool ResizeTLAB                               = true                                   {pd product} {default}
     bool RestoreMXCSROnJNICalls                   = false                                     {product} {default}
     bool RestrictContended                        = true                                      {product} {default}
     bool RestrictReservedStack                    = true                                      {product} {default}
     bool RewriteBytecodes                         = true                                   {pd product} {default}
     bool RewriteFrequentPairs                     = true                                   {pd product} {default}
     bool SafepointALot                            = false                                  {diagnostic} {default}
     bool SafepointTimeout                         = false                                     {product} {default}
     intx SafepointTimeoutDelay                    = 10000                                     {product} {default}
     bool ScavengeBeforeFullGC                     = false                                     {product} {default}
     intx ScavengeRootsInCode                      = 2                                      {diagnostic} {default}
     bool SegmentedCodeCache                       = true                                      {product} {ergonomic}
     intx SelfDestructTimer                        = 0                                         {product} {default}
     bool SerializeVMOutput                        = true                                   {diagnostic} {default}
    ccstr SharedArchiveConfigFile                  =                                           {product} {default}
    ccstr SharedArchiveFile                        =                                           {product} {default}
   size_t SharedBaseAddress                        = 34359738368                               {product} {default}
    ccstr SharedClassListFile                      =                                           {product} {default}
    uintx SharedSymbolTableBucketSize              = 4                                         {product} {default}
     bool ShenandoahAllocFailureALot               = false                                  {diagnostic} {default}
     bool ShenandoahCASBarrier                     = true                                   {diagnostic} {default}
     bool ShenandoahCloneBarrier                   = true                                   {diagnostic} {default}
    uintx ShenandoahCodeRootsStyle                 = 2                                      {diagnostic} {default}
     bool ShenandoahDegeneratedGC                  = true                                   {diagnostic} {default}
     bool ShenandoahElasticTLAB                    = true                                   {diagnostic} {default}
    ccstr ShenandoahGCHeuristics                   = adaptive                                  {product} {default}
    ccstr ShenandoahGCMode                         = satb                                      {product} {default}
     bool ShenandoahHumongousMoves                 = true                                   {diagnostic} {default}
     bool ShenandoahIUBarrier                      = false                                  {diagnostic} {default}
     bool ShenandoahLoadRefBarrier                 = true                                   {diagnostic} {default}
     bool ShenandoahLoopOptsAfterExpansion         = true                                   {diagnostic} {default}
     bool ShenandoahOOMDuringEvacALot              = false                                  {diagnostic} {default}
     bool ShenandoahOptimizeStaticFinals           = true                                   {diagnostic} {default}
     bool ShenandoahPreclean                       = true                                   {diagnostic} {default}
     bool ShenandoahSATBBarrier                    = true                                   {diagnostic} {default}
     bool ShenandoahSelfFixing                     = true                                   {diagnostic} {default}
   size_t ShenandoahSoftMaxHeapSize                = 0                                      {manageable} {default}
     bool ShenandoahVerify                         = false                                  {diagnostic} {default}
     intx ShenandoahVerifyLevel                    = 4                                      {diagnostic} {default}
     bool ShowHiddenFrames                         = false                                  {diagnostic} {default}
     bool ShowMessageBoxOnError                    = false                                     {product} {default}
     bool ShowRegistersOnAssert                    = false                                  {diagnostic} {default}
     bool ShrinkHeapInSteps                        = true                                      {product} {default}
     intx SoftRefLRUPolicyMSPerMB                  = 1000                                      {product} {default}
     bool SpecialArraysEquals                      = true                                {C2 diagnostic} {default}
     bool SpecialEncodeISOArray                    = true                                {C2 diagnostic} {default}
     bool SpecialStringCompareTo                   = true                                {C2 diagnostic} {default}
     bool SpecialStringEquals                      = true                                {C2 diagnostic} {default}
     bool SpecialStringIndexOf                     = true                                {C2 diagnostic} {default}
     bool SplitIfBlocks                            = true                                   {C2 product} {default}
     intx StackRedPages                            = 1                                      {pd product} {default}
     intx StackReservedPages                       = 1                                      {pd product} {default}
     intx StackShadowPages                         = 20                                     {pd product} {default}
     bool StackTraceInThrowable                    = true                                      {product} {default}
     intx StackYellowPages                         = 2                                      {pd product} {default}
    uintx StartAggressiveSweepingAt                = 10                                        {product} {default}
     bool StartAttachListener                      = false                                     {product} {default}
    ccstr StartFlightRecording                     =                                           {product} {default}
     bool StressCodeAging                          = false                                  {diagnostic} {default}
     bool StressGCM                                = false                               {C2 diagnostic} {default}
     bool StressLCM                                = false                               {C2 diagnostic} {default}
     bool StressLdcRewrite                         = false                                     {product} {default}
    uintx StringDeduplicationAgeThreshold          = 3                                         {product} {default}
     bool StringDeduplicationRehashALot            = false                                  {diagnostic} {default}
     bool StringDeduplicationResizeALot            = false                                  {diagnostic} {default}
    uintx StringTableSize                          = 65536                                     {product} {default}
     bool SuperWordLoopUnrollAnalysis              = true                                {C2 pd product} {default}
     bool SuperWordReductions                      = true                                   {C2 product} {default}
     bool SuppressFatalErrorMessage                = false                                     {product} {default}
    uintx SurvivorPadding                          = 3                                         {product} {default}
    uintx SurvivorRatio                            = 8                                         {product} {default}
     intx SuspendRetryCount                        = 50                                        {product} {default}
     intx SuspendRetryDelay                        = 5                                         {product} {default}
    uintx TLABAllocationWeight                     = 35                                        {product} {default}
    uintx TLABRefillWasteFraction                  = 64                                        {product} {default}
   size_t TLABSize                                 = 0                                         {product} {default}
     bool TLABStats                                = true                                      {product} {default}
    uintx TLABWasteIncrement                       = 4                                         {product} {default}
    uintx TLABWasteTargetPercent                   = 1                                         {product} {default}
    uintx TargetPLABWastePct                       = 10                                        {product} {default}
    uintx TargetSurvivorRatio                      = 50                                        {product} {default}
    uintx TenuredGenerationSizeIncrement           = 20                                        {product} {default}
    uintx TenuredGenerationSizeSupplement          = 80                                        {product} {default}
    uintx TenuredGenerationSizeSupplementDecay     = 2                                         {product} {default}
     bool ThreadLocalHandshakes                    = true                                   {pd product} {default}
     intx ThreadPriorityPolicy                     = 0                                         {product} {default}
     bool ThreadPriorityVerbose                    = false                                     {product} {default}
     intx ThreadStackSize                          = 1024                                   {pd product} {default}
    uintx ThresholdTolerance                       = 10                                        {product} {default}
     intx Tier0BackedgeNotifyFreqLog               = 10                                        {product} {default}
     intx Tier0InvokeNotifyFreqLog                 = 7                                         {product} {default}
     intx Tier0ProfilingStartPercentage            = 200                                       {product} {default}
     intx Tier23InlineeNotifyFreqLog               = 20                                        {product} {default}
     intx Tier2BackEdgeThreshold                   = 0                                         {product} {default}
     intx Tier2BackedgeNotifyFreqLog               = 14                                        {product} {default}
     intx Tier2CompileThreshold                    = 0                                         {product} {default}
     intx Tier2InvokeNotifyFreqLog                 = 11                                        {product} {default}
     intx Tier3AOTBackEdgeThreshold                = 120000                                    {product} {default}
     intx Tier3AOTCompileThreshold                 = 15000                                     {product} {default}
     intx Tier3AOTInvocationThreshold              = 10000                                     {product} {default}
     intx Tier3AOTMinInvocationThreshold           = 1000                                      {product} {default}
     intx Tier3BackEdgeThreshold                   = 60000                                     {product} {default}
     intx Tier3BackedgeNotifyFreqLog               = 13                                        {product} {default}
     intx Tier3CompileThreshold                    = 2000                                      {product} {default}
     intx Tier3DelayOff                            = 2                                         {product} {default}
     intx Tier3DelayOn                             = 5                                         {product} {default}
     intx Tier3InvocationThreshold                 = 200                                       {product} {default}
     intx Tier3InvokeNotifyFreqLog                 = 10                                        {product} {default}
     intx Tier3LoadFeedback                        = 5                                         {product} {default}
     intx Tier3MinInvocationThreshold              = 100                                       {product} {default}
     intx Tier4BackEdgeThreshold                   = 40000                                     {product} {default}
     intx Tier4CompileThreshold                    = 15000                                     {product} {default}
     intx Tier4InvocationThreshold                 = 5000                                      {product} {default}
     intx Tier4LoadFeedback                        = 3                                         {product} {default}
     intx Tier4MinInvocationThreshold              = 600                                       {product} {default}
     bool TieredCompilation                        = true                                   {pd product} {default}
     intx TieredCompileTaskTimeout                 = 50                                        {product} {default}
     intx TieredRateUpdateMaxTime                  = 25                                        {product} {default}
     intx TieredRateUpdateMinTime                  = 1                                         {product} {default}
     intx TieredStopAtLevel                        = 4                                         {product} {default}
     bool TimeLinearScan                           = false                                  {C1 product} {default}
     bool TraceCompilerThreads                     = false                                  {diagnostic} {default}
    ccstr TraceJVMTI                               =                                           {product} {default}
     bool TraceJVMTIObjectTagging                  = false                                  {diagnostic} {default}
     bool TraceNMethodInstalls                     = false                                  {diagnostic} {default}
     bool TraceSpilling                            = false                               {C2 diagnostic} {default}
     bool TraceSuspendWaitFailures                 = false                                     {product} {default}
     bool TraceTypeProfile                         = false                               {C2 diagnostic} {default}
     intx TrackedInitializationLimit               = 50                                     {C2 product} {default}
     bool TransmitErrorReport                      = false                                     {product} {default}
     bool TrapBasedNullChecks                      = false                                  {pd product} {default}
     bool TrapBasedRangeChecks                     = false                               {C2 pd product} {default}
     intx TypeProfileArgsLimit                     = 2                                         {product} {default}
    uintx TypeProfileLevel                         = 111                                    {pd product} {default}
     intx TypeProfileMajorReceiverPercent          = 90                                     {C2 product} {default}
     intx TypeProfileParmsLimit                    = 2                                         {product} {default}
     intx TypeProfileWidth                         = 2                                         {product} {default}
     intx UnguardOnExecutionViolation              = 0                                         {product} {default}
     bool UnlinkSymbolsALot                        = false                                     {product} {default}
     bool UnlockDiagnosticVMOptions                = true                                   {diagnostic} {command line}
     bool UseAES                                   = true                                      {product} {default}
     bool UseAESCTRIntrinsics                      = true                                   {diagnostic} {default}
     bool UseAESIntrinsics                         = true                                   {diagnostic} {default}
     bool UseAOTStrictLoading                      = false                                  {diagnostic} {default}
     intx UseAVX                                   = 3                                    {ARCH product} {default}
     bool UseAdaptiveGCBoundary                    = false                                     {product} {default}
     bool UseAdaptiveGenerationSizePolicyAtMajorCollection  = true                             {product} {default}
     bool UseAdaptiveGenerationSizePolicyAtMinorCollection  = true                             {product} {default}
     bool UseAdaptiveNUMAChunkSizing               = true                                      {product} {default}
     bool UseAdaptiveSizeDecayMajorGCCost          = true                                      {product} {default}
     bool UseAdaptiveSizePolicy                    = true                                      {product} {default}
     bool UseAdaptiveSizePolicyFootprintGoal       = true                                      {product} {default}
     bool UseAdaptiveSizePolicyWithSystemGC        = false                                     {product} {default}
     bool UseAddressNop                            = true                                 {ARCH product} {default}
     bool UseAdler32Intrinsics                     = false                                  {diagnostic} {default}
     bool UseBASE64Intrinsics                      = true                                      {product} {default}
     bool UseBMI1Instructions                      = true                                 {ARCH product} {default}
     bool UseBMI2Instructions                      = true                                 {ARCH product} {default}
     bool UseBiasedLocking                         = true                                      {product} {default}
     bool UseBimorphicInlining                     = true                                   {C2 product} {default}
      int UseBootstrapCallInfo                     = 1                                      {diagnostic} {default}
     bool UseCLMUL                                 = true                                 {ARCH product} {default}
     bool UseCMSBestFit                            = true                                      {product} {default}
     bool UseCMSInitiatingOccupancyOnly            = false                                     {product} {default}
     bool UseCMoveUnconditionally                  = false                                  {C2 product} {default}
     bool UseCRC32CIntrinsics                      = true                                   {diagnostic} {default}
     bool UseCRC32Intrinsics                       = true                                   {diagnostic} {default}
     bool UseCharacterCompareIntrinsics            = false                               {C2 diagnostic} {default}
     bool UseCodeAging                             = true                                      {product} {default}
     bool UseCodeCacheFlushing                     = true                                      {product} {default}
     bool UseCompiler                              = true                                      {product} {default}
     bool UseCompressedClassPointers               = true                                 {lp64_product} {ergonomic}
     bool UseCompressedOops                        = true                                 {lp64_product} {ergonomic}
     bool UseConcMarkSweepGC                       = false                                     {product} {default}
     bool UseCondCardMark                          = false                                     {product} {default}
     bool UseContainerSupport                      = true                                      {product} {default}
     bool UseCopySignIntrinsic                     = false                                  {diagnostic} {default}
     bool UseCountLeadingZerosInstruction          = true                                 {ARCH product} {default}
     bool UseCountTrailingZerosInstruction         = true                                 {ARCH product} {default}
     bool UseCountedLoopSafepoints                 = true                                   {C2 product} {default}
     bool UseCounterDecay                          = true                                      {product} {default}
     bool UseCpuAllocPath                          = false                                  {diagnostic} {default}
     bool UseDivMod                                = true                                   {C2 product} {default}
     bool UseDynamicNumberOfCompilerThreads        = true                                      {product} {default}
     bool UseDynamicNumberOfGCThreads              = true                                      {product} {default}
     bool UseFMA                                   = true                                      {product} {default}
     bool UseFPUForSpilling                        = true                                   {C2 product} {default}
     bool UseFastJNIAccessors                      = true                                      {product} {default}
     bool UseFastStosb                             = true                                 {ARCH product} {default}
     bool UseG1GC                                  = true                                      {product} {command line}
     bool UseGCOverheadLimit                       = true                                      {product} {default}
     bool UseGCTaskAffinity                        = false                                     {product} {default}
     bool UseGHASHIntrinsics                       = true                                   {diagnostic} {default}
     bool UseHeavyMonitors                         = false                                     {product} {default}
     bool UseHugeTLBFS                             = false                                     {product} {default}
     bool UseImplicitStableValues                  = true                                {C2 diagnostic} {default}
     bool UseIncDec                                = true                              {ARCH diagnostic} {default}
     bool UseInlineCaches                          = true                                      {product} {default}
     bool UseInlineDepthForSpeculativeTypes        = true                                {C2 diagnostic} {default}
     bool UseInterpreter                           = true                                      {product} {default}
     bool UseJumpTables                            = true                                   {C2 product} {default}
     bool UseLWPSynchronization                    = true                                      {product} {default}
     bool UseLargePages                            = false                                  {pd product} {default}
     bool UseLargePagesInMetaspace                 = false                                     {product} {default}
     bool UseLargePagesIndividualAllocation        = false                                  {pd product} {default}
     bool UseLegacyJNINameEscaping                 = false                                     {product} {default}
     bool UseLibmIntrinsic                         = true                              {ARCH diagnostic} {default}
     bool UseLinuxPosixThreadCPUClocks             = true                                      {product} {default}
     bool UseLoopCounter                           = true                                      {product} {default}
     bool UseLoopInvariantCodeMotion               = true                                   {C1 product} {default}
     bool UseLoopPredicate                         = true                                   {C2 product} {default}
     bool UseMathExactIntrinsics                   = true                                {C2 diagnostic} {default}
     bool UseMaximumCompactionOnSystemGC           = true                                      {product} {default}
     bool UseMembar                                = true                                   {pd product} {default}
     bool UseMontgomeryMultiplyIntrinsic           = true                                {C2 diagnostic} {default}
     bool UseMontgomerySquareIntrinsic             = true                                {C2 diagnostic} {default}
     bool UseMulAddIntrinsic                       = true                                {C2 diagnostic} {default}
     bool UseMultiplyToLenIntrinsic                = true                                {C2 diagnostic} {default}
     bool UseNUMA                                  = false                                     {product} {default}
     bool UseNUMAInterleaving                      = false                                     {product} {default}
     bool UseNewCode                               = false                                  {diagnostic} {default}
     bool UseNewCode2                              = false                                  {diagnostic} {default}
     bool UseNewCode3                              = false                                  {diagnostic} {default}
     bool UseNewLongLShift                         = false                                {ARCH product} {default}
     bool UseOSErrorReporting                      = false                                  {pd product} {default}
     bool UseOnStackReplacement                    = true                                   {pd product} {default}
     bool UseOnlyInlinedBimorphic                  = true                                   {C2 product} {default}
     bool UseOprofile                              = false                                     {product} {default}
     bool UseOptoBiasInlining                      = true                                   {C2 product} {default}
     bool UsePSAdaptiveSurvivorSizePolicy          = true                                      {product} {default}
     bool UseParallelGC                            = false                                     {product} {default}
     bool UseParallelOldGC                         = false                                     {product} {default}
     bool UsePerfData                              = true                                      {product} {default}
     bool UsePopCountInstruction                   = true                                      {product} {default}
     bool UseProfiledLoopPredicate                 = true                                   {C2 product} {default}
     bool UseRDPCForConstantTableBase              = false                                  {C2 product} {default}
     bool UseRTMDeopt                              = false                                {ARCH product} {default}
     bool UseRTMLocking                            = false                                {ARCH product} {default}
     bool UseSHA                                   = true                                      {product} {default}
     bool UseSHA1Intrinsics                        = false                                  {diagnostic} {default}
     bool UseSHA256Intrinsics                      = true                                   {diagnostic} {default}
     bool UseSHA512Intrinsics                      = true                                   {diagnostic} {default}
     bool UseSHM                                   = false                                     {product} {default}
     intx UseSSE                                   = 4                                         {product} {default}
     bool UseSSE42Intrinsics                       = true                                 {ARCH product} {default}
     bool UseSemaphoreGCThreadsSynchronization     = true                                   {diagnostic} {default}
     bool UseSerialGC                              = false                                     {product} {default}
     bool UseSharedSpaces                          = false                                     {product} {default}
     bool UseShenandoahGC                          = false                                     {product} {default}
     bool UseSignalChaining                        = true                                      {product} {default}
     bool UseSignumIntrinsic                       = false                                  {diagnostic} {default}
     bool UseSquareToLenIntrinsic                  = true                                {C2 diagnostic} {default}
     bool UseStoreImmI16                           = false                                {ARCH product} {default}
     bool UseStringDeduplication                   = false                                     {product} {default}
     bool UseSubwordForMaxVector                   = true                                   {C2 product} {default}
     bool UseSuperWord                             = true                                   {C2 product} {default}
     bool UseSwitchProfiling                       = true                                   {diagnostic} {default}
     bool UseTLAB                                  = true                                   {pd product} {default}
     bool UseThreadPriorities                      = true                                   {pd product} {default}
     bool UseTransparentHugePages                  = false                                     {product} {default}
     bool UseTypeProfile                           = true                                      {product} {default}
     bool UseTypeSpeculation                       = true                                   {C2 product} {default}
     bool UseUnalignedAccesses                     = true                                   {diagnostic} {default}
     bool UseUnalignedLoadStores                   = true                                 {ARCH product} {default}
     bool UseVectorCmov                            = false                                  {C2 product} {default}
     bool UseVectorizedMismatchIntrinsic           = true                                   {diagnostic} {default}
     bool UseXMMForArrayCopy                       = true                                      {product} {default}
     bool UseXMMForObjInit                         = false                                {ARCH product} {default}
     bool UseXmmI2D                                = false                                {ARCH product} {default}
     bool UseXmmI2F                                = false                                {ARCH product} {default}
     bool UseXmmLoadAndClearUpper                  = true                                 {ARCH product} {default}
     bool UseXmmRegToRegMoveAll                    = true                                 {ARCH product} {default}
     bool VMThreadHintNoPreempt                    = false                                     {product} {default}
     intx VMThreadPriority                         = -1                                        {product} {default}
     intx VMThreadStackSize                        = 1024                                   {pd product} {default}
     intx ValueMapInitialSize                      = 11                                     {C1 product} {default}
     intx ValueMapMaxLoopSize                      = 8                                      {C1 product} {default}
     intx ValueSearchLimit                         = 1000                                   {C2 product} {default}
     bool VerifyAdapterCalls                       = false                                  {diagnostic} {default}
     bool VerifyAfterGC                            = false                                  {diagnostic} {default}
     bool VerifyBeforeExit                         = false                                  {diagnostic} {default}
     bool VerifyBeforeGC                           = false                                  {diagnostic} {default}
     bool VerifyBeforeIteration                    = false                                  {diagnostic} {default}
     bool VerifyDuringGC                           = false                                  {diagnostic} {default}
     bool VerifyDuringStartup                      = false                                  {diagnostic} {default}
     intx VerifyGCLevel                            = 0                                      {diagnostic} {default}
    uintx VerifyGCStartAt                          = 0                                      {diagnostic} {default}
ccstrlist VerifyGCType                             =                                        {diagnostic} {default}
     bool VerifyMergedCPBytecodes                  = true                                      {product} {default}
     bool VerifyMethodHandles                      = false                                  {diagnostic} {default}
     bool VerifyObjectStartArray                   = true                                   {diagnostic} {default}
     bool VerifyRememberedSets                     = false                                  {diagnostic} {default}
     bool VerifySharedSpaces                       = false                                     {product} {default}
     bool VerifyStringTableAtExit                  = false                                  {diagnostic} {default}
ccstrlist VerifySubSet                             =                                        {diagnostic} {default}
     bool WhiteBoxAPI                              = false                                  {diagnostic} {default}
    uintx YoungGenerationSizeIncrement             = 20                                        {product} {default}
    uintx YoungGenerationSizeSupplement            = 80                                        {product} {default}
    uintx YoungGenerationSizeSupplementDecay       = 8                                         {product} {default}
   size_t YoungPLABSize                            = 4096                                      {product} {default}
   double ZAllocationSpikeTolerance                = 2.000000                                  {product} {default}
     uint ZCollectionInterval                      = 0                                         {product} {default}
     bool ZConcurrentJNIWeakGlobalHandles          = true                                   {diagnostic} {default}
     bool ZConcurrentStringTable                   = true                                   {diagnostic} {default}
     bool ZConcurrentVMWeakHandles                 = true                                   {diagnostic} {default}
   double ZFragmentationLimit                      = 25.000000                                 {product} {default}
   size_t ZMarkStacksMax                           = 8589934592                                {product} {default}
     bool ZOptimizeLoadBarriers                    = true                                   {diagnostic} {default}
    ccstr ZPath                                    =                                           {product} {default}
     bool ZProactive                               = true                                   {diagnostic} {default}
     bool ZStallOnOutOfMemory                      = true                                      {product} {default}
     bool ZStatisticsForceTrace                    = false                                  {diagnostic} {default}
     uint ZStatisticsInterval                      = 10                                        {product} {default}
     bool ZSymbolTableUnloading                    = false                                  {diagnostic} {default}
     bool ZUnmapBadViews                           = false                                  {diagnostic} {default}
     bool ZVerifyForwarding                        = false                                  {diagnostic} {default}
     bool ZVerifyMarking                           = false                                  {diagnostic} {default}
     bool ZWeakRoots                               = true                                   {diagnostic} {default}
     bool ZeroTLAB                                 = false                                     {product} {default}
log4j:WARN No such property [fields] in org.apache.log4j.EnhancedPatternLayout.
[INFO] 2023-11-08 14:50:52,514 [main] io.confluent.agent.monitoring.DiskUsage premain - DiskUsage Agent: config : /opt/confluentinc/etc/kafka/disk-usage-agent.properties
[INFO] 2023-11-08 14:50:52,651 [main] io.confluent.agent.monitoring.DiskUsage premain - DiskUsage Agent: Registering object :io.confluent.caas:type=VolumeMetrics, service=kafka, dir=data for class : io.confluent.agent.monitoring.Volume
[INFO] 2023-11-08 14:50:52,655 [main] io.confluent.agent.monitoring.DiskUsage premain - DiskUsage Agent: Ping Volume{store=/mnt/data/data0 (/dev/sdd), total=10464022528, used=24576, available=10447220736, percentUsed=2.3486187968573916E-4, percentAvailable=99.83943276158818, mountpoint='/mnt/data/data0', deviceName='/dev/sdd'}
I> No access restrictor found, access to any MBean is allowed
Jolokia: Agent started with URL http://10.40.0.17:7777/jolokia/
[INFO] 2023-11-08 14:50:53,002 [main] kafka.utils.Log4jControllerRegistration$ <clinit> - Registered kafka:type=kafka.Log4jController MBean
[INFO] 2023-11-08 14:50:53,854 [main] org.apache.zookeeper.common.X509Util <clinit> - Setting -D jdk.tls.rejectClientInitiatedRenegotiation=true to disable client-initiated TLS renegotiation
[INFO] 2023-11-08 14:50:53,963 [main] org.apache.kafka.common.utils.LoggingSignalHandler register - Registered signal handlers for TERM, INT, HUP
[INFO] 2023-11-08 14:50:53,966 [main] kafka.server.KafkaServer info - starting
[INFO] 2023-11-08 14:50:53,967 [main] kafka.server.KafkaServer info - FIPS mode enabled: false
[INFO] 2023-11-08 14:50:53,967 [main] kafka.server.KafkaServer info - Connecting to zookeeper on zookeeper.confluent.svc.cluster.local:2182/kafka-confluent
[INFO] 2023-11-08 14:50:53,985 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Initializing a new session to zookeeper.confluent.svc.cluster.local:2182.
[INFO] 2023-11-08 14:50:53,991 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:zookeeper.version=3.6.3--6401e4ad2087061bc6b9f80dec2d69f2e3c8660a, built on 04/08/2021 16:35 GMT
[INFO] 2023-11-08 14:50:53,992 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:host.name=kafka-0.kafka.confluent.svc.cluster.local
[INFO] 2023-11-08 14:50:53,992 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.version=11.0.14.1
[INFO] 2023-11-08 14:50:53,992 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.vendor=Azul Systems, Inc.
[INFO] 2023-11-08 14:50:53,992 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.home=/usr/lib/jvm/zulu11-ca
[INFO] 2023-11-08 14:50:53,992 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.class.path=/usr/bin/../ce-broker-plugins/build/libs/*:/usr/bin/../ce-broker-plugins/build/dependant-libs/*:/usr/bin/../ce-auth-providers/build/libs/*:/usr/bin/../ce-auth-providers/build/dependant-libs/*:/usr/bin/../ce-rest-server/build/libs/*:/usr/bin/../ce-rest-server/build/dependant-libs/*:/usr/bin/../ce-audit/build/libs/*:/usr/bin/../ce-audit/build/dependant-libs/*:/usr/bin/../share/java/kafka/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/kafka/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/kafka/broker-plugins-7.1.0-ce.jar:/usr/bin/../share/java/kafka/scala-library-2.13.6.jar:/usr/bin/../share/java/kafka/gax-httpjson-0.89.1.jar:/usr/bin/../share/java/kafka/aws-java-sdk-core-1.11.988.jar:/usr/bin/../share/java/kafka/auto-service-annotations-1.0-rc7.jar:/usr/bin/../share/java/kafka/api-common-2.0.2.jar:/usr/bin/../share/java/kafka/zipkin-2.23.2.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/kafka/netty-codec-haproxy-4.1.73.Final.jar:/usr/bin/../share/java/kafka/logredactor-1.0.10.jar:/usr/bin/../share/java/kafka/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-ce-logs-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jersey-server-2.34.jar:/usr/bin/../share/java/kafka/netty-codec-xml-4.1.73.Final.jar:/usr/bin/../share/java/kafka/auto-value-annotations-1.8.2.jar:/usr/bin/../share/java/kafka/annotations-13.0.jar:/usr/bin/../share/java/kafka/core-1.54.0.0.jar:/usr/bin/../share/java/kafka/gax-2.4.1.jar:/usr/bin/../share/java/kafka/maven-artifact-3.8.1.jar:/usr/bin/../share/java/kafka/commons-logging-1.2.jar:/usr/bin/../share/java/kafka/oauth2-oidc-sdk-9.7.jar:/usr/bin/../share/java/kafka/jackson-dataformat-xml-2.12.3.jar:/usr/bin/../share/java/kafka/google-oauth-client-1.32.1.jar:/usr/bin/../share/java/kafka/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/kafka/reactor-netty-http-1.0.7.jar:/usr/bin/../share/java/kafka/kafka-metadata-7.1.0-ce.jar:/usr/bin/../share/java/kafka/kafka-streams-scala_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/proto-google-iam-v1-1.1.0.jar:/usr/bin/../share/java/kafka/netty-handler-proxy-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final-linux-aarch_64.jar:/usr/bin/../share/java/kafka/netty-codec-http2-4.1.73.Final.jar:/usr/bin/../share/java/kafka/bc-fips-1.0.2.jar:/usr/bin/../share/java/kafka/stax2-api-4.2.1.jar:/usr/bin/../share/java/kafka/reactor-netty-core-1.0.7.jar:/usr/bin/../share/java/kafka/httpclient-4.5.13.jar:/usr/bin/../share/java/kafka/kafka-streams-7.1.0-ce.jar:/usr/bin/../share/java/kafka/brave-instrumentation-http-5.13.3.jar:/usr/bin/../share/java/kafka/ion-java-1.0.2.jar:/usr/bin/../share/java/kafka/netty-codec-stomp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/kafka-raft-7.1.0-ce.jar:/usr/bin/../share/java/kafka/javassist-3.27.0-GA.jar:/usr/bin/../share/java/kafka/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/kafka/netty-transport-sctp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/joda-time-2.9.9.jar:/usr/bin/../share/java/kafka/okio-jvm-3.0.0.jar:/usr/bin/../share/java/kafka/confluent-licensing-new-7.1.0-ce.jar:/usr/bin/../share/java/kafka/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/checker-qual-3.8.0.jar:/usr/bin/../share/java/kafka/azure-core-http-netty-1.10.1.jar:/usr/bin/../share/java/kafka/netty-codec-dns-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-aarch_64.jar:/usr/bin/../share/java/kafka/client-java-14.0.0.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.21.jar:/usr/bin/../share/java/kafka/kafka-streams-examples-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jbcrypt-0.4.jar:/usr/bin/../share/java/kafka/kafka-log4j-appender-7.1.0-ce.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/kafka/annotations-3.0.1.jar:/usr/bin/../share/java/kafka/scala-reflect-2.13.6.jar:/usr/bin/../share/java/kafka/kafka-shell-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jcip-annotations-1.0.jar:/usr/bin/../share/java/kafka/httpcore-4.4.14.jar:/usr/bin/../share/java/kafka/jaxb-api-2.3.0.jar:/usr/bin/../share/java/kafka/kafka_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jna-platform-5.6.0.jar:/usr/bin/../share/java/kafka/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/kafka/internal-rest-server-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/bin/../share/java/kafka/simpleclient_common-0.12.0.jar:/usr/bin/../share/java/kafka/netty-transport-native-epoll-4.1.73.Final.jar:/usr/bin/../share/java/kafka/confluent-audit-7.1.0-ce.jar:/usr/bin/../share/java/kafka/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/kafka/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka/netty-tcnative-classes-2.0.46.Final.jar:/usr/bin/../share/java/kafka/database-2.1.4.jar:/usr/bin/../share/java/kafka/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/kafka/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/kafka/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/kafka/activation-1.1.1.jar:/usr/bin/../share/java/kafka/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/kafka/opencensus-contrib-http-util-0.28.0.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-classes-macos-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-json-7.1.0-ce.jar:/usr/bin/../share/java/kafka/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/kafka/auth-providers-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/bin/../share/java/kafka/nimbus-jose-jwt-9.9.3.jar:/usr/bin/../share/java/kafka/azure-core-1.18.0.jar:/usr/bin/../share/java/kafka/aws-java-sdk-sts-1.11.988.jar:/usr/bin/../share/java/kafka/plexus-utils-3.2.1.jar:/usr/bin/../share/java/kafka/google-auth-library-credentials-1.1.0.jar:/usr/bin/../share/java/kafka/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/kafka/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/kafka/msal4j-1.10.1.jar:/usr/bin/../share/java/kafka/rbac-7.1.0-ce.jar:/usr/bin/../share/java/kafka/netty-handler-4.1.73.Final.jar:/usr/bin/../share/java/kafka/javax.json-1.0.4.jar:/usr/bin/../share/java/kafka/hk2-api-2.6.1.jar:/usr/bin/../share/java/kafka/zipkin-reporter-2.16.3.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-transforms-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jose4j-0.7.8.jar:/usr/bin/../share/java/kafka/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka/kafka.jar:/usr/bin/../share/java/kafka/jmespath-java-1.11.988.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-common-1.5.31.jar:/usr/bin/../share/java/kafka/google-cloud-core-http-2.1.3.jar:/usr/bin/../share/java/kafka/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jsr305-3.0.1.jar:/usr/bin/../share/java/kafka/flatbuffers-java-1.9.0.jar:/usr/bin/../share/java/kafka/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/kafka/netty-codec-redis-4.1.73.Final.jar:/usr/bin/../share/java/kafka/connect-runtime-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/kafka/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/annotations-15.0.jar:/usr/bin/../share/java/kafka/grpc-context-1.40.1.jar:/usr/bin/../share/java/kafka/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/confluent-log4j-1.2.17-cp10.jar:/usr/bin/../share/java/kafka/zipkin-reporter-brave-2.16.3.jar:/usr/bin/../share/java/kafka/simpleclient-0.12.0.jar:/usr/bin/../share/java/kafka/kafka-tools-7.1.0-ce.jar:/usr/bin/../share/java/kafka/ce-sbk_2.13-7.1.0-ce.jar:/usr/bin/../share/java/kafka/aws-java-sdk-s3-1.11.988.jar:/usr/bin/../share/java/kafka/reflections-0.9.12.jar:/usr/bin/../share/java/kafka/woodstox-core-6.2.4.jar:/usr/bin/../share/java/kafka/netty-codec-memcache-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_otel-0.12.0.jar:/usr/bin/../share/java/kafka/netty-tcnative-boringssl-static-2.0.46.Final.jar:/usr/bin/../share/java/kafka/proto-google-common-protos-2.5.0.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.30.jar:/usr/bin/../share/java/kafka/google-cloud-storage-2.1.2.jar:/usr/bin/../share/java/kafka/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/kafka/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/kafka/brave-5.13.3.jar:/usr/bin/../share/java/kafka/azure-storage-internal-avro-12.0.5.jar:/usr/bin/../share/java/kafka/google-auth-library-oauth2-http-1.1.0.jar:/usr/bin/../share/java/kafka/netty-transport-udt-4.1.73.Final.jar:/usr/bin/../share/java/kafka/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka/kafka-server-common-7.1.0-ce.jar:/usr/bin/../share/java/kafka/google-api-client-1.32.1.jar:/usr/bin/../share/java/kafka/auto-common-0.10.jar:/usr/bin/../share/java/kafka/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/kafka/commons-cli-1.4.jar:/usr/bin/../share/java/kafka/reactor-core-3.4.6.jar:/usr/bin/../share/java/kafka/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/bin/../share/java/kafka/telemetry-client-1.745.0.jar:/usr/bin/../share/java/kafka/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka/KeePassJava2-2.1.4.jar:/usr/bin/../share/java/kafka/google-cloud-core-2.1.3.jar:/usr/bin/../share/java/kafka/confluent-serializers-new-7.1.0-ce.jar:/usr/bin/../share/java/kafka/auto-service-1.0-rc7.jar:/usr/bin/../share/java/kafka/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/slf4j-api-1.7.32.jar:/usr/bin/../share/java/kafka/netty-codec-mqtt-4.1.73.Final.jar:/usr/bin/../share/java/kafka/okhttp-4.9.1.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-1.5.31.jar:/usr/bin/../share/java/kafka/threetenbp-1.5.1.jar:/usr/bin/../share/java/kafka/minimal-json-0.9.5.jar:/usr/bin/../share/java/kafka/checker-qual-3.5.0.jar:/usr/bin/../share/java/kafka/netty-all-4.1.73.Final.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_otel_agent-0.12.0.jar:/usr/bin/../share/java/kafka/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/kafka/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka/jackson-dataformat-cbor-2.12.3.jar:/usr/bin/../share/java/kafka/KeePassJava2-simple-2.1.4.jar:/usr/bin/../share/java/kafka/telemetry-api-1.745.0.jar:/usr/bin/../share/java/kafka/paranamer-2.8.jar:/usr/bin/../share/java/kafka/google-http-client-apache-v2-1.40.0.jar:/usr/bin/../share/java/kafka/tink-1.6.0.jar:/usr/bin/../share/java/kafka/bcpkix-fips-1.0.3.jar:/usr/bin/../share/java/kafka/bctls-fips-1.0.10.jar:/usr/bin/../share/java/kafka/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/bin/../share/java/kafka/client-java-api-14.0.0.jar:/usr/bin/../share/java/kafka/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka/commons-lang3-3.11.jar:/usr/bin/../share/java/kafka/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/kafka/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/kafka/google-http-client-1.40.0.jar:/usr/bin/../share/java/kafka/j2objc-annotations-1.3.jar:/usr/bin/../share/java/kafka/icu4j-61.1.jar:/usr/bin/../share/java/kafka/guava-30.0-jre.jar:/usr/bin/../share/java/kafka/jline-3.12.1.jar:/usr/bin/../share/java/kafka/netty-handler-proxy-4.1.65.Final.jar:/usr/bin/../share/java/kafka/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/kafka/azure-identity-1.3.3.jar:/usr/bin/../share/java/kafka/slf4j-log4j12-1.7.30.jar:/usr/bin/../share/java/kafka/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/snakeyaml-1.29.jar:/usr/bin/../share/java/kafka/commons-compress-1.21.jar:/usr/bin/../share/java/kafka/azure-storage-blob-12.12.0.jar:/usr/bin/../share/java/kafka/commons-collections4-4.4.jar:/usr/bin/../share/java/kafka/azure-storage-common-12.12.0.jar:/usr/bin/../share/java/kafka/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/kafka/google-http-client-appengine-1.40.0.jar:/usr/bin/../share/java/kafka/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/kafka/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/kafka/netty-transport-rxtx-4.1.73.Final.jar:/usr/bin/../share/java/kafka/google-api-services-cloudkms-v1-rev108-1.25.0.jar:/usr/bin/../share/java/kafka/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka/httpcore-4.4.13.jar:/usr/bin/../share/java/kafka/jackson-databind-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/kafka/gson-fire-1.8.5.jar:/usr/bin/../share/java/kafka/reactive-streams-1.0.3.jar:/usr/bin/../share/java/kafka/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/connect-mirror-client-7.1.0-ce.jar:/usr/bin/../share/java/kafka/reactor-netty-http-brave-1.0.7.jar:/usr/bin/../share/java/kafka/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/kafka/reactor-netty-1.0.7.jar:/usr/bin/../share/java/kafka/jcip-annotations-1.0-1.jar:/usr/bin/../share/java/kafka/hk2-locator-2.6.1.jar:/usr/bin/../share/java/kafka/netty-codec-smtp-4.1.73.Final.jar:/usr/bin/../share/java/kafka/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/kafka/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/kafka/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/kafka/simpleclient_httpserver-0.12.0.jar:/usr/bin/../share/java/kafka/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/kafka/ST4-4.3.jar:/usr/bin/../share/java/kafka/kafka-storage-7.1.0-ce.jar:/usr/bin/../share/java/kafka/rocksdbjni-6.22.1.1.jar:/usr/bin/../share/java/kafka/hk2-utils-2.6.1.jar:/usr/bin/../share/java/kafka/commons-codec-1.15.jar:/usr/bin/../share/java/kafka/KeePassJava2-dom-2.1.4.jar:/usr/bin/../share/java/kafka/netty-codec-socks-4.1.73.Final.jar:/usr/bin/../share/java/kafka/client-java-proto-14.0.0.jar:/usr/bin/../share/java/kafka/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/kafka/jna-5.6.0.jar:/usr/bin/../share/java/kafka/google-api-services-storage-v1-rev20210127-1.32.1.jar:/usr/bin/../share/java/kafka/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka/KeePassJava2-jaxb-2.1.4.jar:/usr/bin/../share/java/kafka/KeePassJava2-kdb-2.1.4.jar:/usr/bin/../share/java/kafka/simpleclient_tracer_common-0.12.0.jar:/usr/bin/../share/java/kafka/logging-interceptor-4.9.1.jar:/usr/bin/../share/java/kafka/kafka-storage-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jersey-hk2-2.34.jar:/usr/bin/../share/java/kafka/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/kafka/aalto-xml-1.0.0.jar:/usr/bin/../share/java/kafka/KeePassJava2-kdbx-2.1.4.jar:/usr/bin/../share/java/kafka/antlr4-4.9.2.jar:/usr/bin/../share/java/kafka/connect-mirror-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jsr305-3.0.2.jar:/usr/bin/../share/java/kafka/google-http-client-jackson2-1.40.0.jar:/usr/bin/../share/java/kafka/lang-tag-1.5.jar:/usr/bin/../share/java/kafka/connector-datapreview-extension-7.1.0-ce.jar:/usr/bin/../share/java/kafka/opencensus-api-0.28.0.jar:/usr/bin/../share/java/kafka/connect-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/google-http-client-gson-1.40.0.jar:/usr/bin/../share/java/kafka/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/tink-gcpkms-1.6.0.jar:/usr/bin/../share/java/kafka/swagger-annotations-1.6.3.jar:/usr/bin/../share/java/kafka/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/kafka/trogdor-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jersey-common-2.34.jar:/usr/bin/../share/java/kafka/aws-java-sdk-kms-1.11.988.jar:/usr/bin/../share/java/kafka/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka/logredactor-metrics-1.0.10.jar:/usr/bin/../share/java/kafka/re2j-1.6.jar:/usr/bin/../share/java/kafka/protobuf-java-3.17.3.jar:/usr/bin/../share/java/kafka/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/kafka/jersey-client-2.34.jar:/usr/bin/../share/java/kafka/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/kafka/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/kafka/commons-math3-3.6.1.jar:/usr/bin/../share/java/kafka/asm-9.1.jar:/usr/bin/../share/java/kafka/connect-basic-auth-extension-7.1.0-ce.jar:/usr/bin/../share/java/kafka/failureaccess-1.0.1.jar:/usr/bin/../share/java/kafka/accessors-smart-2.4.7.jar:/usr/bin/../share/java/kafka/kafka-streams-test-utils-7.1.0-ce.jar:/usr/bin/../share/java/kafka/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka/content-type-2.1.jar:/usr/bin/../share/java/kafka/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/kafka/netty-resolver-dns-native-macos-4.1.73.Final-osx-aarch_64.jar:/usr/bin/../share/java/kafka/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/kafka/commons-io-2.11.0.jar:/usr/bin/../share/java/kafka/gson-2.8.6.jar:/usr/bin/../share/java/kafka/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/kafka/jackson-dataformat-properties-2.12.3.jar:/usr/bin/../share/java/kafka/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/kafka/snakeyaml-1.27.jar:/usr/bin/../share/java/kafka/json-smart-2.4.7.jar:/usr/bin/../share/java/kafka/error_prone_annotations-2.3.4.jar:/usr/bin/../share/java/kafka/msal4j-persistence-extension-1.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/confluent-metadata-service/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-server-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/auto-value-annotations-1.8.1.jar:/usr/bin/../share/java/confluent-metadata-service/netty-handler-proxy-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jcommander-1.72.jar:/usr/bin/../share/java/confluent-metadata-service/jul-to-slf4j-1.7.30.jar:/usr/bin/../share/java/confluent-metadata-service/authorizer-client-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jbcrypt-0.4.jar:/usr/bin/../share/java/confluent-metadata-service/annotations-3.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/ce-kafka-http-server-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-epoll-4.1.73.Final-linux-x86_64.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-audit-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-metadata-service/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/confluent-metadata-service/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/commons-lang3-3.8.1.jar:/usr/bin/../share/java/confluent-metadata-service/auth-providers-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-classes-kqueue-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/javax.json-1.0.4.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/bsh-2.0b6.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-metadata-service/javax.servlet-api-4.0.1.jar:/usr/bin/../share/java/confluent-metadata-service/metrics-core-2.2.0.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-server-common-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-classes-epoll-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-client-1.745.0.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-api-1.745.0.jar:/usr/bin/../share/java/confluent-metadata-service/bcpkix-fips-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/bctls-fips-1.0.10.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-unix-common-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-api-server-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-metadata-service/netty-transport-native-kqueue-4.1.73.Final-osx-x86_64.jar:/usr/bin/../share/java/confluent-metadata-service/icu4j-61.1.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.el-3.0.3.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/confluent-metadata-service/confluent-security-plugins-common-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-metadata-service/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/ST4-4.3.jar:/usr/bin/../share/java/confluent-metadata-service/jose4j-0.7.2.jar:/usr/bin/../share/java/confluent-metadata-service/rbac-common-7.1.0.jar:/usr/bin/../share/java/confluent-metadata-service/netty-codec-socks-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/antlr4-4.9.2.jar:/usr/bin/../share/java/confluent-metadata-service/bc-fips-1.0.2.1.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-common-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-metadata-service/jetty-proxy-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-metadata-service/jersey-client-2.34.jar:/usr/bin/../share/java/confluent-metadata-service/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/confluent-metadata-service/concurrent-trees-2.6.1.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-metadata-service/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/confluent-metadata-service/gson-2.8.6.jar:/usr/bin/../share/java/confluent-metadata-service/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/confluent-metadata-service/jackson-dataformat-properties-2.12.3.jar:/usr/bin/../share/java/confluent-metadata-service/snakeyaml-1.27.jar:/usr/bin/../share/java/confluent-metadata-service/testng-6.14.3.jar:/usr/bin/../share/java/rest-utils/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/rest-utils/javax-websocket-server-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-xml-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/rest-utils/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/jersey-server-2.34.jar:/usr/bin/../share/java/rest-utils/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/rest-utils/guava-30.1.1-jre.jar:/usr/bin/../share/java/rest-utils/jetty-plus-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/rest-utils/javax-websocket-client-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-webapp-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/http2-common-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/classmate-1.3.4.jar:/usr/bin/../share/java/rest-utils/http2-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/websocket-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/rest-utils/checker-qual-3.8.0.jar:/usr/bin/../share/java/rest-utils/asm-9.2.jar:/usr/bin/../share/java/rest-utils/jetty-jaas-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/asm-analysis-9.2.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/rest-utils/jetty-alpn-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-jmx-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jaxb-api-2.3.0.jar:/usr/bin/../share/java/rest-utils/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/lz4-java-1.8.0.jar:/usr/bin/../share/java/rest-utils/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/rest-utils/activation-1.1.1.jar:/usr/bin/../share/java/rest-utils/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/rest-utils/javassist-3.25.0-GA.jar:/usr/bin/../share/java/rest-utils/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/rest-utils/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/rest-utils/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/rest-utils/websocket-api-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/hk2-api-2.6.1.jar:/usr/bin/../share/java/rest-utils/hibernate-validator-6.1.7.Final.jar:/usr/bin/../share/java/rest-utils/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/rest-utils/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/asm-tree-9.2.jar:/usr/bin/../share/java/rest-utils/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/rest-utils/websocket-common-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/asm-commons-9.2.jar:/usr/bin/../share/java/rest-utils/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/rest-utils/javax.websocket-api-1.0.jar:/usr/bin/../share/java/rest-utils/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/rest-utils/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/rest-utils/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/http2-hpack-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/rest-utils/jetty-annotations-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/rest-utils/j2objc-annotations-1.3.jar:/usr/bin/../share/java/rest-utils/jakarta.el-3.0.3.jar:/usr/bin/../share/java/rest-utils/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/rest-utils/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jackson-databind-2.12.3.jar:/usr/bin/../share/java/rest-utils/websocket-client-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/jetty-jndi-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/hk2-locator-2.6.1.jar:/usr/bin/../share/java/rest-utils/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/rest-utils/hk2-utils-2.6.1.jar:/usr/bin/../share/java/rest-utils/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/rest-utils/jackson-core-2.12.3.jar:/usr/bin/../share/java/rest-utils/jersey-hk2-2.34.jar:/usr/bin/../share/java/rest-utils/websocket-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/kafka-clients-7.1.0-ccs.jar:/usr/bin/../share/java/rest-utils/jsr305-3.0.2.jar:/usr/bin/../share/java/rest-utils/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jersey-common-2.34.jar:/usr/bin/../share/java/rest-utils/rest-utils-7.1.0.jar:/usr/bin/../share/java/rest-utils/jersey-client-2.34.jar:/usr/bin/../share/java/rest-utils/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/failureaccess-1.0.1.jar:/usr/bin/../share/java/rest-utils/jetty-alpn-java-server-9.4.44.v20210927.jar:/usr/bin/../share/java/rest-utils/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/rest-utils/javax.websocket-client-api-1.0.jar:/usr/bin/../share/java/rest-utils/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-common/build-tools-7.1.0.jar:/usr/bin/../share/java/confluent-common/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-common/slf4j-api-1.7.30.jar:/usr/bin/../share/java/confluent-common/common-metrics-7.1.0.jar:/usr/bin/../share/java/confluent-common/common-config-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/ce-kafka-http-server/javax-websocket-server-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-xml-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-server-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jboss-logging-3.3.2.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/guava-30.1.1-jre.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-plus-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/ce-kafka-http-server/javax-websocket-client-impl-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-webapp-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-common-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/classmate-1.3.4.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/checker-qual-3.8.0.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jaas-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-analysis-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-alpn-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jmx-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/ce-kafka-http-server-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jaxb-api-2.3.0.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/lz4-java-1.8.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/ce-kafka-http-server/activation-1.1.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/ce-kafka-http-server/javassist-3.25.0-GA.jar:/usr/bin/../share/java/ce-kafka-http-server/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-bean-validation-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/common-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.el-api-4.0.0.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-api-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-api-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/hibernate-validator-6.1.7.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-tree-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-common-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-http-server/asm-commons-9.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.websocket-api-1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/http2-hpack-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-annotations-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/ce-kafka-http-server/j2objc-annotations-1.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.el-3.0.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-databind-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-client-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-jndi-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-locator-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.servlet-api-3.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-http-server/hk2-utils-2.6.1.jar:/usr/bin/../share/java/ce-kafka-http-server/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-core-2.12.3.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-hk2-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/websocket-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jsr305-3.0.2.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-common-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/rest-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jersey-client-2.34.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/failureaccess-1.0.1.jar:/usr/bin/../share/java/ce-kafka-http-server/jetty-alpn-java-server-9.4.44.v20210927.jar:/usr/bin/../share/java/ce-kafka-http-server/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-http-server/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/ce-kafka-http-server/javax.websocket-client-api-1.0.jar:/usr/bin/../share/java/ce-kafka-http-server/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/ce-kafka-rest-servlet/ce-kafka-rest-servlet-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/lz4-java-1.8.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/common-utils-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/ce-kafka-rest-extensions-7.1.0.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/slf4j-api-1.7.30.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/ce-kafka-rest-extensions/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/kafka-rest-lib/scala-java8-compat_2.13-1.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/logredactor-1.0.10.jar:/usr/bin/../share/java/kafka-rest-lib/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/kafka-rest-lib/annotations-13.0.jar:/usr/bin/../share/java/kafka-rest-lib/guava-30.1.1-jre.jar:/usr/bin/../share/java/kafka-rest-lib/commons-logging-1.2.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-common-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-metadata-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-provider-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/okio-jvm-3.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/wire-schema-jvm-4.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/checker-qual-3.8.0.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-module-parameter-names-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/resilience4j-core-1.7.1.jar:/usr/bin/../share/java/kafka-rest-lib/lz4-java-1.8.0.jar:/usr/bin/../share/java/kafka-rest-lib/zookeeper-jute-3.6.3.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-schema-provider-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-guava-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/common-utils-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/wire-runtime-jvm-4.0.0.jar:/usr/bin/../share/java/kafka-rest-lib/org.everit.json.schema-1.12.2.jar:/usr/bin/../share/java/kafka-rest-lib/handy-uri-templates-2.1.8.jar:/usr/bin/../share/java/kafka-rest-lib/kotlinx-coroutines-core-1.3.7.jar:/usr/bin/../share/java/kafka-rest-lib/jose4j-0.7.8.jar:/usr/bin/../share/java/kafka-rest-lib/scala-collection-compat_2.13-2.4.4.jar:/usr/bin/../share/java/kafka-rest-lib/proto-google-common-protos-2.5.1.jar:/usr/bin/../share/java/kafka-rest-lib/scala-reflect-2.13.5.jar:/usr/bin/../share/java/kafka-rest-lib/kafka_2.13-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/confluent-log4j-1.2.17-cp10.jar:/usr/bin/../share/java/kafka-rest-lib/resilience4j-ratelimiter-1.7.1.jar:/usr/bin/../share/java/kafka-rest-lib/commons-validator-1.6.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/metrics-core-2.2.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-joda-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/commons-cli-1.4.jar:/usr/bin/../share/java/kafka-rest-lib/scala-logging_2.13-3.9.3.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-raft-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/zookeeper-3.6.3.jar:/usr/bin/../share/java/kafka-rest-lib/spotbugs-annotations-4.3.0.jar:/usr/bin/../share/java/kafka-rest-lib/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/kafka-rest-lib/minimal-json-0.9.5.jar:/usr/bin/../share/java/kafka-rest-lib/jopt-simple-5.0.4.jar:/usr/bin/../share/java/kafka-rest-lib/scala-library-2.13.5.jar:/usr/bin/../share/java/kafka-rest-lib/paranamer-2.8.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-server-common-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-schema-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/commons-collections-3.2.2.jar:/usr/bin/../share/java/kafka-rest-lib/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-module-scala_2.13-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/kafka-rest-lib/j2objc-annotations-1.3.jar:/usr/bin/../share/java/kafka-rest-lib/joda-time-2.10.8.jar:/usr/bin/../share/java/kafka-rest-lib/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/bin/../share/java/kafka-rest-lib/vavr-match-0.10.2.jar:/usr/bin/../share/java/kafka-rest-lib/commons-compress-1.21.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-script-runtime-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-avro-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/metrics-core-4.1.12.1.jar:/usr/bin/../share/java/kafka-rest-lib/common-config-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/audience-annotations-0.5.0.jar:/usr/bin/../share/java/kafka-rest-lib/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/kafka-rest-lib/commons-digester-1.8.1.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-storage-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/vavr-0.10.2.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-core-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-storage-api-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-protobuf-types-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-clients-7.1.0-ccs.jar:/usr/bin/../share/java/kafka-rest-lib/jsr305-3.0.2.jar:/usr/bin/../share/java/kafka-rest-lib/json-20201115.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-json-schema-serializer-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/kafka-rest-7.1.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-dataformat-csv-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/logredactor-metrics-1.0.10.jar:/usr/bin/../share/java/kafka-rest-lib/re2j-1.6.jar:/usr/bin/../share/java/kafka-rest-lib/protobuf-java-3.17.3.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/kafka-rest-lib/avro-1.11.0.jar:/usr/bin/../share/java/kafka-rest-lib/classgraph-4.8.21.jar:/usr/bin/../share/java/kafka-rest-lib/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/kafka-rest-lib/auto-value-annotations-1.7.2.jar:/usr/bin/../share/java/kafka-rest-lib/failureaccess-1.0.1.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/bin/../share/java/kafka-rest-lib/argparse4j-0.7.0.jar:/usr/bin/../share/java/kafka-rest-lib/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-stdlib-common-1.4.21.jar:/usr/bin/../share/java/kafka-rest-lib/gson-2.8.6.jar:/usr/bin/../share/java/kafka-rest-lib/kotlin-scripting-jvm-1.4.21.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-client-plugins-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.annotation-api-1.3.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-common-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-ce-logs-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-server-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/bcprov-jdk15on-1.68.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-socks-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/broker-plugins-7.1.0-ce-test.jar:/usr/bin/../share/java/confluent-security/kafka-rest/guava-30.1.1-jre.jar:/usr/bin/../share/java/confluent-security/kafka-rest/maven-artifact-3.8.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-buffer-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-licensing-new-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/security-extensions-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/checker-qual-3.8.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-container-servlet-core-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jbcrypt-0.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-log4j-appender-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/annotations-3.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-unix-common-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jaxb-api-2.3.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-events-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-tcnative-classes-2.0.46.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.activation-api-1.2.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/opencensus-proto-0.2.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/rest-authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/activation-1.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.xml.bind-api-2.3.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javassist-3.25.0-GA.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-json-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/aopalliance-repackaged-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/plexus-utils-3.2.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-http-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-resource-names-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-handler-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.json-1.0.4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-api-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-transforms-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jose4j-0.7.8.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-kqueue-4.1.65.Final-osx-x86_64.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-servlets-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.inject-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-runtime-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-jaxrs-base-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-continuation-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/bcpkix-jdk15on-1.68.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-util-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-tools-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/reflections-0.9.12.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-container-servlet-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.ws.rs-api-2.1.6.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-jaxrs-json-provider-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.servlet-api-4.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/metrics-core-2.2.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/osgi-resource-locator-1.0.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-client-1.745.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-serializers-new-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-security-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-core-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-api-1.745.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/org.abego.treelayout.core-1.0.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-security/kafka-rest/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/icu4j-61.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-handler-proxy-4.1.65.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-datatype-protobuf-0.9.11-jackson2.9.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-server-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/commons-compress-1.21.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.annotation-api-1.3.5.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-http-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-io-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-kafka-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr-runtime-3.5.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-databind-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-dataformat-yaml-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-servlet-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-codec-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-locator-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/javax.ws.rs-api-2.1.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-security-plugins-common-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/ST4-4.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/hk2-utils-2.6.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-resolver-4.1.73.Final.jar:/usr/bin/../share/java/confluent-security/kafka-rest/netty-transport-native-epoll-4.1.65.Final-linux-x86_64.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-hk2-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-api-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr4-4.9.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/connect-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-client-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/authorizer-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-common-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jersey-client-2.34.jar:/usr/bin/../share/java/confluent-security/kafka-rest/cloudevents-json-jackson-2.0.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jetty-util-ajax-9.4.44.v20210927.jar:/usr/bin/../share/java/confluent-security/kafka-rest/avro-1.11.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/confluent-security/kafka-rest/commons-lang3-3.12.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/confluent-kafka-rest-security-plugin-7.1.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-security/kafka-rest/argparse4j-0.7.0.jar:/usr/bin/../share/java/confluent-security/kafka-rest/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jakarta.validation-api-2.0.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/gson-2.8.6.jar:/usr/bin/../share/java/confluent-security/kafka-rest/antlr4-runtime-4.9.2.jar:/usr/bin/../share/java/confluent-security/kafka-rest/jackson-module-jaxb-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/kafka-rest/snakeyaml-1.27.jar:/usr/bin/../share/java/confluent-security/schema-validator/snappy-java-1.1.8.4.jar:/usr/bin/../share/java/confluent-security/schema-validator/annotations-13.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/guava-30.1.1-jre.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-logging-1.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-common-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/listenablefuture-9999.0-empty-to-avoid-conflict-with-guava.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-compiler-impl-embeddable-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-protobuf-provider-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/okio-jvm-3.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/wire-schema-jvm-4.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-annotations-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/checker-qual-3.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/confluent-schema-registry-validator-plugin-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-jdk7-1.5.31.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-module-parameter-names-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/lz4-java-1.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-json-schema-provider-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-guava-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/common-utils-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/wire-runtime-jvm-4.0.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/org.everit.json.schema-1.12.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/handy-uri-templates-2.1.8.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlinx-coroutines-core-1.3.7.jar:/usr/bin/../share/java/confluent-security/schema-validator/proto-google-common-protos-2.5.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-validator-1.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-joda-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-schema-registry-client-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/error_prone_annotations-2.5.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/scala-library-2.13.5.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-schema-serializer-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-collections-3.2.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/validation-api-2.0.1.Final.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-jdk8-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/zstd-jni-1.5.0-4.jar:/usr/bin/../share/java/confluent-security/schema-validator/j2objc-annotations-1.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/joda-time-2.10.8.jar:/usr/bin/../share/java/confluent-security/schema-validator/mbknor-jackson-jsonschema_2.13-1.0.39.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-compress-1.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-script-runtime-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-avro-serializer-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-databind-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/caffeine-2.8.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/protobuf-java-util-3.17.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-clients-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/schema-validator/commons-digester-1.8.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-core-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kafka-protobuf-types-7.1.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/jsr305-3.0.2.jar:/usr/bin/../share/java/confluent-security/schema-validator/json-20201115.jar:/usr/bin/../share/java/confluent-security/schema-validator/re2j-1.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/protobuf-java-3.17.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-jdk8-1.5.31.jar:/usr/bin/../share/java/confluent-security/schema-validator/avro-1.11.0.jar:/usr/bin/../share/java/confluent-security/schema-validator/classgraph-4.8.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/swagger-annotations-2.1.10.jar:/usr/bin/../share/java/confluent-security/schema-validator/failureaccess-1.0.1.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-compiler-embeddable-1.3.50.jar:/usr/bin/../share/java/confluent-security/schema-validator/jackson-datatype-jsr310-2.12.3.jar:/usr/bin/../share/java/confluent-security/schema-validator/telemetry-events-api-7.1.0-ce.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-stdlib-common-1.4.21.jar:/usr/bin/../share/java/confluent-security/schema-validator/gson-2.8.6.jar:/usr/bin/../share/java/confluent-security/schema-validator/kotlin-scripting-jvm-1.4.21.jar:/usr/bin/../support-metrics-client/build/dependant-libs-2.13.6/*:/usr/bin/../support-metrics-client/build/libs/*:/usr/bin/../share/java/confluent-telemetry/confluent-metrics-7.1.0-ce.jar:/usr/share/java/support-metrics-client/*
[INFO] 2023-11-08 14:50:53,996 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.library.path=/usr/java/packages/lib:/usr/lib64:/lib64:/lib:/usr/lib
[INFO] 2023-11-08 14:50:53,996 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.io.tmpdir=/tmp
[INFO] 2023-11-08 14:50:53,997 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:java.compiler=<NA>
[INFO] 2023-11-08 14:50:53,997 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.name=Linux
[INFO] 2023-11-08 14:50:53,997 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.arch=amd64
[INFO] 2023-11-08 14:50:53,997 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.version=5.15.109+
[INFO] 2023-11-08 14:50:53,997 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:user.name=?
[INFO] 2023-11-08 14:50:53,997 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:user.home=?
[INFO] 2023-11-08 14:50:53,997 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:user.dir=/home/appuser
[INFO] 2023-11-08 14:50:53,997 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.memory.free=1623MB
[INFO] 2023-11-08 14:50:53,997 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.memory.max=16080MB
[INFO] 2023-11-08 14:50:53,997 [main] org.apache.zookeeper.ZooKeeper logEnv - Client environment:os.memory.total=1712MB
[INFO] 2023-11-08 14:50:54,000 [main] org.apache.zookeeper.ZooKeeper <init> - Initiating client connection, connectString=zookeeper.confluent.svc.cluster.local:2182 sessionTimeout=22500 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@7bca6fac
[INFO] 2023-11-08 14:50:54,075 [main] org.apache.zookeeper.ClientCnxnSocket initProperties - jute.maxbuffer value is 4194304 Bytes
[INFO] 2023-11-08 14:50:54,082 [main] org.apache.zookeeper.ClientCnxn initRequestTimeout - zookeeper.request.timeout value is 0. feature enabled=false
[INFO] 2023-11-08 14:50:54,084 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Waiting until connected.
[INFO] 2023-11-08 14:50:54,118 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:50:54,121 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:50:54,148 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.2.12:2182.
[INFO] 2023-11-08 14:50:54,148 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:50:54,247 [nioEventLoopGroup-2-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:50:54,350 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0x7969f085]
[INFO] 2023-11-08 14:50:54,359 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.17:55714, server: zookeeper.confluent.svc.cluster.local/10.40.2.12:2182
[INFO] 2023-11-08 14:50:54,425 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0x7969f085, L:/10.40.0.17:55714 - R:zookeeper.confluent.svc.cluster.local/10.40.2.12:2182]
[INFO] 2023-11-08 14:50:54,662 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty channelInactive - channel is disconnected: [id: 0x7969f085, L:/10.40.0.17:55714 ! R:zookeeper.confluent.svc.cluster.local/10.40.2.12:2182]
[INFO] 2023-11-08 14:50:54,662 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[WARN] 2023-11-08 14:50:54,663 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn run - Session 0x0 for sever zookeeper.confluent.svc.cluster.local/10.40.2.12:2182, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
EndOfStreamException: channel for sessionid 0x0 is lost
	at org.apache.zookeeper.ClientCnxnSocketNetty.doTransport(ClientCnxnSocketNetty.java:285)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
[INFO] 2023-11-08 14:50:55,846 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:50:55,846 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:50:55,847 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.2.12:2182.
[INFO] 2023-11-08 14:50:55,847 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:50:55,847 [nioEventLoopGroup-2-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:50:55,849 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0xbbb883e0]
[INFO] 2023-11-08 14:50:55,850 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.17:55722, server: zookeeper.confluent.svc.cluster.local/10.40.2.12:2182
[INFO] 2023-11-08 14:50:55,853 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0xbbb883e0, L:/10.40.0.17:55722 - R:zookeeper.confluent.svc.cluster.local/10.40.2.12:2182]
[INFO] 2023-11-08 14:50:55,914 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty channelInactive - channel is disconnected: [id: 0xbbb883e0, L:/10.40.0.17:55722 ! R:zookeeper.confluent.svc.cluster.local/10.40.2.12:2182]
[INFO] 2023-11-08 14:50:55,914 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[WARN] 2023-11-08 14:50:55,914 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn run - Session 0x0 for sever zookeeper.confluent.svc.cluster.local/10.40.2.12:2182, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
EndOfStreamException: channel for sessionid 0x0 is lost
	at org.apache.zookeeper.ClientCnxnSocketNetty.doTransport(ClientCnxnSocketNetty.java:285)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
[INFO] 2023-11-08 14:50:57,713 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:50:57,714 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:50:57,714 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.1.10:2182.
[INFO] 2023-11-08 14:50:57,714 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:50:57,715 [nioEventLoopGroup-2-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:50:57,716 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0x1de97e80]
[INFO] 2023-11-08 14:50:57,718 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.17:40752, server: zookeeper.confluent.svc.cluster.local/10.40.1.10:2182
[INFO] 2023-11-08 14:50:57,721 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0x1de97e80, L:/10.40.0.17:40752 - R:zookeeper.confluent.svc.cluster.local/10.40.1.10:2182]
[INFO] 2023-11-08 14:50:57,771 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty channelInactive - channel is disconnected: [id: 0x1de97e80, L:/10.40.0.17:40752 ! R:zookeeper.confluent.svc.cluster.local/10.40.1.10:2182]
[INFO] 2023-11-08 14:50:57,771 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[WARN] 2023-11-08 14:50:57,771 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn run - Session 0x0 for sever zookeeper.confluent.svc.cluster.local/10.40.1.10:2182, Closing socket connection. Attempting reconnect except it is a SessionExpiredException.
EndOfStreamException: channel for sessionid 0x0 is lost
	at org.apache.zookeeper.ClientCnxnSocketNetty.doTransport(ClientCnxnSocketNetty.java:285)
	at org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1290)
[INFO] 2023-11-08 14:50:59,319 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:50:59,319 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:50:59,320 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.1.8:2182.
[INFO] 2023-11-08 14:50:59,320 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:50:59,321 [nioEventLoopGroup-2-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:50:59,322 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0x538c3e18]
[INFO] 2023-11-08 14:50:59,324 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.17:44684, server: zookeeper.confluent.svc.cluster.local/10.40.1.8:2182
[INFO] 2023-11-08 14:50:59,327 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0x538c3e18, L:/10.40.0.17:44684 - R:zookeeper.confluent.svc.cluster.local/10.40.1.8:2182]
[INFO] 2023-11-08 14:50:59,676 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxn onConnected - Session establishment complete on server zookeeper.confluent.svc.cluster.local/10.40.1.8:2182, session id = 0x12f96c0002, negotiated timeout = 22500
[INFO] 2023-11-08 14:50:59,680 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Connected.
[INFO] 2023-11-08 14:50:59,713 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Closing.
[INFO] 2023-11-08 14:51:00,189 [main] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[INFO] 2023-11-08 14:51:00,191 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty channelInactive - channel is disconnected: [id: 0x538c3e18, L:/10.40.0.17:44684 ! R:zookeeper.confluent.svc.cluster.local/10.40.1.8:2182]
[INFO] 2023-11-08 14:51:00,191 [nioEventLoopGroup-2-1] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[INFO] 2023-11-08 14:51:00,192 [main] org.apache.zookeeper.ZooKeeper close - Session: 0x12f96c0002 closed
[INFO] 2023-11-08 14:51:00,192 [main-EventThread] org.apache.zookeeper.ClientCnxn run - EventThread shut down for session: 0x12f96c0002
[INFO] 2023-11-08 14:51:00,193 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Closed.
[INFO] 2023-11-08 14:51:00,195 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Initializing a new session to zookeeper.confluent.svc.cluster.local:2182/kafka-confluent.
[INFO] 2023-11-08 14:51:00,195 [main] org.apache.zookeeper.ZooKeeper <init> - Initiating client connection, connectString=zookeeper.confluent.svc.cluster.local:2182/kafka-confluent sessionTimeout=22500 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@29a1505c
[INFO] 2023-11-08 14:51:00,195 [main] org.apache.zookeeper.ClientCnxnSocket initProperties - jute.maxbuffer value is 4194304 Bytes
[INFO] 2023-11-08 14:51:00,195 [main] org.apache.zookeeper.ClientCnxn initRequestTimeout - zookeeper.request.timeout value is 0. feature enabled=false
[INFO] 2023-11-08 14:51:00,196 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Waiting until connected.
[INFO] 2023-11-08 14:51:00,196 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:51:00,196 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:51:00,196 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.1.10:2182.
[INFO] 2023-11-08 14:51:00,197 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:51:00,198 [nioEventLoopGroup-3-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:51:00,199 [nioEventLoopGroup-3-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0xe0053f7f]
[INFO] 2023-11-08 14:51:00,200 [nioEventLoopGroup-3-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.17:50936, server: zookeeper.confluent.svc.cluster.local/10.40.1.10:2182
[INFO] 2023-11-08 14:51:00,226 [nioEventLoopGroup-3-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0xe0053f7f, L:/10.40.0.17:50936 - R:zookeeper.confluent.svc.cluster.local/10.40.1.10:2182]
[INFO] 2023-11-08 14:51:00,724 [nioEventLoopGroup-3-1] org.apache.zookeeper.ClientCnxn onConnected - Session establishment complete on server zookeeper.confluent.svc.cluster.local/10.40.1.10:2182, session id = 0x100001300e00000, negotiated timeout = 22500
[INFO] 2023-11-08 14:51:00,724 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient Kafka server] Connected.
[INFO] 2023-11-08 14:51:03,887 [main] kafka.server.KafkaServer info - Cluster ID = 9PWH12e6ROOpezLRDGV6Ag
[WARN] 2023-11-08 14:51:03,890 [main] kafka.server.BrokerMetadataCheckpoint warn - No meta.properties file under dir /mnt/data/data0/logs/meta.properties
[INFO] 2023-11-08 14:51:03,945 [main] kafka.server.KafkaConfig logAll - KafkaConfig values: 
	advertised.listeners = EXTERNAL://rb0.my.domain:9092,INTERNAL://kafka-0.kafka.confluent.svc.cluster.local:9071,REPLICATION://kafka-0.kafka.confluent.svc.cluster.local:9072,TOKEN://kafka-0.kafka.confluent.svc.cluster.local:9073
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = 0
	broker.session.timeout.ms = 9000
	broker.session.uuid = _xmBsq6vTSmXhoYfo9SXDw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 120
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = true
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.reporting.enable = false
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1200000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = REPLICATION
	inter.broker.protocol.version = 2.6
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = EXTERNAL:SASL_SSL,INTERNAL:SASL_SSL,REPLICATION:SASL_SSL,TOKEN:SASL_SSL
	listeners = EXTERNAL://:9092,INTERNAL://:9071,REPLICATION://:9072,TOKEN://:9073
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /mnt/data/data0/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.6
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	node.id = 0
	num.io.threads = 8
	num.network.threads = 4
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 15000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 3
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.coordinator.enable = false
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 45000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [PLAIN]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = org.apache.zookeeper.ClientCnxnSocketNetty
	zookeeper.connect = zookeeper.confluent.svc.cluster.local:2182/kafka-confluent
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 22500
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = true
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	zookeeper.ssl.truststore.password = [hidden]
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[INFO] 2023-11-08 14:51:03,961 [main] kafka.server.KafkaConfig logAll - KafkaConfig values: 
	advertised.listeners = EXTERNAL://rb0.my.domain:9092,INTERNAL://kafka-0.kafka.confluent.svc.cluster.local:9071,REPLICATION://kafka-0.kafka.confluent.svc.cluster.local:9072,TOKEN://kafka-0.kafka.confluent.svc.cluster.local:9073
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = 0
	broker.session.timeout.ms = 9000
	broker.session.uuid = _xmBsq6vTSmXhoYfo9SXDw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 120
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = true
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.reporting.enable = false
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1200000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = REPLICATION
	inter.broker.protocol.version = 2.6
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = EXTERNAL:SASL_SSL,INTERNAL:SASL_SSL,REPLICATION:SASL_SSL,TOKEN:SASL_SSL
	listeners = EXTERNAL://:9092,INTERNAL://:9071,REPLICATION://:9072,TOKEN://:9073
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /mnt/data/data0/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.6
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	node.id = 0
	num.io.threads = 8
	num.network.threads = 4
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 15000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 3
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.coordinator.enable = false
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 45000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [PLAIN]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = org.apache.zookeeper.ClientCnxnSocketNetty
	zookeeper.connect = zookeeper.confluent.svc.cluster.local:2182/kafka-confluent
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 22500
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = true
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	zookeeper.ssl.truststore.password = [hidden]
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[INFO] 2023-11-08 14:51:04,018 [main] kafka.log.LogManager info - Log directory /mnt/data/data0/logs not found, creating it.
[INFO] 2023-11-08 14:51:04,043 [main] kafka.log.LogManager info - Loading logs from log dirs ArraySeq(/mnt/data/data0/logs)
[INFO] 2023-11-08 14:51:04,047 [main] kafka.log.LogManager info - Attempting recovery for all logs in /mnt/data/data0/logs since no clean shutdown file was found
[INFO] 2023-11-08 14:51:04,051 [main] kafka.log.LogManager info - Loaded 0 logs in 9ms.
[INFO] 2023-11-08 14:51:04,052 [main] kafka.log.LogManager info - Starting log cleanup with a period of 300000 ms.
[INFO] 2023-11-08 14:51:04,055 [main] kafka.log.LogManager info - Starting log flusher with a default period of 9223372036854775807 ms.
[INFO] 2023-11-08 14:51:04,073 [main] kafka.log.LogCleaner info - Starting the log cleaner
[INFO] 2023-11-08 14:51:04,179 [kafka-log-cleaner-thread-0] kafka.log.LogCleaner info - [kafka-log-cleaner-thread-0]: Starting
[INFO] 2023-11-08 14:51:04,192 [main] kafka.server.KafkaServer info - [KafkaServer id=0] Creating metadataCache (multi-tenant: false)
[INFO] 2023-11-08 14:51:04,202 [main] io.confluent.security.audit.AuditLogConfig logAll - AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 

[INFO] 2023-11-08 14:51:04,203 [main] io.confluent.security.audit.AuditLogConfig logAll - AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 

[INFO] 2023-11-08 14:51:04,230 [main] io.confluent.crn.CrnAuthorityConfig logAll - CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP

[INFO] 2023-11-08 14:51:04,231 [main] io.confluent.crn.CrnAuthorityConfig logAll - CrnAuthorityConfig values: 
	confluent.authorizer.authority.cache.entries = 10000
	confluent.authorizer.authority.name = 
	confluent.metadata.server.api.flavor = CP

[INFO] 2023-11-08 14:51:04,232 [main] io.confluent.kafka.multitenant.authorizer.MultiTenantAuditLogConfig logAll - MultiTenantAuditLogConfig values: 
	confluent.security.event.logger.client.ip.enable = false
	confluent.security.event.logger.multitenant.enable = false

[INFO] 2023-11-08 14:51:04,253 [ThrottledChannelReaper-Produce] kafka.server.ClientQuotaManager$ThrottledChannelReaper info - [ThrottledChannelReaper-Produce]: Starting
[INFO] 2023-11-08 14:51:04,258 [main] kafka.server.DiskUsageBasedThrottlingConfig$ apply - Empty logDirs received! Disk based throttling won't be activated!
[INFO] 2023-11-08 14:51:04,259 [ThrottledChannelReaper-Fetch] kafka.server.ClientQuotaManager$ThrottledChannelReaper info - [ThrottledChannelReaper-Fetch]: Starting
[INFO] 2023-11-08 14:51:04,260 [main] kafka.server.DiskUsageBasedThrottlingConfig$ apply - Empty logDirs received! Disk based throttling won't be activated!
[INFO] 2023-11-08 14:51:04,263 [ThrottledChannelReaper-Request] kafka.server.ClientQuotaManager$ThrottledChannelReaper info - [ThrottledChannelReaper-Request]: Starting
[INFO] 2023-11-08 14:51:04,266 [main] kafka.server.DiskUsageBasedThrottlingConfig$ apply - Empty logDirs received! Disk based throttling won't be activated!
[INFO] 2023-11-08 14:51:04,266 [ThrottledChannelReaper-ControllerMutation] kafka.server.ClientQuotaManager$ThrottledChannelReaper info - [ThrottledChannelReaper-ControllerMutation]: Starting
[INFO] 2023-11-08 14:51:04,318 [ExpirationReaper-0-ClusterLink] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-0-ClusterLink]: Starting
[INFO] 2023-11-08 14:51:04,330 [main] kafka.server.link.ClusterLinkManager info - Enforce create cluster link policy.
[INFO] 2023-11-08 14:51:04,475 [main] io.confluent.kafka.server.plugins.policy.ClusterLinkPolicyConfig logAll - ClusterLinkPolicyConfig values: 
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.link.policy.acl.sync.ms.max = 300000
	confluent.plugins.link.policy.acl.sync.ms.min = 1000
	confluent.plugins.link.policy.availability.check.ms.max = 60000
	confluent.plugins.link.policy.availability.check.ms.min = 3000
	confluent.plugins.link.policy.consumer.offset.sync.ms.max = 300000
	confluent.plugins.link.policy.consumer.offset.sync.ms.min = 1000
	confluent.plugins.link.policy.replica.socket.receive.buffer.bytes.max = 1048576
	confluent.plugins.link.policy.replica.socket.receive.buffer.bytes.min = 32768
	confluent.plugins.link.policy.sasl.mechanism.allowed = [PLAIN, SCRAM-SHA-256, SCRAM-SHA-512]
	confluent.plugins.link.policy.topic.config.sync.ms.max = 300000
	confluent.plugins.link.policy.topic.config.sync.ms.min = 1000

[INFO] 2023-11-08 14:51:04,476 [main] io.confluent.kafka.server.plugins.policy.CreateClusterLinkPolicy reconfigure - Setting maximum number of destination links to 5 and maximum number of source links to 5
[INFO] 2023-11-08 14:51:04,529 [main] org.apache.kafka.common.security.authenticator.AbstractLogin login - Successfully logged in.
[INFO] 2023-11-08 14:51:04,563 [BrokerToControllerChannelManager broker=0 name=forwarding] kafka.server.BrokerToControllerRequestThread info - [BrokerToControllerChannelManager broker=0 name=forwarding]: Starting
[INFO] 2023-11-08 14:51:04,733 [main] kafka.network.ConnectionQuotas info - Updated connection-tokens max connection creation rate to 2147483647
[INFO] 2023-11-08 14:51:04,744 [main] kafka.network.Acceptor info - Awaiting socket connections on 0.0.0.0:9092.
[INFO] 2023-11-08 14:51:04,769 [main] org.apache.kafka.common.security.authenticator.AbstractLogin login - Successfully logged in.
[INFO] 2023-11-08 14:51:04,800 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(EXTERNAL)
[INFO] 2023-11-08 14:51:04,800 [main] kafka.network.ConnectionQuotas info - Updated connection-tokens max connection creation rate to 2147483647
[INFO] 2023-11-08 14:51:04,801 [main] kafka.network.Acceptor info - Awaiting socket connections on 0.0.0.0:9071.
[INFO] 2023-11-08 14:51:04,802 [main] org.apache.kafka.common.security.authenticator.AbstractLogin login - Successfully logged in.
[INFO] 2023-11-08 14:51:04,820 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(INTERNAL)
[INFO] 2023-11-08 14:51:04,821 [main] kafka.network.ConnectionQuotas info - Updated connection-tokens max connection creation rate to 2147483647
[INFO] 2023-11-08 14:51:04,821 [main] kafka.network.Acceptor info - Awaiting socket connections on 0.0.0.0:9072.
[INFO] 2023-11-08 14:51:05,090 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(REPLICATION)
[INFO] 2023-11-08 14:51:05,093 [main] kafka.network.ConnectionQuotas info - Updated connection-tokens max connection creation rate to 2147483647
[INFO] 2023-11-08 14:51:05,094 [main] kafka.network.Acceptor info - Awaiting socket connections on 0.0.0.0:9073.
[INFO] 2023-11-08 14:51:05,099 [main] io.confluent.kafka.clients.plugins.auth.jwt.JwtAuthenticatorConfig logAll - JwtAuthenticatorConfig values: 
	allowUnsafeURL = false
	audience = 
	issuer = Confluent
	jkuDomainWhiteList = []
	jwksLocation = /mnt/secrets/mds-token/mdsPublicKey.pem
	verificationKeyRefreshInterval = 3600000
	verificationKeyResolver = pemfile

[INFO] 2023-11-08 14:51:05,245 [main] org.apache.kafka.common.security.authenticator.AbstractLogin login - Successfully logged in.
[INFO] 2023-11-08 14:51:05,250 [main] io.confluent.kafka.clients.plugins.auth.jwt.JwtAuthenticatorConfig logAll - JwtAuthenticatorConfig values: 
	allowUnsafeURL = false
	audience = 
	issuer = Confluent
	jkuDomainWhiteList = []
	jwksLocation = /mnt/secrets/mds-token/mdsPublicKey.pem
	verificationKeyRefreshInterval = 3600000
	verificationKeyResolver = pemfile

[INFO] 2023-11-08 14:51:05,255 [main] io.confluent.kafka.clients.plugins.auth.jwt.JwtAuthenticatorConfig logAll - JwtAuthenticatorConfig values: 
	allowUnsafeURL = false
	audience = 
	issuer = Confluent
	jkuDomainWhiteList = []
	jwksLocation = /mnt/secrets/mds-token/mdsPublicKey.pem
	verificationKeyRefreshInterval = 3600000
	verificationKeyResolver = pemfile

[INFO] 2023-11-08 14:51:05,262 [main] io.confluent.kafka.clients.plugins.auth.jwt.JwtAuthenticatorConfig logAll - JwtAuthenticatorConfig values: 
	allowUnsafeURL = false
	audience = 
	issuer = Confluent
	jkuDomainWhiteList = []
	jwksLocation = /mnt/secrets/mds-token/mdsPublicKey.pem
	verificationKeyRefreshInterval = 3600000
	verificationKeyResolver = pemfile

[INFO] 2023-11-08 14:51:05,267 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=0] Created data-plane acceptor and processors for endpoint : ListenerName(TOKEN)
[INFO] 2023-11-08 14:51:05,320 [ExpirationReaper-0-Produce] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-0-Produce]: Starting
[INFO] 2023-11-08 14:51:05,322 [ExpirationReaper-0-Fetch] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-0-Fetch]: Starting
[INFO] 2023-11-08 14:51:05,322 [ExpirationReaper-0-DeleteRecords] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-0-DeleteRecords]: Starting
[INFO] 2023-11-08 14:51:05,323 [ExpirationReaper-0-ElectLeader] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-0-ElectLeader]: Starting
[INFO] 2023-11-08 14:51:05,324 [ExpirationReaper-0-ListOffsets] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-0-ListOffsets]: Starting
[INFO] 2023-11-08 14:51:05,344 [LogDirFailureHandler] kafka.server.ReplicaManager$LogDirFailureHandler info - [LogDirFailureHandler]: Starting
[INFO] 2023-11-08 14:51:05,364 [main] kafka.zk.KafkaZkClient info - Creating /brokers/ids/0 (is it secure? false)
[INFO] 2023-11-08 14:51:05,393 [main] kafka.zk.KafkaZkClient info - Stat of the created znode at /brokers/ids/0 is: 4294967372,4294967372,1699455065379,1699455065379,1,0,0,72057675656986624,449,0,4294967372

[INFO] 2023-11-08 14:51:05,394 [main] kafka.zk.KafkaZkClient info - Registered broker 0 at path /brokers/ids/0 with addresses: EXTERNAL://rb0.my.domain:9092,INTERNAL://kafka-0.kafka.confluent.svc.cluster.local:9071,REPLICATION://kafka-0.kafka.confluent.svc.cluster.local:9072,TOKEN://kafka-0.kafka.confluent.svc.cluster.local:9073, czxid (broker epoch): 4294967372
[INFO] 2023-11-08 14:51:05,701 [main] kafka.controller.DataBalanceManager apply - DataBalancer: attempting startup with io.confluent.databalancer.KafkaDataBalanceManager
[INFO] 2023-11-08 14:51:05,714 [main] io.confluent.databalancer.KafkaDataBalanceManager enableDatabalancerMetric - Registering metric ActiveBalancerCount
[INFO] 2023-11-08 14:51:05,717 [controller-event-thread] kafka.controller.ControllerEventManager$ControllerEventThread info - [ControllerEventThread controllerId=0] Starting
[INFO] 2023-11-08 14:51:05,731 [ExpirationReaper-0-topic] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-0-topic]: Starting
[INFO] 2023-11-08 14:51:05,739 [ExpirationReaper-0-Heartbeat] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-0-Heartbeat]: Starting
[INFO] 2023-11-08 14:51:05,740 [ExpirationReaper-0-Rebalance] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-0-Rebalance]: Starting
[INFO] 2023-11-08 14:51:05,762 [main] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Starting up.
[INFO] 2023-11-08 14:51:05,769 [main] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Startup complete.
[INFO] 2023-11-08 14:51:05,788 [main] kafka.coordinator.transaction.ZkProducerIdManager info - [ZK ProducerId Manager 0]: Acquired new producerId block ProducerIdsBlock{brokerId=0, producerIdStart=1000, producerIdLen=1000} by writing to Zk with path version 2
[INFO] 2023-11-08 14:51:05,812 [main] kafka.coordinator.transaction.TransactionCoordinator info - [TransactionCoordinator id=0] Starting up.
[INFO] 2023-11-08 14:51:05,818 [TxnMarkerSenderThread-0] kafka.coordinator.transaction.TransactionMarkerChannelManager info - [Transaction Marker Channel Manager 0]: Starting
[INFO] 2023-11-08 14:51:05,818 [main] kafka.coordinator.transaction.TransactionCoordinator info - [TransactionCoordinator id=0] Startup complete.
[INFO] 2023-11-08 14:51:05,827 [main] io.confluent.security.authorizer.ConfluentAuthorizerConfig logAll - ConfluentAuthorizerConfig values: 
	allow.everyone.if.no.acl.found = false
	broker.users = 
	confluent.authorizer.access.rule.providers = [ZK_ACL, CONFLUENT]
	confluent.authorizer.acl.migration.batch.size = 1000
	confluent.authorizer.init.timeout.ms = 600000
	confluent.authorizer.migrate.acls.from.zk = false
	super.users = User:operator;User:kafka

[INFO] 2023-11-08 14:51:05,867 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Initializing a new session to zookeeper.confluent.svc.cluster.local:2182.
[INFO] 2023-11-08 14:51:05,867 [main] org.apache.zookeeper.ZooKeeper <init> - Initiating client connection, connectString=zookeeper.confluent.svc.cluster.local:2182 sessionTimeout=22500 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@46612bfc
[INFO] 2023-11-08 14:51:05,868 [main] org.apache.zookeeper.ClientCnxnSocket initProperties - jute.maxbuffer value is 4194304 Bytes
[INFO] 2023-11-08 14:51:05,868 [main] org.apache.zookeeper.ClientCnxn initRequestTimeout - zookeeper.request.timeout value is 0. feature enabled=false
[INFO] 2023-11-08 14:51:05,869 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Waiting until connected.
[INFO] 2023-11-08 14:51:05,869 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:51:05,869 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:51:05,871 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.1.10:2182.
[INFO] 2023-11-08 14:51:05,871 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:51:05,872 [nioEventLoopGroup-4-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:51:05,874 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0xe239817a]
[INFO] 2023-11-08 14:51:05,875 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.17:50950, server: zookeeper.confluent.svc.cluster.local/10.40.1.10:2182
[INFO] 2023-11-08 14:51:05,878 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0xe239817a, L:/10.40.0.17:50950 - R:zookeeper.confluent.svc.cluster.local/10.40.1.10:2182]
[INFO] 2023-11-08 14:51:05,930 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxn onConnected - Session establishment complete on server zookeeper.confluent.svc.cluster.local/10.40.1.10:2182, session id = 0x100001300e00002, negotiated timeout = 22500
[INFO] 2023-11-08 14:51:05,930 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Connected.
[INFO] 2023-11-08 14:51:05,940 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Closing.
[INFO] 2023-11-08 14:51:05,947 [main] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[INFO] 2023-11-08 14:51:05,948 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxnSocketNetty channelInactive - channel is disconnected: [id: 0xe239817a, L:/10.40.0.17:50950 ! R:zookeeper.confluent.svc.cluster.local/10.40.1.10:2182]
[INFO] 2023-11-08 14:51:05,948 [main-EventThread] org.apache.zookeeper.ClientCnxn run - EventThread shut down for session: 0x100001300e00002
[INFO] 2023-11-08 14:51:05,948 [main] org.apache.zookeeper.ZooKeeper close - Session: 0x100001300e00002 closed
[INFO] 2023-11-08 14:51:05,948 [nioEventLoopGroup-4-1] org.apache.zookeeper.ClientCnxnSocketNetty onClosing - channel is told closing
[INFO] 2023-11-08 14:51:05,949 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Closed.
[INFO] 2023-11-08 14:51:05,950 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Initializing a new session to zookeeper.confluent.svc.cluster.local:2182/kafka-confluent.
[INFO] 2023-11-08 14:51:05,950 [main] org.apache.zookeeper.ZooKeeper <init> - Initiating client connection, connectString=zookeeper.confluent.svc.cluster.local:2182/kafka-confluent sessionTimeout=22500 watcher=kafka.zookeeper.ZooKeeperClient$ZooKeeperClientWatcher$@5a47730c
[INFO] 2023-11-08 14:51:05,950 [main] org.apache.zookeeper.ClientCnxnSocket initProperties - jute.maxbuffer value is 4194304 Bytes
[INFO] 2023-11-08 14:51:05,951 [main] org.apache.zookeeper.ClientCnxn initRequestTimeout - zookeeper.request.timeout value is 0. feature enabled=false
[INFO] 2023-11-08 14:51:05,951 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Waiting until connected.
[INFO] 2023-11-08 14:51:05,951 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.Login login - Client successfully logged in.
[INFO] 2023-11-08 14:51:05,952 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.client.ZooKeeperSaslClient createSaslClient - Client will use DIGEST-MD5 as SASL mechanism.
[INFO] 2023-11-08 14:51:05,952 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - Opening socket connection to server zookeeper.confluent.svc.cluster.local/10.40.2.12:2182.
[INFO] 2023-11-08 14:51:05,952 [main-SendThread(zookeeper.confluent.svc.cluster.local:2182)] org.apache.zookeeper.ClientCnxn logStartConnect - SASL config status: Will attempt to SASL-authenticate using Login Context section 'Client'
[WARN] 2023-11-08 14:51:05,954 [nioEventLoopGroup-5-1] org.apache.zookeeper.common.X509Util createSSLContextAndOptionsFromConfig - zookeeper.ssl.keyStore.location not specified
[INFO] 2023-11-08 14:51:05,955 [nioEventLoopGroup-5-1] org.apache.zookeeper.ClientCnxnSocketNetty initSSL - SSL handler added for channel: [id: 0x6f082c53]
[INFO] 2023-11-08 14:51:05,957 [nioEventLoopGroup-5-1] org.apache.zookeeper.ClientCnxn primeConnection - Socket connection established, initiating session, client: /10.40.0.17:46732, server: zookeeper.confluent.svc.cluster.local/10.40.2.12:2182
[INFO] 2023-11-08 14:51:05,977 [nioEventLoopGroup-5-1] org.apache.zookeeper.ClientCnxnSocketNetty operationComplete - channel is connected: [id: 0x6f082c53, L:/10.40.0.17:46732 - R:zookeeper.confluent.svc.cluster.local/10.40.2.12:2182]
[INFO] 2023-11-08 14:51:06,022 [nioEventLoopGroup-5-1] org.apache.zookeeper.ClientCnxn onConnected - Session establishment complete on server zookeeper.confluent.svc.cluster.local/10.40.2.12:2182, session id = 0x2000013a55b0001, negotiated timeout = 22500
[INFO] 2023-11-08 14:51:06,023 [main] kafka.zookeeper.ZooKeeperClient info - [ZooKeeperClient ACL authorizer] Connected.
[INFO] 2023-11-08 14:51:06,103 [/kafka-acl-changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread info - [/kafka-acl-changes-event-process-thread]: Starting
[INFO] 2023-11-08 14:51:06,103 [/kafka-acl-extended-changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread info - [/kafka-acl-extended-changes-event-process-thread]: Starting
[INFO] 2023-11-08 14:51:06,137 [main] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-0.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[INFO] 2023-11-08 14:51:06,145 [main] kafka.server.KafkaConfig logAll - KafkaConfig values: 
	advertised.listeners = EXTERNAL://rb0.my.domain:9092,INTERNAL://kafka-0.kafka.confluent.svc.cluster.local:9071,REPLICATION://kafka-0.kafka.confluent.svc.cluster.local:9072,TOKEN://kafka-0.kafka.confluent.svc.cluster.local:9073
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = 0
	broker.session.timeout.ms = 9000
	broker.session.uuid = _xmBsq6vTSmXhoYfo9SXDw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 120
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = true
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.reporting.enable = false
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1200000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = REPLICATION
	inter.broker.protocol.version = 2.6
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = EXTERNAL:SASL_SSL,INTERNAL:SASL_SSL,REPLICATION:SASL_SSL,TOKEN:SASL_SSL
	listeners = EXTERNAL://:9092,INTERNAL://:9071,REPLICATION://:9072,TOKEN://:9073
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /mnt/data/data0/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.6
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	node.id = 0
	num.io.threads = 8
	num.network.threads = 4
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 15000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 3
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.coordinator.enable = false
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 45000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [PLAIN]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = org.apache.zookeeper.ClientCnxnSocketNetty
	zookeeper.connect = zookeeper.confluent.svc.cluster.local:2182/kafka-confluent
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 22500
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = true
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	zookeeper.ssl.truststore.password = [hidden]
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[INFO] 2023-11-08 14:51:06,160 [main] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-0.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[INFO] 2023-11-08 14:51:06,360 [main] io.confluent.security.store.kafka.KafkaStoreConfig logAll - KafkaStoreConfig values: 
	confluent.metadata.refresh.timeout.ms = 60000
	confluent.metadata.retry.timeout.ms = 86400000
	confluent.metadata.topic.create.timeout.ms = 600000
	confluent.metadata.topic.replication.factor = 3

[INFO] 2023-11-08 14:51:06,384 [main] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-auth-consumer-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.security.store.kafka.clients.JsonSerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.security.store.kafka.clients.JsonSerde

[WARN] 2023-11-08 14:51:06,426 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,426 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,426 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'auto.create.topics.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,426 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,426 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,426 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,426 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,426 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,426 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,427 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,427 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,427 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'num.recovery.threads.per.data.dir' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,427 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,427 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,427 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,427 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.server.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,427 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.mechanism.inter.broker.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,427 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'default.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,427 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'broker.session.uuid' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.operator.managed' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'transaction.state.log.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'num.io.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,428 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,429 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'delete.topic.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,429 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,429 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.clientCnxnSocket' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,429 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,429 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,429 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.ssl.client.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,429 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.login.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,429 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'offsets.commit.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,429 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,431 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,432 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'authorizer.class.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,432 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,434 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,435 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,435 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,435 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,435 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,435 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'offsets.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,435 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'transaction.state.log.min.isr' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,435 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,435 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,435 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'socket.request.max.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,435 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,436 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,436 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,436 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,436 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'offsets.retention.minutes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,436 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'inter.broker.protocol.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,436 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,436 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,436 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.security.protocol.map' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,436 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'log.retention.hours' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,437 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,437 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'num.partitions' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,437 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,437 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,437 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,437 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'inter.broker.listener.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,437 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,441 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,442 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,442 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,442 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,442 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,442 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,442 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,442 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,443 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,443 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.balancer.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,443 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'node.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,443 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,443 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,443 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.external.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,443 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,444 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'log.message.format.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,445 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,445 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,445 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,445 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,445 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,445 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'log.segment.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,445 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.token.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,445 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'group.max.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,445 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,446 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,446 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,446 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'log.dirs' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,446 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,446 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,446 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,446 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,446 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,447 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,447 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,447 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,447 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'replica.lag.time.max.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,447 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,447 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,447 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,447 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,447 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,449 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'num.network.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,449 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'socket.send.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,449 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'listener.name.replication.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,450 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'socket.receive.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,451 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,451 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'broker.rack' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,451 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'zookeeper.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,451 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,451 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'broker.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,451 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'offsets.topic.compression.codec' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,451 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'log.retention.check.interval.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,451 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,452 [main] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:06,453 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:06,453 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:06,453 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455066452
[DEBUG] 2023-11-08 14:51:06,457 [main] io.confluent.security.auth.store.kafka.KafkaAuthStore configure - Configured auth store with configs io.confluent.security.store.kafka.KafkaStoreConfig: 
	confluent.metadata.topic.replication.factor=3%n	confluent.metadata.retry.timeout.ms=86400000%n	confluent.metadata.topic.create.timeout.ms=600000%n	confluent.metadata.refresh.timeout.ms=60000
[INFO] 2023-11-08 14:51:06,466 [main] io.confluent.security.auth.provider.ldap.LdapConfig logAll - LdapConfig values: 
	ldap.group.dn.name.pattern = 
	ldap.group.member.attribute = member
	ldap.group.member.attribute.pattern = CN=(.*),DC=test,DC=com
	ldap.group.name.attribute = cn
	ldap.group.name.attribute.pattern = 
	ldap.group.object.class = group
	ldap.group.search.base = dc=test,dc=com
	ldap.group.search.filter = 
	ldap.group.search.scope = 1
	ldap.refresh.interval.ms = 60000
	ldap.retry.backoff.max.ms = 1000
	ldap.retry.backoff.ms = 100
	ldap.retry.timeout.ms = 86400000
	ldap.sasl.jaas.config = null
	ldap.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	ldap.sasl.kerberos.min.time.before.relogin = 60000
	ldap.sasl.kerberos.service.name = ldap
	ldap.sasl.kerberos.ticket.renew.jitter = 0.05
	ldap.sasl.kerberos.ticket.renew.window.factor = 0.8
	ldap.sasl.login.callback.handler.class = null
	ldap.sasl.login.class = null
	ldap.sasl.login.connect.timeout.ms = null
	ldap.sasl.login.read.timeout.ms = null
	ldap.sasl.login.retry.backoff.max.ms = 10000
	ldap.sasl.login.retry.backoff.ms = 100
	ldap.sasl.oauthbearer.clock.skew.seconds = 30
	ldap.sasl.oauthbearer.expected.audience = null
	ldap.sasl.oauthbearer.expected.issuer = null
	ldap.sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	ldap.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	ldap.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	ldap.sasl.oauthbearer.jwks.endpoint.url = null
	ldap.sasl.oauthbearer.scope.claim.name = scope
	ldap.sasl.oauthbearer.sub.claim.name = sub
	ldap.sasl.oauthbearer.token.endpoint.url = null
	ldap.search.mode = GROUPS
	ldap.search.page.size = 0
	ldap.ssl.cipher.suites = null
	ldap.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ldap.ssl.endpoint.identification.algorithm = https
	ldap.ssl.engine.factory.class = null
	ldap.ssl.key.password = null
	ldap.ssl.keymanager.algorithm = SunX509
	ldap.ssl.keystore.certificate.chain = null
	ldap.ssl.keystore.key = null
	ldap.ssl.keystore.location = null
	ldap.ssl.keystore.password = null
	ldap.ssl.keystore.type = JKS
	ldap.ssl.protocol = TLSv1.3
	ldap.ssl.provider = null
	ldap.ssl.secure.random.implementation = null
	ldap.ssl.trustmanager.algorithm = PKIX
	ldap.ssl.truststore.certificates = null
	ldap.ssl.truststore.location = null
	ldap.ssl.truststore.password = null
	ldap.ssl.truststore.type = JKS
	ldap.user.dn.name.pattern = 
	ldap.user.memberof.attribute = memberof
	ldap.user.memberof.attribute.pattern = CN=(.*),DC=test,DC=com
	ldap.user.name.attribute = cn
	ldap.user.name.attribute.pattern = 
	ldap.user.object.class = organizationalRole
	ldap.user.password.attribute = null
	ldap.user.search.base = dc=test,dc=com
	ldap.user.search.filter = 
	ldap.user.search.scope = 1

[INFO] 2023-11-08 14:51:06,471 [main] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-0.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[INFO] 2023-11-08 14:51:06,472 [main] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-0.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[DEBUG] 2023-11-08 14:51:06,473 [main] io.confluent.security.auth.store.kafka.KafkaAuthStore startService - Starting writer for auth store [https://kafka-0.kafka.confluent.svc.cluster.local:8090]
[INFO] 2023-11-08 14:51:06,487 [main] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-auth-producer-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = true
	interceptor.classes = []
	key.serializer = class io.confluent.security.store.kafka.clients.JsonSerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 0
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.security.store.kafka.clients.JsonSerde

[WARN] 2023-11-08 14:51:06,521 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,523 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,523 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'auto.create.topics.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,526 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,526 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,526 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,526 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,526 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,529 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,529 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,529 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,529 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'num.recovery.threads.per.data.dir' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.server.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'sasl.mechanism.inter.broker.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'default.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'broker.session.uuid' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,530 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.operator.managed' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'transaction.state.log.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'num.io.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'delete.topic.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.clientCnxnSocket' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.ssl.client.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.login.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'offsets.commit.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,531 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'authorizer.class.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'offsets.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'transaction.state.log.min.isr' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'socket.request.max.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,532 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'offsets.retention.minutes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'inter.broker.protocol.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.security.protocol.map' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'log.retention.hours' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'num.partitions' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,533 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'inter.broker.listener.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,534 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,534 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,534 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,534 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,535 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,535 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,535 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,535 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,535 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,535 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,535 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.balancer.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,536 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'node.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,536 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,536 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,536 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.external.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,536 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,536 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,536 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,536 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,537 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,537 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,537 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,537 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,537 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,537 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,537 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,537 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,538 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'log.message.format.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,538 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,538 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,538 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,538 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,538 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,538 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'log.segment.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,539 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.token.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,539 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'group.max.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,539 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,539 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,539 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,539 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'log.dirs' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,539 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,540 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,540 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,540 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,540 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,540 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,540 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,541 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,541 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'replica.lag.time.max.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,541 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,541 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,541 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,541 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,541 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,541 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'num.network.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,542 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'socket.send.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,542 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'listener.name.replication.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,542 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'socket.receive.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,542 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,542 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'broker.rack' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,542 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'zookeeper.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,542 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,543 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'broker.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,543 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'offsets.topic.compression.codec' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,543 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'log.retention.check.interval.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,543 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,543 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:06,544 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:06,544 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:06,544 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455066543
[INFO] 2023-11-08 14:51:06,553 [main] io.confluent.security.auth.provider.ldap.LdapConfig logAll - LdapConfig values: 
	ldap.group.dn.name.pattern = 
	ldap.group.member.attribute = member
	ldap.group.member.attribute.pattern = CN=(.*),DC=test,DC=com
	ldap.group.name.attribute = cn
	ldap.group.name.attribute.pattern = 
	ldap.group.object.class = group
	ldap.group.search.base = dc=test,dc=com
	ldap.group.search.filter = 
	ldap.group.search.scope = 1
	ldap.refresh.interval.ms = 60000
	ldap.retry.backoff.max.ms = 1000
	ldap.retry.backoff.ms = 100
	ldap.retry.timeout.ms = 86400000
	ldap.sasl.jaas.config = null
	ldap.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	ldap.sasl.kerberos.min.time.before.relogin = 60000
	ldap.sasl.kerberos.service.name = ldap
	ldap.sasl.kerberos.ticket.renew.jitter = 0.05
	ldap.sasl.kerberos.ticket.renew.window.factor = 0.8
	ldap.sasl.login.callback.handler.class = null
	ldap.sasl.login.class = null
	ldap.sasl.login.connect.timeout.ms = null
	ldap.sasl.login.read.timeout.ms = null
	ldap.sasl.login.retry.backoff.max.ms = 10000
	ldap.sasl.login.retry.backoff.ms = 100
	ldap.sasl.oauthbearer.clock.skew.seconds = 30
	ldap.sasl.oauthbearer.expected.audience = null
	ldap.sasl.oauthbearer.expected.issuer = null
	ldap.sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	ldap.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	ldap.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	ldap.sasl.oauthbearer.jwks.endpoint.url = null
	ldap.sasl.oauthbearer.scope.claim.name = scope
	ldap.sasl.oauthbearer.sub.claim.name = sub
	ldap.sasl.oauthbearer.token.endpoint.url = null
	ldap.search.mode = GROUPS
	ldap.search.page.size = 0
	ldap.ssl.cipher.suites = null
	ldap.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ldap.ssl.endpoint.identification.algorithm = https
	ldap.ssl.engine.factory.class = null
	ldap.ssl.key.password = null
	ldap.ssl.keymanager.algorithm = SunX509
	ldap.ssl.keystore.certificate.chain = null
	ldap.ssl.keystore.key = null
	ldap.ssl.keystore.location = null
	ldap.ssl.keystore.password = null
	ldap.ssl.keystore.type = JKS
	ldap.ssl.protocol = TLSv1.3
	ldap.ssl.provider = null
	ldap.ssl.secure.random.implementation = null
	ldap.ssl.trustmanager.algorithm = PKIX
	ldap.ssl.truststore.certificates = null
	ldap.ssl.truststore.location = null
	ldap.ssl.truststore.password = null
	ldap.ssl.truststore.type = JKS
	ldap.user.dn.name.pattern = 
	ldap.user.memberof.attribute = memberof
	ldap.user.memberof.attribute.pattern = CN=(.*),DC=test,DC=com
	ldap.user.name.attribute = cn
	ldap.user.name.attribute.pattern = 
	ldap.user.object.class = organizationalRole
	ldap.user.password.attribute = null
	ldap.user.search.base = dc=test,dc=com
	ldap.user.search.filter = 
	ldap.user.search.scope = 1

[INFO] 2023-11-08 14:51:06,560 [main] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = false
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-coordinator-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = _confluent-metadata-coordinator-group
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class org.apache.kafka.common.serialization.ByteArrayDeserializer

[INFO] 2023-11-08 14:51:06,561 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:06,561 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:06,561 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455066561
[DEBUG] 2023-11-08 14:51:06,574 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager run - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Starting metadata node coordinator
[DEBUG] 2023-11-08 14:51:06,578 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-0.kafka.confluent.svc.cluster.local:9072 (id: -1 rack: null)
[INFO] 2023-11-08 14:51:06,586 [main] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-0.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[INFO] 2023-11-08 14:51:06,601 [main] kafka.server.link.ClusterLinkMetadataManager info - [ClusterLinkMetadataManager-broker-0] Cluster link metadata manager started without metadata topic, controller will be the link coordinator.
[INFO] 2023-11-08 14:51:06,612 [main] kafka.server.link.ClusterLinkManager info - ClusterLinkManager has started up.
[INFO] 2023-11-08 14:51:06,662 [ExpirationReaper-0-AlterAcls] kafka.server.DelayedOperationPurgatory$ExpiredOperationReaper info - [ExpirationReaper-0-AlterAcls]: Starting
[INFO] 2023-11-08 14:51:06,668 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,673 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,678 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,679 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,683 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,684 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,690 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,694 [main] org.apache.kafka.common.requests.SamplingRequestLogFilter$Config logAll - Config values: 
	confluent.request.log.api.samples.per.min = 
	confluent.request.log.enable.admin.apis = true
	confluent.request.log.samples.per.min = 0

[INFO] 2023-11-08 14:51:06,706 [BrokerHealthManager] kafka.availability.BrokerHealthManager info - [BrokerHealthManager]: Starting
[INFO] 2023-11-08 14:51:06,830 [main] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:06,898 [main] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[INFO] 2023-11-08 14:51:06,921 [main] org.apache.kafka.common.security.authenticator.AbstractLogin login - Successfully logged in.
[WARN] 2023-11-08 14:51:06,925 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:06,925 [main] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:06,931 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:06,931 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:06,931 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455066925
[INFO] 2023-11-08 14:51:06,963 [main] io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig logAll - EventEmitterConfig values: 

[INFO] 2023-11-08 14:51:06,968 [main] io.confluent.metrics.reporter.ConfluentMetricsReporter onUpdate - Starting Confluent metrics reporter for cluster id 9PWH12e6ROOpezLRDGV6Ag with an interval of 30000 ms
[WARN] 2023-11-08 14:51:07,135 [main] io.confluent.telemetry.ConfluentTelemetryConfig <init> - no telemetry exporters are enabled
[WARN] 2023-11-08 14:51:07,294 [main] io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade withLabel - Ignoring redefinition of existing telemetry label kafka.version
[INFO] 2023-11-08 14:51:07,320 [main] io.confluent.telemetry.ConfluentTelemetryConfig logAll - ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

[INFO] 2023-11-08 14:51:07,326 [main] io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig logAll - VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

[INFO] 2023-11-08 14:51:07,328 [main] io.confluent.telemetry.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.server/.*(confluent_audit/audit_log_fallback_rate_per_minute|confluent_audit/audit_log_rate_per_minute|confluent_authorizer/authorization_request_rate_per_minute|confluent_authorizer/authorization_allowed_rate_per_minute|confluent_authorizer/authorization_denied_rate_per_minute|confluent_auth_store/rbac_role_bindings_count|confluent_auth_store/rbac_access_rules_count|confluent_auth_store/acl_access_rules_count).*|.*io.confluent.kafka.server/.*(acl_authorizer/zookeeper_disconnects/total/delta|acl_authorizer/zookeeper_expires/total/delta|broker_failure/zookeeper_disconnects/total/delta|broker_failure/zookeeper_expires/total/delta|broker_topic/bytes_in/total/delta|broker_topic/bytes_out/total/delta|broker_topic/failed_produce_requests/total/delta|broker_topic/failed_fetch_requests/total/delta|broker_topic/produce_message_conversions/total/delta|broker_topic/fetch_message_conversions/total/delta|controller/active_controller_count|controller/leader_election_rate_and_time_ms|controller/offline_partitions_count|controller/partition_availability|controller/global_under_min_isr_partition_count|controller/unclean_leader_elections/total|controller_channel/connection_close_rate|controller_channel/connection_close_total|controller_channel/connection_count|controller_channel/connection_creation_rate|controller_channel/connection_creation_total|controller_channel/request_size_avg|controller_channel/request_size_max|controller_channel_manager/queue_size|controller_channel_manager/total_queue_size|controller_event_manager/event_queue_size|delayed_operation_purgatory/purgatory_size|executor/zookeeper_disconnects/total/delta|executor/zookeeper_expires/total/delta|fetch/queue_size|group_coordinator/partition_load_time_max|log_cleaner_manager/achieved_cleaning_ratio/time/delta|log_cleaner_manager/achieved_cleaning_ratio/total/delta|log_cleaner_manager/compacted_partition_bytes|log_cleaner_manager/max_dirty_percent|log_cleaner_manager/time_since_last_run_ms|log_cleaner_manager/uncleanable_bytes|log_cleaner_manager/uncleanable_partitions_count|replica_alter_log_dirs_manager/max_lag|replica_fetcher/request_size_avg|replica_fetcher/request_size_max|replica_fetcher_manager/max_lag|replica_manager/isr_shrinks|replica_manager/leader_count|replica_manager/partition_count|replica_manager/under_min_isr_partition_count|replica_manager/under_replicated_partitions|request/errors/total/delta|request/local_time_ms/time/delta|request/local_time_ms/total/delta|request/queue_size|request/remote_time_ms/time/delta|request/remote_time_ms/total/delta|request/request_queue_time_ms/time/delta|request/request_queue_time_ms/total/delta|request/requests|request/response_queue_time_ms/time/delta|request/response_queue_time_ms/total/delta|request/response_send_time_ms/time/delta|request/response_send_time_ms/total/delta|request/total_time_ms/time/delta|request/total_time_ms/total/delta|request_channel/request_queue_size|request_channel/response_queue_size|request_handler_pool/request_handler_avg_idle_percent|session_expire_listener/zookeeper_disconnects/total/delta|session_expire_listener/zookeeper_expires/total/delta|socket_server/connections|socket_server/successful_authentication_total/delta|socket_server/failed_authentication_total/delta|socket_server/network_processor_avg_idle_percent|socket_server/request_size_avg|socket_server/request_size_max).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[INFO] 2023-11-08 14:51:07,336 [main] io.confluent.telemetry.exporter.kafka.KafkaExporterConfig logAll - KafkaExporterConfig values: 
	enabled = true
	events.enabled = true
	metrics.enabled = true
	metrics.include = (io\.confluent\.kafka\.server/broker_topic/bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/messages_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_in/rate/1_min|io\.confluent\.kafka\.server/broker_topic/replication_bytes_out/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_fetch_requests/rate/1_min|io\.confluent\.kafka\.server/broker_topic/total_produce_requests/rate/1_min|io\.confluent\.kafka\.server/log/size|io\.confluent\.kafka\.server/log_flush/log_flush_rate_and_time_ms|io\.confluent\.kafka\.server/log_flush/log_flush_rate_and_time_ms/rate/1_min|io\.confluent\.kafka\.server/request/local_time_ms|io\.confluent\.kafka\.server/request/request_queue_time_ms|io\.confluent\.kafka\.server/request/requests/rate/1_min|io\.confluent\.kafka\.server/request/total_time_ms|io\.confluent\.kafka\.server/request_channel/request_queue_size|io\.confluent\.kafka\.server/request_channel/response_queue_size|io\.confluent\.kafka\.server/request_handler_pool/request_handler_avg_idle_percent/rate/1_min|io\.confluent\.system/jvm/os/process_cpu_load|io\.confluent\.system/volume/disk_total_bytes)
	producer.bootstrap.servers = kafka-0.kafka.confluent.svc.cluster.local:9072
	topic.create = true
	topic.max.message.bytes = 10485760
	topic.name = _confluent-telemetry-metrics
	topic.partitions = 12
	topic.replicas = 3
	topic.retention.bytes = -1
	topic.retention.ms = 259200000
	topic.roll.ms = 14400000
	type = kafka

[INFO] 2023-11-08 14:51:07,336 [main] io.confluent.telemetry.reporter.TelemetryReporter initEventLogger - Initializing the event logger
[INFO] 2023-11-08 14:51:07,343 [main] io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig logAll - EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter

[INFO] 2023-11-08 14:51:07,357 [main] io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[INFO] 2023-11-08 14:51:07,785 [main] io.confluent.telemetry.reporter.TelemetryReporter initExporters - Creating kafka exporter named '_local'
[INFO] 2023-11-08 14:51:07,797 [main] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.telemetry.serde.OpencensusMetricsProto

[INFO] 2023-11-08 14:51:07,801 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:07,801 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:07,801 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455067800
[INFO] 2023-11-08 14:51:07,803 [main] io.confluent.telemetry.reporter.TelemetryReporter startMetricCollectorTask - Starting Confluent telemetry reporter with an interval of 60000 ms for resource = (type = kafka)
[INFO] 2023-11-08 14:51:07,939 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener$ChangeEventProcessThread info - [/config/changes-event-process-thread]: Starting
[INFO] 2023-11-08 14:51:07,987 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=0] Starting socket server acceptors and processors
[INFO] 2023-11-08 14:51:08,023 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(REPLICATION)
[INFO] 2023-11-08 14:51:08,429 [data-plane-kafka-request-handler-3] kafka.server.ZkAdminManager info - [Admin Manager on Broker 0]: Error processing create topic request CreatableTopic(name='__consumer_offsets', numPartitions=50, replicationFactor=3, assignments=[], configs=[CreateableTopicConfig(name='compression.type', value='producer'), CreateableTopicConfig(name='cleanup.policy', value='compact'), CreateableTopicConfig(name='segment.bytes', value='104857600'), CreateableTopicConfig(name='confluent.placement.constraints', value='')], linkName=null, mirrorTopic=null)
org.apache.kafka.common.errors.InvalidReplicationFactorException: Replication factor: 3 larger than available brokers: 0.
[DEBUG] 2023-11-08 14:51:08,437 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068436, latencyMs=1856, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-0, correlationId=0), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=-1, host='', port=-1, errorCode=15, errorMessage='')]))
[DEBUG] 2023-11-08 14:51:08,437 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Group coordinator lookup failed: 
[DEBUG] 2023-11-08 14:51:08,437 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator ensureCoordinatorReady - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Coordinator discovery failed, refreshing metadata
org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
[INFO] 2023-11-08 14:51:08,440 [metadata-service-coordinator] org.apache.kafka.clients.Metadata update - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:08,440 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[INFO] 2023-11-08 14:51:08,441 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 1 from controller 1 for 50 partitions
[INFO] 2023-11-08 14:51:08,459 [BrokerToControllerChannelManager broker=0 name=forwarding] kafka.server.BrokerToControllerRequestThread info - [BrokerToControllerChannelManager broker=0 name=forwarding]: Recorded new controller, from now on will use broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1 tags: [])
[DEBUG] 2023-11-08 14:51:08,478 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068478, latencyMs=38, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-0, correlationId=4), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=-1, host='', port=-1, errorCode=15, errorMessage='')]))
[DEBUG] 2023-11-08 14:51:08,478 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Group coordinator lookup failed: 
[DEBUG] 2023-11-08 14:51:08,478 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator ensureCoordinatorReady - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Coordinator discovery failed, refreshing metadata
org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
[INFO] 2023-11-08 14:51:08,485 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] org.apache.kafka.clients.Metadata update - [Producer clientId=_confluent-metadata-auth-producer-0] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:08,532 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-8, __consumer_offsets-35, __consumer_offsets-47, __consumer_offsets-38, __consumer_offsets-17, __consumer_offsets-11, __consumer_offsets-29, __consumer_offsets-32, __consumer_offsets-41, __consumer_offsets-23, __consumer_offsets-2, __consumer_offsets-14, __consumer_offsets-20, __consumer_offsets-44, __consumer_offsets-5, __consumer_offsets-26)
[INFO] 2023-11-08 14:51:08,537 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 1 from controller 1 epoch 1 as part of the become-leader transition for 16 partitions
[DEBUG] 2023-11-08 14:51:08,541 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[INFO] 2023-11-08 14:51:08,542 [auth-reader-1] org.apache.kafka.clients.Metadata update - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:08,545 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068545, latencyMs=4, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-0, correlationId=7), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=-1, host='', port=-1, errorCode=15, errorMessage='')]))
[DEBUG] 2023-11-08 14:51:08,546 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Group coordinator lookup failed: 
[DEBUG] 2023-11-08 14:51:08,546 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator ensureCoordinatorReady - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Coordinator discovery failed, refreshing metadata
org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
[DEBUG] 2023-11-08 14:51:08,643 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[DEBUG] 2023-11-08 14:51:08,645 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068645, latencyMs=2, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-0, correlationId=9), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=-1, host='', port=-1, errorCode=15, errorMessage='')]))
[DEBUG] 2023-11-08 14:51:08,645 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Group coordinator lookup failed: 
[DEBUG] 2023-11-08 14:51:08,645 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator ensureCoordinatorReady - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Coordinator discovery failed, refreshing metadata
org.apache.kafka.common.errors.CoordinatorNotAvailableException: The coordinator is not available.
[INFO] 2023-11-08 14:51:08,701 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-35, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,707 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-35 in /mnt/data/data0/logs/__consumer_offsets-35 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,711 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-35 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-35
[INFO] 2023-11-08 14:51:08,714 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-35 broker=0] Log loaded for partition __consumer_offsets-35 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,716 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-35 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,733 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,735 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-5 in /mnt/data/data0/logs/__consumer_offsets-5 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,735 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-5 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-5
[INFO] 2023-11-08 14:51:08,735 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-5 broker=0] Log loaded for partition __consumer_offsets-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,735 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,742 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-20, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,743 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-20 in /mnt/data/data0/logs/__consumer_offsets-20 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,743 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-20 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-20
[INFO] 2023-11-08 14:51:08,743 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-20 broker=0] Log loaded for partition __consumer_offsets-20 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,743 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:51:08,752 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[INFO] 2023-11-08 14:51:08,752 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-41, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,754 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-41 in /mnt/data/data0/logs/__consumer_offsets-41 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,754 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-41 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-41
[INFO] 2023-11-08 14:51:08,754 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-41 broker=0] Log loaded for partition __consumer_offsets-41 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,754 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-41 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:51:08,755 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068755, latencyMs=2, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-0, correlationId=11), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=0, host='kafka-0.kafka.confluent.svc.cluster.local', port=9072, errorCode=0, errorMessage='')]))
[INFO] 2023-11-08 14:51:08,755 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Discovered group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:08,758 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager onRevoked - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Metadata writer assignment revoked for generation -1
[INFO] 2023-11-08 14:51:08,759 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group](Re-)joining group
[DEBUG] 2023-11-08 14:51:08,758 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator run - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Heartbeat thread started
[INFO] 2023-11-08 14:51:08,764 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-29, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,767 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-29 in /mnt/data/data0/logs/__consumer_offsets-29 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,767 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-29 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-29
[INFO] 2023-11-08 14:51:08,767 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-29 broker=0] Log loaded for partition __consumer_offsets-29 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,768 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-29 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,777 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-44, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,778 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-44 in /mnt/data/data0/logs/__consumer_offsets-44 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,778 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-44 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-44
[INFO] 2023-11-08 14:51:08,778 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-44 broker=0] Log loaded for partition __consumer_offsets-44 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,778 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-44 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:51:08,780 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending JoinGroup (JoinGroupRequestData(groupId='_confluent-metadata-coordinator-group', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='metadata-service', protocols=[JoinGroupRequestProtocol(name='v0', metadata=[123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 48, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125])])) to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:08,783 [kafka-producer-network-thread | confluent-telemetry-reporter-local-producer] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-telemetry-reporter-local-producer] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:08,784 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager run - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Wake up exception from poll
[INFO] 2023-11-08 14:51:08,789 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-14, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,791 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-14 in /mnt/data/data0/logs/__consumer_offsets-14 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,791 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-14 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-14
[INFO] 2023-11-08 14:51:08,792 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-14 broker=0] Log loaded for partition __consumer_offsets-14 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,792 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,801 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,802 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-2 in /mnt/data/data0/logs/__consumer_offsets-2 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,802 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-2 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-2
[INFO] 2023-11-08 14:51:08,802 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-2 broker=0] Log loaded for partition __consumer_offsets-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,802 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,811 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-23, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,812 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-23 in /mnt/data/data0/logs/__consumer_offsets-23 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,812 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-23 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-23
[INFO] 2023-11-08 14:51:08,813 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-23 broker=0] Log loaded for partition __consumer_offsets-23 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,813 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,820 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-38, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,821 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-38 in /mnt/data/data0/logs/__consumer_offsets-38 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,821 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-38 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-38
[INFO] 2023-11-08 14:51:08,821 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-38 broker=0] Log loaded for partition __consumer_offsets-38 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,821 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-38 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,825 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted.
[INFO] 2023-11-08 14:51:08,825 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Requesting disconnect from last known coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:08,826 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
[INFO] 2023-11-08 14:51:08,828 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,829 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-8 in /mnt/data/data0/logs/__consumer_offsets-8 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,830 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-8 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-8
[INFO] 2023-11-08 14:51:08,830 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-8 broker=0] Log loaded for partition __consumer_offsets-8 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,830 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,836 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,838 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-11 in /mnt/data/data0/logs/__consumer_offsets-11 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,838 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-11 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-11
[INFO] 2023-11-08 14:51:08,838 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-11 broker=0] Log loaded for partition __consumer_offsets-11 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,838 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,845 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-26, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,848 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-26 in /mnt/data/data0/logs/__consumer_offsets-26 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,848 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-26 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-26
[INFO] 2023-11-08 14:51:08,848 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-26 broker=0] Log loaded for partition __consumer_offsets-26 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,848 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-26 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,854 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-47, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,855 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-47 in /mnt/data/data0/logs/__consumer_offsets-47 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,855 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-47 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-47
[INFO] 2023-11-08 14:51:08,855 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-47 broker=0] Log loaded for partition __consumer_offsets-47 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,855 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-47 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,863 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-17, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,864 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-17 in /mnt/data/data0/logs/__consumer_offsets-17 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,864 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-17 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-17
[INFO] 2023-11-08 14:51:08,864 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-17 broker=0] Log loaded for partition __consumer_offsets-17 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,864 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,870 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-32, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,872 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-32 in /mnt/data/data0/logs/__consumer_offsets-32 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,872 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-32 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-32
[INFO] 2023-11-08 14:51:08,872 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-32 broker=0] Log loaded for partition __consumer_offsets-32 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,872 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader __consumer_offsets-32 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,886 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,888 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-3 in /mnt/data/data0/logs/__consumer_offsets-3 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,888 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-3 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-3
[INFO] 2023-11-08 14:51:08,888 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-3 broker=0] Log loaded for partition __consumer_offsets-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,889 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,896 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-18, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,897 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-18 in /mnt/data/data0/logs/__consumer_offsets-18 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,897 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-18 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-18
[INFO] 2023-11-08 14:51:08,897 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-18 broker=0] Log loaded for partition __consumer_offsets-18 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,897 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,903 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-37, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,904 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-37 in /mnt/data/data0/logs/__consumer_offsets-37 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,904 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-37 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-37
[INFO] 2023-11-08 14:51:08,904 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-37 broker=0] Log loaded for partition __consumer_offsets-37 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,904 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-37 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,910 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,911 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-7 in /mnt/data/data0/logs/__consumer_offsets-7 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,911 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-7 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-7
[INFO] 2023-11-08 14:51:08,911 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-7 broker=0] Log loaded for partition __consumer_offsets-7 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,911 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,917 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-22, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,918 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-22 in /mnt/data/data0/logs/__consumer_offsets-22 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,918 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-22 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-22
[INFO] 2023-11-08 14:51:08,919 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-22 broker=0] Log loaded for partition __consumer_offsets-22 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,919 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,925 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[DEBUG] 2023-11-08 14:51:08,926 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[INFO] 2023-11-08 14:51:08,926 [metadata-service-coordinator] org.apache.kafka.clients.NetworkClient disconnect - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Client requested disconnect from node 2147483647
[INFO] 2023-11-08 14:51:08,926 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-10 in /mnt/data/data0/logs/__consumer_offsets-10 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,927 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-10 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-10
[INFO] 2023-11-08 14:51:08,927 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-10 broker=0] Log loaded for partition __consumer_offsets-10 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,927 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:51:08,931 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455068931, latencyMs=5, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-0, correlationId=14), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=0, host='kafka-0.kafka.confluent.svc.cluster.local', port=9072, errorCode=0, errorMessage='')]))
[INFO] 2023-11-08 14:51:08,931 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Discovered group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:08,931 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted.
[INFO] 2023-11-08 14:51:08,931 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Requesting disconnect from last known coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:08,934 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-33, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,935 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-33 in /mnt/data/data0/logs/__consumer_offsets-33 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,935 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-33 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-33
[INFO] 2023-11-08 14:51:08,935 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-33 broker=0] Log loaded for partition __consumer_offsets-33 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,935 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-33 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,940 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-48, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,941 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-48 in /mnt/data/data0/logs/__consumer_offsets-48 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,941 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-48 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-48
[INFO] 2023-11-08 14:51:08,941 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-48 broker=0] Log loaded for partition __consumer_offsets-48 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,941 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-48 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,946 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-19, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,947 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-19 in /mnt/data/data0/logs/__consumer_offsets-19 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,947 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-19 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-19
[INFO] 2023-11-08 14:51:08,947 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-19 broker=0] Log loaded for partition __consumer_offsets-19 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,947 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,953 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-34, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,954 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-34 in /mnt/data/data0/logs/__consumer_offsets-34 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,954 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-34 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-34
[INFO] 2023-11-08 14:51:08,954 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-34 broker=0] Log loaded for partition __consumer_offsets-34 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,954 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-34 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,960 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,961 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-4 in /mnt/data/data0/logs/__consumer_offsets-4 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,961 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-4 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-4
[INFO] 2023-11-08 14:51:08,961 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-4 broker=0] Log loaded for partition __consumer_offsets-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,961 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,966 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-45, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,968 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-45 in /mnt/data/data0/logs/__consumer_offsets-45 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,968 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-45 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-45
[INFO] 2023-11-08 14:51:08,968 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-45 broker=0] Log loaded for partition __consumer_offsets-45 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,968 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-45 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,973 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-15, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,974 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-15 in /mnt/data/data0/logs/__consumer_offsets-15 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,974 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-15 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-15
[INFO] 2023-11-08 14:51:08,974 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-15 broker=0] Log loaded for partition __consumer_offsets-15 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,975 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,980 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-30, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,981 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-30 in /mnt/data/data0/logs/__consumer_offsets-30 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,981 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-30 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-30
[INFO] 2023-11-08 14:51:08,981 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-30 broker=0] Log loaded for partition __consumer_offsets-30 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,982 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-30 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,987 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-49, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,988 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-49 in /mnt/data/data0/logs/__consumer_offsets-49 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,988 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-49 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-49
[INFO] 2023-11-08 14:51:08,988 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-49 broker=0] Log loaded for partition __consumer_offsets-49 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,988 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-49 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,993 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:08,994 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-0 in /mnt/data/data0/logs/__consumer_offsets-0 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:08,994 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-0 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-0
[INFO] 2023-11-08 14:51:08,994 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-0 broker=0] Log loaded for partition __consumer_offsets-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:08,994 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:08,999 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-39, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,000 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-39 in /mnt/data/data0/logs/__consumer_offsets-39 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,000 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-39 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-39
[INFO] 2023-11-08 14:51:09,000 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-39 broker=0] Log loaded for partition __consumer_offsets-39 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,000 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-39 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,005 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,005 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-9 in /mnt/data/data0/logs/__consumer_offsets-9 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,006 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-9 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-9
[INFO] 2023-11-08 14:51:09,006 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-9 broker=0] Log loaded for partition __consumer_offsets-9 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,006 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,010 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-24, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,011 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-24 in /mnt/data/data0/logs/__consumer_offsets-24 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,011 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-24 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-24
[INFO] 2023-11-08 14:51:09,012 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-24 broker=0] Log loaded for partition __consumer_offsets-24 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,012 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,017 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-27, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,019 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-27 in /mnt/data/data0/logs/__consumer_offsets-27 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,019 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-27 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-27
[INFO] 2023-11-08 14:51:09,019 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-27 broker=0] Log loaded for partition __consumer_offsets-27 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,019 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-27 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,025 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-42, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,026 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-42 in /mnt/data/data0/logs/__consumer_offsets-42 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,026 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-42 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-42
[INFO] 2023-11-08 14:51:09,026 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-42 broker=0] Log loaded for partition __consumer_offsets-42 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,026 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-42 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,031 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-12, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[DEBUG] 2023-11-08 14:51:09,031 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[INFO] 2023-11-08 14:51:09,032 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-12 in /mnt/data/data0/logs/__consumer_offsets-12 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,032 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-12 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-12
[INFO] 2023-11-08 14:51:09,032 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-12 broker=0] Log loaded for partition __consumer_offsets-12 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,032 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:51:09,034 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455069033, latencyMs=2, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-0, correlationId=15), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=0, host='kafka-0.kafka.confluent.svc.cluster.local', port=9072, errorCode=0, errorMessage='')]))
[INFO] 2023-11-08 14:51:09,034 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Discovered group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,035 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group](Re-)joining group
[DEBUG] 2023-11-08 14:51:09,035 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending JoinGroup (JoinGroupRequestData(groupId='_confluent-metadata-coordinator-group', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='metadata-service', protocols=[JoinGroupRequestProtocol(name='v0', metadata=[123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 48, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125])])) to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,037 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-31, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,038 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-31 in /mnt/data/data0/logs/__consumer_offsets-31 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,038 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-31 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-31
[INFO] 2023-11-08 14:51:09,038 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-31 broker=0] Log loaded for partition __consumer_offsets-31 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,038 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-31 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,046 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-46, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,047 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-46 in /mnt/data/data0/logs/__consumer_offsets-46 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,047 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-46 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-46
[INFO] 2023-11-08 14:51:09,048 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-46 broker=0] Log loaded for partition __consumer_offsets-46 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,048 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-46 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,053 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,054 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-1 in /mnt/data/data0/logs/__consumer_offsets-1 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,054 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-1 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-1
[INFO] 2023-11-08 14:51:09,054 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-1 broker=0] Log loaded for partition __consumer_offsets-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,055 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,055 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null) is unavailable or invalid due to cause: error response NOT_COORDINATOR.isDisconnected: false. Rediscovery will be attempted.
[INFO] 2023-11-08 14:51:09,055 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Requesting disconnect from last known coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,055 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]JoinGroup failed: This is not the correct coordinator. Marking coordinator unknown. Sent generation was Generation{generationId=-1, memberId='', protocol='null'}
[INFO] 2023-11-08 14:51:09,060 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-16, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,061 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-16 in /mnt/data/data0/logs/__consumer_offsets-16 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,061 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-16 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-16
[INFO] 2023-11-08 14:51:09,061 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-16 broker=0] Log loaded for partition __consumer_offsets-16 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,061 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,069 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-21, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,070 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-21 in /mnt/data/data0/logs/__consumer_offsets-21 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,071 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-21 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-21
[INFO] 2023-11-08 14:51:09,071 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-21 broker=0] Log loaded for partition __consumer_offsets-21 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,071 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,076 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-36, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,077 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-36 in /mnt/data/data0/logs/__consumer_offsets-36 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,077 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-36 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-36
[INFO] 2023-11-08 14:51:09,077 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-36 broker=0] Log loaded for partition __consumer_offsets-36 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,077 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-36 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,082 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,083 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-6 in /mnt/data/data0/logs/__consumer_offsets-6 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,083 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-6 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-6
[INFO] 2023-11-08 14:51:09,083 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-6 broker=0] Log loaded for partition __consumer_offsets-6 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,083 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,088 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-25, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,089 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-25 in /mnt/data/data0/logs/__consumer_offsets-25 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,089 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-25 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-25
[INFO] 2023-11-08 14:51:09,089 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-25 broker=0] Log loaded for partition __consumer_offsets-25 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,090 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-25 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,094 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-40, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,095 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-40 in /mnt/data/data0/logs/__consumer_offsets-40 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,095 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-40 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-40
[INFO] 2023-11-08 14:51:09,096 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-40 broker=0] Log loaded for partition __consumer_offsets-40 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,096 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-40 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,101 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-43, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,102 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-43 in /mnt/data/data0/logs/__consumer_offsets-43 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,102 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-43 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-43
[INFO] 2023-11-08 14:51:09,102 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-43 broker=0] Log loaded for partition __consumer_offsets-43 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,102 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-43 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,107 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-13, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,107 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-13 in /mnt/data/data0/logs/__consumer_offsets-13 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,107 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-13 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-13
[INFO] 2023-11-08 14:51:09,108 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-13 broker=0] Log loaded for partition __consumer_offsets-13 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,108 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,113 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=__consumer_offsets-28, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,114 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition __consumer_offsets-28 in /mnt/data/data0/logs/__consumer_offsets-28 with properties {cleanup.policy=compact, compression.type="producer", confluent.placement.constraints="", segment.bytes=104857600}
[INFO] 2023-11-08 14:51:09,114 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-28 broker=0] No checkpointed highwatermark is found for partition __consumer_offsets-28
[INFO] 2023-11-08 14:51:09,114 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition __consumer_offsets-28 broker=0] Log loaded for partition __consumer_offsets-28 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,114 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower __consumer_offsets-28 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,116 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(__consumer_offsets-15, __consumer_offsets-48, __consumer_offsets-13, __consumer_offsets-46, __consumer_offsets-9, __consumer_offsets-42, __consumer_offsets-21, __consumer_offsets-19, __consumer_offsets-30, __consumer_offsets-28, __consumer_offsets-7, __consumer_offsets-40, __consumer_offsets-3, __consumer_offsets-36, __consumer_offsets-1, __consumer_offsets-34, __consumer_offsets-16, __consumer_offsets-45, __consumer_offsets-43, __consumer_offsets-12, __consumer_offsets-10, __consumer_offsets-24, __consumer_offsets-22, __consumer_offsets-49, __consumer_offsets-18, __consumer_offsets-31, __consumer_offsets-0, __consumer_offsets-27, __consumer_offsets-25, __consumer_offsets-39, __consumer_offsets-37, __consumer_offsets-6, __consumer_offsets-4, __consumer_offsets-33)
[INFO] 2023-11-08 14:51:09,118 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 1 for 34 partitions
[INFO] 2023-11-08 14:51:09,144 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Starting
[DEBUG] 2023-11-08 14:51:09,175 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[INFO] 2023-11-08 14:51:09,175 [metadata-service-coordinator] org.apache.kafka.clients.NetworkClient disconnect - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Client requested disconnect from node 2147483647
[INFO] 2023-11-08 14:51:09,178 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(__consumer_offsets-22 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-25 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-49 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-31 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-37 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-19 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-13 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-43 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-34 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-46 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-16 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-28 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-40 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:09,182 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0
[DEBUG] 2023-11-08 14:51:09,182 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455069182, latencyMs=7, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-0, correlationId=18), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=0, host='kafka-0.kafka.confluent.svc.cluster.local', port=9072, errorCode=0, errorMessage='')]))
[INFO] 2023-11-08 14:51:09,182 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Discovered group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,182 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null) is unavailable or invalid due to cause: coordinator unavailable.isDisconnected: false. Rediscovery will be attempted.
[INFO] 2023-11-08 14:51:09,182 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator markCoordinatorUnknown - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Requesting disconnect from last known coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,183 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Starting
[INFO] 2023-11-08 14:51:09,183 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(__consumer_offsets-30 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-21 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-33 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-36 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-48 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-45 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-27 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-42 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-18 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-15 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-24 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-39 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), __consumer_offsets-12 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:09,184 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-15 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,184 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-16, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,184 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-15, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,188 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-48 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,188 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,188 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-48, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,188 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-13, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,189 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-45 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,189 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-46 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,189 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-45, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,189 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-46, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,189 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-12 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,189 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-43 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,189 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-12, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-43, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-42 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-42, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-22, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-24 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,190 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-24, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-19, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-21 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-49 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-21, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-49, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-18 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-31 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-18, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-31, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,191 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-28 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-28, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-30 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-25 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-30, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-25, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-27 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-27, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,192 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-39 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,193 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-40 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,193 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-39, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,193 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-40, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,193 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,193 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-37 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,193 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-37, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,193 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,193 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,194 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,194 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,194 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition __consumer_offsets-34 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,194 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-34, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,193 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,197 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,198 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,198 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-36 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,198 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-36, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,198 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition __consumer_offsets-33 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,198 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=__consumer_offsets-33, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,204 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 35 in epoch 0
[INFO] 2023-11-08 14:51:09,205 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-35 for epoch 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 5 in epoch 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-5 for epoch 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 20 in epoch 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-20 for epoch 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 41 in epoch 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-41 for epoch 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 29 in epoch 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-29 for epoch 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 44 in epoch 0
[INFO] 2023-11-08 14:51:09,207 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-44 for epoch 0
[INFO] 2023-11-08 14:51:09,208 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 14 in epoch 0
[INFO] 2023-11-08 14:51:09,208 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-14 for epoch 0
[INFO] 2023-11-08 14:51:09,208 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 2 in epoch 0
[INFO] 2023-11-08 14:51:09,208 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-2 for epoch 0
[INFO] 2023-11-08 14:51:09,208 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 23 in epoch 0
[INFO] 2023-11-08 14:51:09,208 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-23 for epoch 0
[INFO] 2023-11-08 14:51:09,208 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 38 in epoch 0
[INFO] 2023-11-08 14:51:09,208 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-38 for epoch 0
[INFO] 2023-11-08 14:51:09,208 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 8 in epoch 0
[INFO] 2023-11-08 14:51:09,208 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-8 for epoch 0
[INFO] 2023-11-08 14:51:09,209 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 11 in epoch 0
[INFO] 2023-11-08 14:51:09,209 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-11 for epoch 0
[INFO] 2023-11-08 14:51:09,209 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-35 is aborted and paused
[INFO] 2023-11-08 14:51:09,209 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 26 in epoch 0
[INFO] 2023-11-08 14:51:09,209 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-26 for epoch 0
[INFO] 2023-11-08 14:51:09,209 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 47 in epoch 0
[INFO] 2023-11-08 14:51:09,209 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-47 for epoch 0
[INFO] 2023-11-08 14:51:09,209 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 17 in epoch 0
[INFO] 2023-11-08 14:51:09,209 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-17 for epoch 0
[INFO] 2023-11-08 14:51:09,209 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Elected as the group coordinator for partition 32 in epoch 0
[INFO] 2023-11-08 14:51:09,209 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling loading of offsets and group metadata from __consumer_offsets-32 for epoch 0
[INFO] 2023-11-08 14:51:09,210 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 3 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,211 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-3
[INFO] 2023-11-08 14:51:09,211 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 18 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-18
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 37 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-37
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 7 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-7
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 22 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-22
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 10 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-10
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 33 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-33
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 48 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-48
[INFO] 2023-11-08 14:51:09,212 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 19 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-19
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 34 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-34
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 4 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-4
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 45 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-45
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 15 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-15
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 30 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-30
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 49 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,213 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-49
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 0 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-0
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 39 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-39
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 9 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-9
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 24 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-24
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 27 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-27
[INFO] 2023-11-08 14:51:09,214 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 42 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,215 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-42
[INFO] 2023-11-08 14:51:09,215 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 12 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,215 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-12
[INFO] 2023-11-08 14:51:09,215 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-35 in 9 milliseconds for epoch 0, of which 4 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,215 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 31 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,216 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-31
[INFO] 2023-11-08 14:51:09,216 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 46 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,216 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-46
[INFO] 2023-11-08 14:51:09,216 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 1 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,216 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-1
[INFO] 2023-11-08 14:51:09,216 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 16 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,216 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-35 is resumed
[INFO] 2023-11-08 14:51:09,216 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-5 is aborted and paused
[INFO] 2023-11-08 14:51:09,217 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-5 in 10 milliseconds for epoch 0, of which 9 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,217 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-5 is resumed
[INFO] 2023-11-08 14:51:09,217 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-20 is aborted and paused
[INFO] 2023-11-08 14:51:09,217 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-20 in 10 milliseconds for epoch 0, of which 10 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,217 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-20 is resumed
[INFO] 2023-11-08 14:51:09,217 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-41 is aborted and paused
[INFO] 2023-11-08 14:51:09,218 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-41 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,218 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-41 is resumed
[INFO] 2023-11-08 14:51:09,218 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-29 is aborted and paused
[INFO] 2023-11-08 14:51:09,219 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-29 in 12 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,219 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-29 is resumed
[INFO] 2023-11-08 14:51:09,219 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-44 is aborted and paused
[INFO] 2023-11-08 14:51:09,219 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-44 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,219 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-44 is resumed
[INFO] 2023-11-08 14:51:09,219 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-14 is aborted and paused
[INFO] 2023-11-08 14:51:09,219 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-14 in 11 milliseconds for epoch 0, of which 11 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,219 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-14 is resumed
[INFO] 2023-11-08 14:51:09,220 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-2 is aborted and paused
[INFO] 2023-11-08 14:51:09,220 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-2 in 12 milliseconds for epoch 0, of which 12 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,220 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-2 is resumed
[INFO] 2023-11-08 14:51:09,220 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-23 is aborted and paused
[INFO] 2023-11-08 14:51:09,220 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-23 in 12 milliseconds for epoch 0, of which 12 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,220 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-23 is resumed
[INFO] 2023-11-08 14:51:09,220 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-38 is aborted and paused
[INFO] 2023-11-08 14:51:09,220 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-38 in 12 milliseconds for epoch 0, of which 12 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,221 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-38 is resumed
[INFO] 2023-11-08 14:51:09,221 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-8 is aborted and paused
[INFO] 2023-11-08 14:51:09,221 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-8 in 13 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,221 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-8 is resumed
[INFO] 2023-11-08 14:51:09,221 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-11 is aborted and paused
[INFO] 2023-11-08 14:51:09,221 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-11 in 12 milliseconds for epoch 0, of which 12 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,221 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-11 is resumed
[INFO] 2023-11-08 14:51:09,222 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-26 is aborted and paused
[INFO] 2023-11-08 14:51:09,222 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-26 in 13 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,222 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-26 is resumed
[INFO] 2023-11-08 14:51:09,222 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-47 is aborted and paused
[INFO] 2023-11-08 14:51:09,222 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-47 in 13 milliseconds for epoch 0, of which 13 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,216 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-16
[INFO] 2023-11-08 14:51:09,222 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 21 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,222 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-21
[INFO] 2023-11-08 14:51:09,223 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 36 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,223 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-36
[INFO] 2023-11-08 14:51:09,223 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 6 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,223 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-6
[INFO] 2023-11-08 14:51:09,223 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 25 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,223 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-25
[INFO] 2023-11-08 14:51:09,223 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 40 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,223 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-40
[INFO] 2023-11-08 14:51:09,223 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 43 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,223 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-43
[INFO] 2023-11-08 14:51:09,224 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 13 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,224 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-13
[INFO] 2023-11-08 14:51:09,224 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Resigned as the group coordinator for partition 28 in epoch Some(0)
[INFO] 2023-11-08 14:51:09,224 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-47 is resumed
[INFO] 2023-11-08 14:51:09,224 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-17 is aborted and paused
[INFO] 2023-11-08 14:51:09,224 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-17 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,224 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-17 is resumed
[INFO] 2023-11-08 14:51:09,224 [group-metadata-manager-0] kafka.log.LogManager info - The cleaning for partition __consumer_offsets-32 is aborted and paused
[INFO] 2023-11-08 14:51:09,225 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished loading offsets and group metadata from __consumer_offsets-32 in 15 milliseconds for epoch 0, of which 15 milliseconds was spent in the scheduler.
[INFO] 2023-11-08 14:51:09,225 [group-metadata-manager-0] kafka.log.LogManager info - Cleaning for partition __consumer_offsets-32 is resumed
[INFO] 2023-11-08 14:51:09,226 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-3 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,226 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-18 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,226 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-37 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,227 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-7 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,227 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-22 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,227 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-10 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,227 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-33 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,227 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-48 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,227 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-19 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,227 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-34 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,227 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-4 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-45 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-15 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-30 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-49 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-0 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-39 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-9 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-24 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-27 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-42 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,228 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Scheduling unloading of offsets and group metadata from __consumer_offsets-28
[INFO] 2023-11-08 14:51:09,228 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-12 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,229 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-31 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,229 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-46 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,229 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-1 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,229 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-16 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,229 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-21 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,229 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-36 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,229 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-6 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,229 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-25 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,229 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-40 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,230 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-43 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,230 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-13 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,230 [group-metadata-manager-0] kafka.coordinator.group.GroupMetadataManager info - [GroupMetadataManager brokerId=0] Finished unloading __consumer_offsets-28 for coordinator epoch Some(0). Removed 0 cached offsets and 0 cached groups.
[INFO] 2023-11-08 14:51:09,231 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 791ms correlationId 1 from controller 1 for 50 partitions
[WARN] 2023-11-08 14:51:09,233 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-16. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,233 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-13. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,233 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-46. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,233 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-43. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,233 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-10. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,233 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-22. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,233 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-19. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,233 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-49. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,233 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-31. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,233 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-28. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,234 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-25. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,234 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-7. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,234 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-40. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,234 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-37. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,234 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-4. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,234 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-1. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,234 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition __consumer_offsets-34. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:51:09,243 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Add 50 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 2
[INFO] 2023-11-08 14:51:09,249 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 3 from controller 1 for 32 partitions
[DEBUG] 2023-11-08 14:51:09,283 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendFindCoordinatorRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending FindCoordinator request to broker kafka-1.kafka.confluent.svc.cluster.local:9072 (id: 1 rack: 1)
[DEBUG] 2023-11-08 14:51:09,286 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received FindCoordinator response ClientResponse(receivedTimeMs=1699455069285, latencyMs=2, disconnected=false, requestHeader=RequestHeader(apiKey=FIND_COORDINATOR, apiVersion=4, clientId=_confluent-metadata-coordinator-0, correlationId=19), responseBody=FindCoordinatorResponseData(throttleTimeMs=0, errorCode=0, errorMessage='', nodeId=0, host='', port=0, coordinators=[Coordinator(key='_confluent-metadata-coordinator-group', nodeId=0, host='kafka-0.kafka.confluent.svc.cluster.local', port=9072, errorCode=0, errorMessage='')]))
[INFO] 2023-11-08 14:51:09,286 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onSuccess - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Discovered group coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,287 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent_balancer_partition_samples-18, _confluent_balancer_partition_samples-27, _confluent_balancer_partition_samples-6, _confluent_balancer_partition_samples-30, _confluent_balancer_partition_samples-9, _confluent_balancer_partition_samples-3, _confluent_balancer_partition_samples-24, _confluent_balancer_partition_samples-12, _confluent_balancer_partition_samples-21, _confluent_balancer_partition_samples-15, _confluent_balancer_partition_samples-0)
[INFO] 2023-11-08 14:51:09,288 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group](Re-)joining group
[INFO] 2023-11-08 14:51:09,289 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 3 from controller 1 epoch 1 as part of the become-leader transition for 11 partitions
[DEBUG] 2023-11-08 14:51:09,289 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending JoinGroup (JoinGroupRequestData(groupId='_confluent-metadata-coordinator-group', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='', groupInstanceId=null, protocolType='metadata-service', protocols=[JoinGroupRequestProtocol(name='v0', metadata=[123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 48, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125])])) to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,298 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-metadata-coordinator-group in Empty state. Created a new member id _confluent-metadata-coordinator-1-042022d4-5dfd-4955-aa0c-2ff1512f68b3 and request the member to rejoin with this id.
[INFO] 2023-11-08 14:51:09,306 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-12, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,310 [data-plane-kafka-request-handler-7] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-metadata-coordinator-group in Empty state. Created a new member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d and request the member to rejoin with this id.
[INFO] 2023-11-08 14:51:09,310 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-12 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-12 with properties {cleanup.policy=delete, retention.ms=3600000}
[DEBUG] 2023-11-08 14:51:09,313 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]JoinGroup failed due to non-fatal error: MEMBER_ID_REQUIRED Will set the member id as _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d and then rejoin. Sent generation was  Generation{generationId=-1, memberId='', protocol='null'}
[INFO] 2023-11-08 14:51:09,314 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator requestRejoin - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Request joining group due to: need to re-join with the given member-id
[INFO] 2023-11-08 14:51:09,314 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group](Re-)joining group
[DEBUG] 2023-11-08 14:51:09,314 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendJoinGroupRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending JoinGroup (JoinGroupRequestData(groupId='_confluent-metadata-coordinator-group', sessionTimeoutMs=45000, rebalanceTimeoutMs=300000, memberId='_confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d', groupInstanceId=null, protocolType='metadata-service', protocols=[JoinGroupRequestProtocol(name='v0', metadata=[123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 48, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125])])) to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:51:09,314 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-12 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-12
[INFO] 2023-11-08 14:51:09,316 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-12 broker=0] Log loaded for partition _confluent_balancer_partition_samples-12 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,316 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,328 [data-plane-kafka-request-handler-4] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Preparing to rebalance group _confluent-metadata-coordinator-group in state PreparingRebalance with old generation 0 (__consumer_offsets-44) (reason: Adding new member _confluent-metadata-coordinator-1-042022d4-5dfd-4955-aa0c-2ff1512f68b3 with group instance id None)
[INFO] 2023-11-08 14:51:09,331 [data-plane-kafka-request-handler-5] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-metadata-coordinator-group in PreparingRebalance state. Created a new member id _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb and request the member to rejoin with this id.
[INFO] 2023-11-08 14:51:09,333 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-27, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,336 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-27 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-27 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,336 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-27 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-27
[INFO] 2023-11-08 14:51:09,336 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-27 broker=0] Log loaded for partition _confluent_balancer_partition_samples-27 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,336 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-27 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,342 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-24, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,343 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-24 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-24 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,343 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-24 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-24
[INFO] 2023-11-08 14:51:09,343 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-24 broker=0] Log loaded for partition _confluent_balancer_partition_samples-24 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,343 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,348 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,349 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-9 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-9 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,349 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-9 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-9
[INFO] 2023-11-08 14:51:09,349 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-9 broker=0] Log loaded for partition _confluent_balancer_partition_samples-9 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,349 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,354 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,355 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-6 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-6 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,355 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-6 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-6
[INFO] 2023-11-08 14:51:09,355 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-6 broker=0] Log loaded for partition _confluent_balancer_partition_samples-6 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,355 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,360 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-21, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,361 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-21 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-21 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,361 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-21 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-21
[INFO] 2023-11-08 14:51:09,361 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-21 broker=0] Log loaded for partition _confluent_balancer_partition_samples-21 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,361 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,366 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-18, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,367 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-18 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-18 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,367 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-18 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-18
[INFO] 2023-11-08 14:51:09,367 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-18 broker=0] Log loaded for partition _confluent_balancer_partition_samples-18 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,367 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,373 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,374 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-3 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-3 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,374 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-3 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-3
[INFO] 2023-11-08 14:51:09,374 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-3 broker=0] Log loaded for partition _confluent_balancer_partition_samples-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,375 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,380 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,381 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-0 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-0 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,381 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-0 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-0
[INFO] 2023-11-08 14:51:09,381 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-0 broker=0] Log loaded for partition _confluent_balancer_partition_samples-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,382 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,387 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-30, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,388 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-30 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-30 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,388 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-30 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-30
[INFO] 2023-11-08 14:51:09,388 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-30 broker=0] Log loaded for partition _confluent_balancer_partition_samples-30 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,388 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-30 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,393 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-15, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,394 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-15 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-15 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,395 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-15 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-15
[INFO] 2023-11-08 14:51:09,395 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-15 broker=0] Log loaded for partition _confluent_balancer_partition_samples-15 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,395 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent_balancer_partition_samples-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,400 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-28, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,401 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-28 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-28 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,401 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-28 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-28
[INFO] 2023-11-08 14:51:09,401 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-28 broker=0] Log loaded for partition _confluent_balancer_partition_samples-28 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,401 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-28 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,407 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-13, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,407 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-13 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-13 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,408 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-13 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-13
[INFO] 2023-11-08 14:51:09,408 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-13 broker=0] Log loaded for partition _confluent_balancer_partition_samples-13 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,408 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-13 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,413 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-26, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,414 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-26 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-26 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,415 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-26 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-26
[INFO] 2023-11-08 14:51:09,415 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-26 broker=0] Log loaded for partition _confluent_balancer_partition_samples-26 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,415 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-26 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,420 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,420 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-11 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-11 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,421 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-11 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-11
[INFO] 2023-11-08 14:51:09,421 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-11 broker=0] Log loaded for partition _confluent_balancer_partition_samples-11 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,421 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,426 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-22, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,427 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-22 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-22 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,427 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-22 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-22
[INFO] 2023-11-08 14:51:09,427 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-22 broker=0] Log loaded for partition _confluent_balancer_partition_samples-22 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,427 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-22 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,432 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,433 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-7 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-7 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,434 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-7 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-7
[INFO] 2023-11-08 14:51:09,434 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-7 broker=0] Log loaded for partition _confluent_balancer_partition_samples-7 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,434 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,440 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,440 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-4 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-4 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,440 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-4 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-4
[INFO] 2023-11-08 14:51:09,441 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-4 broker=0] Log loaded for partition _confluent_balancer_partition_samples-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,441 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,446 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,447 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-2 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-2 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,447 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-2 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-2
[INFO] 2023-11-08 14:51:09,447 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-2 broker=0] Log loaded for partition _confluent_balancer_partition_samples-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,447 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,453 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-19, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,454 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-19 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-19 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,454 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-19 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-19
[INFO] 2023-11-08 14:51:09,454 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-19 broker=0] Log loaded for partition _confluent_balancer_partition_samples-19 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,454 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-19 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,459 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-17, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,460 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-17 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-17 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,460 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-17 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-17
[INFO] 2023-11-08 14:51:09,461 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-17 broker=0] Log loaded for partition _confluent_balancer_partition_samples-17 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,461 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-17 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,466 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-29, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,467 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-29 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-29 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,467 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-29 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-29
[INFO] 2023-11-08 14:51:09,467 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-29 broker=0] Log loaded for partition _confluent_balancer_partition_samples-29 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,468 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-29 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,473 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,474 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-10 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-10 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,474 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-10 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-10
[INFO] 2023-11-08 14:51:09,474 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-10 broker=0] Log loaded for partition _confluent_balancer_partition_samples-10 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,474 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,480 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,481 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-8 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-8 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,482 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-8 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-8
[INFO] 2023-11-08 14:51:09,482 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-8 broker=0] Log loaded for partition _confluent_balancer_partition_samples-8 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,482 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,489 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-25, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,490 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-25 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-25 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,490 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-25 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-25
[INFO] 2023-11-08 14:51:09,491 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-25 broker=0] Log loaded for partition _confluent_balancer_partition_samples-25 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,491 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-25 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,496 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-23, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,497 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-23 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-23 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,497 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-23 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-23
[INFO] 2023-11-08 14:51:09,498 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-23 broker=0] Log loaded for partition _confluent_balancer_partition_samples-23 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,498 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-23 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,502 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-20, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,503 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-20 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-20 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,504 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-20 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-20
[INFO] 2023-11-08 14:51:09,504 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-20 broker=0] Log loaded for partition _confluent_balancer_partition_samples-20 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,504 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-20 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,509 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,511 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-5 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-5 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,511 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-5 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-5
[INFO] 2023-11-08 14:51:09,511 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-5 broker=0] Log loaded for partition _confluent_balancer_partition_samples-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,511 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,516 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-16, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,517 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-16 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-16 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,518 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-16 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-16
[INFO] 2023-11-08 14:51:09,518 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-16 broker=0] Log loaded for partition _confluent_balancer_partition_samples-16 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,518 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-16 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,522 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,523 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-1 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-1 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,523 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-1 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-1
[INFO] 2023-11-08 14:51:09,523 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-1 broker=0] Log loaded for partition _confluent_balancer_partition_samples-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,523 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,527 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-14, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,528 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-14 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-14 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,528 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-14 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-14
[INFO] 2023-11-08 14:51:09,528 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-14 broker=0] Log loaded for partition _confluent_balancer_partition_samples-14 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,528 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-14 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,533 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_partition_samples-31, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,534 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent_balancer_partition_samples-31 in /mnt/data/data0/logs/_confluent_balancer_partition_samples-31 with properties {cleanup.policy=delete, retention.ms=3600000}
[INFO] 2023-11-08 14:51:09,534 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-31 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_partition_samples-31
[INFO] 2023-11-08 14:51:09,534 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent_balancer_partition_samples-31 broker=0] Log loaded for partition _confluent_balancer_partition_samples-31 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,534 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent_balancer_partition_samples-31 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,535 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent_balancer_partition_samples-5, _confluent_balancer_partition_samples-8, _confluent_balancer_partition_samples-7, _confluent_balancer_partition_samples-10, _confluent_balancer_partition_samples-11, _confluent_balancer_partition_samples-29, _confluent_balancer_partition_samples-31, _confluent_balancer_partition_samples-2, _confluent_balancer_partition_samples-1, _confluent_balancer_partition_samples-4, _confluent_balancer_partition_samples-22, _confluent_balancer_partition_samples-23, _confluent_balancer_partition_samples-26, _confluent_balancer_partition_samples-25, _confluent_balancer_partition_samples-28, _confluent_balancer_partition_samples-14, _confluent_balancer_partition_samples-13, _confluent_balancer_partition_samples-16, _confluent_balancer_partition_samples-17, _confluent_balancer_partition_samples-20, _confluent_balancer_partition_samples-19)
[INFO] 2023-11-08 14:51:09,535 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 3 for 21 partitions
[INFO] 2023-11-08 14:51:09,543 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,543 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent_balancer_partition_samples-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-29 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-26 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-14 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-23 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-20 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-17 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:09,544 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,544 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent_balancer_partition_samples-28 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-13 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-31 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-19 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-22 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-25 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-16 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_partition_samples-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:09,544 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,545 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,545 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-23 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,546 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-23, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,546 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-26 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,546 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-26, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,546 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,547 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,547 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-14 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,547 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-14, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,547 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-29 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,548 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-29, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,548 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,548 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,548 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-17 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,548 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-17, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,549 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-20 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,549 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-20, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,550 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 302ms correlationId 3 from controller 1 for 32 partitions
[WARN] 2023-11-08 14:51:09,553 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_partition_samples-5. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,553 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_partition_samples-11. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,553 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_partition_samples-26. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,553 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_partition_samples-14. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,553 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_partition_samples-20. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,553 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_partition_samples-8. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,554 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_partition_samples-29. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,554 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_partition_samples-2. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,554 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_partition_samples-23. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,554 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_partition_samples-17. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:51:09,555 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Add 32 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 4
[INFO] 2023-11-08 14:51:09,559 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 5 from controller 1 for 32 partitions
[INFO] 2023-11-08 14:51:09,598 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent_balancer_broker_samples-25, _confluent_balancer_broker_samples-13, _confluent_balancer_broker_samples-7, _confluent_balancer_broker_samples-16, _confluent_balancer_broker_samples-22, _confluent_balancer_broker_samples-1, _confluent_balancer_broker_samples-10, _confluent_balancer_broker_samples-19, _confluent_balancer_broker_samples-4, _confluent_balancer_broker_samples-31, _confluent_balancer_broker_samples-28)
[INFO] 2023-11-08 14:51:09,598 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 5 from controller 1 epoch 1 as part of the become-leader transition for 11 partitions
[INFO] 2023-11-08 14:51:09,604 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-28, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,605 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-28 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-28 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,606 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-28 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-28
[INFO] 2023-11-08 14:51:09,607 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-28 broker=0] Log loaded for partition _confluent_balancer_broker_samples-28 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,607 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-28 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,612 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-25, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,613 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-25 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-25 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,613 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-25 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-25
[INFO] 2023-11-08 14:51:09,613 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-25 broker=0] Log loaded for partition _confluent_balancer_broker_samples-25 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,613 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-25 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,617 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,618 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-10 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-10 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,619 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-10 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-10
[INFO] 2023-11-08 14:51:09,619 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-10 broker=0] Log loaded for partition _confluent_balancer_broker_samples-10 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,619 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,623 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,626 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-7 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-7 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,626 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-7 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-7
[INFO] 2023-11-08 14:51:09,626 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-7 broker=0] Log loaded for partition _confluent_balancer_broker_samples-7 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,627 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,631 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-22, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,631 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-22 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-22 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,632 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-22 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-22
[INFO] 2023-11-08 14:51:09,632 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-22 broker=0] Log loaded for partition _confluent_balancer_broker_samples-22 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,632 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-22 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,636 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-19, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,636 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-19 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-19 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,637 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-19 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-19
[INFO] 2023-11-08 14:51:09,637 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-19 broker=0] Log loaded for partition _confluent_balancer_broker_samples-19 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,637 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-19 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,641 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,642 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-4 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-4 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,642 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-4 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-4
[INFO] 2023-11-08 14:51:09,642 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-4 broker=0] Log loaded for partition _confluent_balancer_broker_samples-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,642 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,647 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,648 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-1 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-1 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,648 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-1 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-1
[INFO] 2023-11-08 14:51:09,648 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-1 broker=0] Log loaded for partition _confluent_balancer_broker_samples-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,648 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,653 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-31, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,653 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-31 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-31 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,653 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-31 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-31
[INFO] 2023-11-08 14:51:09,653 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-31 broker=0] Log loaded for partition _confluent_balancer_broker_samples-31 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,654 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-31 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,658 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-16, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,658 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-16 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-16 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,658 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-16 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-16
[INFO] 2023-11-08 14:51:09,658 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-16 broker=0] Log loaded for partition _confluent_balancer_broker_samples-16 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,659 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-16 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,663 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-13, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,664 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-13 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-13 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,664 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-13 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-13
[INFO] 2023-11-08 14:51:09,664 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-13 broker=0] Log loaded for partition _confluent_balancer_broker_samples-13 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,664 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent_balancer_broker_samples-13 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,668 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-27, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,669 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-27 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-27 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,669 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-27 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-27
[INFO] 2023-11-08 14:51:09,669 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-27 broker=0] Log loaded for partition _confluent_balancer_broker_samples-27 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,669 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-27 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,673 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-12, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,674 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-12 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-12 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,674 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-12 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-12
[INFO] 2023-11-08 14:51:09,674 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-12 broker=0] Log loaded for partition _confluent_balancer_broker_samples-12 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,675 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-12 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,678 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-23, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,679 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-23 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-23 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,679 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-23 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-23
[INFO] 2023-11-08 14:51:09,679 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-23 broker=0] Log loaded for partition _confluent_balancer_broker_samples-23 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,679 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-23 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,683 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,684 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-8 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-8 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,684 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-8 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-8
[INFO] 2023-11-08 14:51:09,684 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-8 broker=0] Log loaded for partition _confluent_balancer_broker_samples-8 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,684 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,687 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-21, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,688 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-21 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-21 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,688 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-21 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-21
[INFO] 2023-11-08 14:51:09,688 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-21 broker=0] Log loaded for partition _confluent_balancer_broker_samples-21 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,688 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-21 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,691 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,692 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-6 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-6 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,692 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-6 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-6
[INFO] 2023-11-08 14:51:09,692 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-6 broker=0] Log loaded for partition _confluent_balancer_broker_samples-6 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,692 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,695 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,696 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-3 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-3 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,696 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-3 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-3
[INFO] 2023-11-08 14:51:09,696 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-3 broker=0] Log loaded for partition _confluent_balancer_broker_samples-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,696 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,699 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-20, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,700 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-20 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-20 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,700 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-20 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-20
[INFO] 2023-11-08 14:51:09,700 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-20 broker=0] Log loaded for partition _confluent_balancer_broker_samples-20 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,700 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-20 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,703 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-18, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,704 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-18 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-18 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,704 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-18 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-18
[INFO] 2023-11-08 14:51:09,704 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-18 broker=0] Log loaded for partition _confluent_balancer_broker_samples-18 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,704 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-18 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,707 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-29, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,708 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-29 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-29 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,708 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-29 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-29
[INFO] 2023-11-08 14:51:09,708 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-29 broker=0] Log loaded for partition _confluent_balancer_broker_samples-29 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,708 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-29 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,711 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-14, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,712 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-14 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-14 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,712 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-14 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-14
[INFO] 2023-11-08 14:51:09,712 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-14 broker=0] Log loaded for partition _confluent_balancer_broker_samples-14 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,712 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-14 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,715 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,716 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-11 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-11 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,716 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-11 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-11
[INFO] 2023-11-08 14:51:09,716 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-11 broker=0] Log loaded for partition _confluent_balancer_broker_samples-11 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,716 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,719 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,720 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-9 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-9 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,720 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-9 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-9
[INFO] 2023-11-08 14:51:09,720 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-9 broker=0] Log loaded for partition _confluent_balancer_broker_samples-9 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,720 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,724 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-26, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,724 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-26 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-26 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,724 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-26 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-26
[INFO] 2023-11-08 14:51:09,724 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-26 broker=0] Log loaded for partition _confluent_balancer_broker_samples-26 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,725 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-26 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,728 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-24, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,728 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-24 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-24 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,728 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-24 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-24
[INFO] 2023-11-08 14:51:09,729 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-24 broker=0] Log loaded for partition _confluent_balancer_broker_samples-24 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,729 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-24 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,732 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,733 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-5 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-5 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,733 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-5 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-5
[INFO] 2023-11-08 14:51:09,733 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-5 broker=0] Log loaded for partition _confluent_balancer_broker_samples-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,733 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,737 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-17, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,738 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-17 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-17 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,738 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-17 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-17
[INFO] 2023-11-08 14:51:09,738 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-17 broker=0] Log loaded for partition _confluent_balancer_broker_samples-17 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,738 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-17 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,742 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,742 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-2 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-2 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,742 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-2 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-2
[INFO] 2023-11-08 14:51:09,742 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-2 broker=0] Log loaded for partition _confluent_balancer_broker_samples-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,743 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,747 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-15, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,748 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-15 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-15 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,748 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-15 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-15
[INFO] 2023-11-08 14:51:09,748 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-15 broker=0] Log loaded for partition _confluent_balancer_broker_samples-15 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,748 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-15 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,752 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,753 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-0 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-0 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,753 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-0 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-0
[INFO] 2023-11-08 14:51:09,753 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-0 broker=0] Log loaded for partition _confluent_balancer_broker_samples-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,753 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,757 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_broker_samples-30, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:09,757 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent_balancer_broker_samples-30 in /mnt/data/data0/logs/_confluent_balancer_broker_samples-30 with properties {cleanup.policy=delete, retention.ms=12000000}
[INFO] 2023-11-08 14:51:09,757 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-30 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_broker_samples-30
[INFO] 2023-11-08 14:51:09,758 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent_balancer_broker_samples-30 broker=0] Log loaded for partition _confluent_balancer_broker_samples-30 with initial high watermark 0
[INFO] 2023-11-08 14:51:09,758 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent_balancer_broker_samples-30 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:09,758 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent_balancer_broker_samples-23, _confluent_balancer_broker_samples-24, _confluent_balancer_broker_samples-27, _confluent_balancer_broker_samples-26, _confluent_balancer_broker_samples-29, _confluent_balancer_broker_samples-15, _confluent_balancer_broker_samples-14, _confluent_balancer_broker_samples-17, _confluent_balancer_broker_samples-18, _confluent_balancer_broker_samples-21, _confluent_balancer_broker_samples-20, _confluent_balancer_broker_samples-6, _confluent_balancer_broker_samples-9, _confluent_balancer_broker_samples-8, _confluent_balancer_broker_samples-11, _confluent_balancer_broker_samples-12, _confluent_balancer_broker_samples-30, _confluent_balancer_broker_samples-0, _confluent_balancer_broker_samples-3, _confluent_balancer_broker_samples-2, _confluent_balancer_broker_samples-5)
[INFO] 2023-11-08 14:51:09,758 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 5 for 21 partitions
[INFO] 2023-11-08 14:51:09,760 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent_balancer_broker_samples-24 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-21 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-15 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-30 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-18 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-12 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-27 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:09,760 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,761 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,761 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent_balancer_broker_samples-17 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-14 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-26 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-23 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-20 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-29 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_broker_samples-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:09,761 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,761 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,761 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-24 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,761 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-24, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,761 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-27 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,761 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-27, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,761 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-12 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,762 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-12, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,762 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-15 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,762 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-15, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,762 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-30 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,762 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-30, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,762 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,762 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,762 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,762 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,763 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-18 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,763 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-18, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,763 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-21 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,763 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-21, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,764 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 204ms correlationId 5 from controller 1 for 32 partitions
[WARN] 2023-11-08 14:51:09,766 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-27. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-15. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-21. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-6. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-12. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:51:09,767 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Add 32 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 6
[WARN] 2023-11-08 14:51:09,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-0. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-24. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-18. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-9. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-30. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:51:09,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent_balancer_broker_samples-3. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:51:09,798 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-23 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,798 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-23, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,798 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-26 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-26, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-29 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-29, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-14 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-14, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-17 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-17, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-31 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,799 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-31, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-20 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-20, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-22 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-22, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,800 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-25 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-25, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-28 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-28, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-13 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-13, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-16 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-16, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_broker_samples-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_broker_samples-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent_balancer_partition_samples-19 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:09,801 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_partition_samples-19, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:15,337 [executor-Rebalance] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Stabilized group _confluent-metadata-coordinator-group generation 1 (__consumer_offsets-44) with 3 members
[DEBUG] 2023-11-08 14:51:15,340 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful JoinGroup response: JoinGroupResponseData(throttleTimeMs=0, errorCode=0, generationId=1, protocolType='metadata-service', protocolName='v0', leader='_confluent-metadata-coordinator-1-042022d4-5dfd-4955-aa0c-2ff1512f68b3', memberId='_confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d', members=[])
[DEBUG] 2023-11-08 14:51:15,340 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator enable - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Enabling heartbeat thread
[INFO] 2023-11-08 14:51:15,340 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Successfully joined group with generation Generation{generationId=1, memberId='_confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d', protocol='v0'}
[DEBUG] 2023-11-08 14:51:15,340 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator onJoinFollower - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending follower SyncGroup to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null) at generation Generation{generationId=1, memberId='_confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d', protocol='v0'}: SyncGroupRequestData(groupId='_confluent-metadata-coordinator-group', generationId=1, memberId='_confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d', groupInstanceId=null, protocolType='metadata-service', protocolName='v0', assignments=[])
[INFO] 2023-11-08 14:51:15,355 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Assignment received from leader _confluent-metadata-coordinator-1-042022d4-5dfd-4955-aa0c-2ff1512f68b3 for group _confluent-metadata-coordinator-group for generation 1. The group has 3 members, 0 of which are static.
[DEBUG] 2023-11-08 14:51:15,566 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful SyncGroup response: SyncGroupResponseData(throttleTimeMs=0, errorCode=0, protocolType='metadata-service', protocolName='v0', assignment=[123, 34, 118, 101, 114, 115, 105, 111, 110, 34, 58, 48, 44, 34, 101, 114, 114, 111, 114, 34, 58, 48, 44, 34, 110, 111, 100, 101, 115, 34, 58, 123, 34, 95, 99, 111, 110, 102, 108, 117, 101, 110, 116, 45, 109, 101, 116, 97, 100, 97, 116, 97, 45, 99, 111, 111, 114, 100, 105, 110, 97, 116, 111, 114, 45, 49, 45, 48, 52, 50, 48, 50, 50, 100, 52, 45, 53, 100, 102, 100, 45, 52, 57, 53, 53, 45, 97, 97, 48, 99, 45, 50, 102, 102, 49, 53, 49, 50, 102, 54, 56, 98, 51, 34, 58, 123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 49, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125, 44, 34, 95, 99, 111, 110, 102, 108, 117, 101, 110, 116, 45, 109, 101, 116, 97, 100, 97, 116, 97, 45, 99, 111, 111, 114, 100, 105, 110, 97, 116, 111, 114, 45, 48, 45, 100, 99, 57, 101, 98, 102, 53, 102, 45, 48, 49, 52, 52, 45, 52, 57, 55, 55, 45, 97, 101, 99, 56, 45, 98, 102, 56, 100, 55, 55, 53, 97, 101, 56, 52, 100, 34, 58, 123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 48, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125, 44, 34, 95, 99, 111, 110, 102, 108, 117, 101, 110, 116, 45, 109, 101, 116, 97, 100, 97, 116, 97, 45, 99, 111, 111, 114, 100, 105, 110, 97, 116, 111, 114, 45, 50, 45, 48, 51, 52, 50, 52, 56, 97, 97, 45, 99, 98, 55, 53, 45, 52, 53, 101, 97, 45, 57, 54, 99, 53, 45, 101, 100, 57, 98, 49, 48, 48, 97, 57, 97, 100, 98, 34, 58, 123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 50, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125, 125, 44, 34, 119, 114, 105, 116, 101, 114, 77, 101, 109, 98, 101, 114, 73, 100, 34, 58, 34, 95, 99, 111, 110, 102, 108, 117, 101, 110, 116, 45, 109, 101, 116, 97, 100, 97, 116, 97, 45, 99, 111, 111, 114, 100, 105, 110, 97, 116, 111, 114, 45, 48, 45, 100, 99, 57, 101, 98, 102, 53, 102, 45, 48, 49, 52, 52, 45, 52, 57, 55, 55, 45, 97, 101, 99, 56, 45, 98, 102, 56, 100, 55, 55, 53, 97, 101, 56, 52, 100, 34, 44, 34, 119, 114, 105, 116, 101, 114, 78, 111, 100, 101, 77, 101, 116, 97, 100, 97, 116, 97, 34, 58, 123, 34, 117, 114, 108, 115, 34, 58, 91, 34, 104, 116, 116, 112, 115, 58, 47, 47, 107, 97, 102, 107, 97, 45, 48, 46, 107, 97, 102, 107, 97, 46, 99, 111, 110, 102, 108, 117, 101, 110, 116, 46, 115, 118, 99, 46, 99, 108, 117, 115, 116, 101, 114, 46, 108, 111, 99, 97, 108, 58, 56, 48, 57, 48, 34, 93, 125, 125])
[INFO] 2023-11-08 14:51:15,567 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Successfully synced group in generation Generation{generationId=1, memberId='_confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d', protocol='v0'}
[INFO] 2023-11-08 14:51:15,575 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager onAssigned - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Metadata writer assignment complete: generation MetadataServiceAssignment(error=0, nodes={_confluent-metadata-coordinator-1-042022d4-5dfd-4955-aa0c-2ff1512f68b3=NodeMetadata(urls={https=https://kafka-1.kafka.confluent.svc.cluster.local:8090}), _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d=NodeMetadata(urls={https=https://kafka-0.kafka.confluent.svc.cluster.local:8090}), _confluent-metadata-coordinator-2-034248aa-cb75-45ea-96c5-ed9b100a9adb=NodeMetadata(urls={https=https://kafka-2.kafka.confluent.svc.cluster.local:8090})}, writerMemberId='_confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d', writerNodeMetadata=NodeMetadata(urls={https=https://kafka-0.kafka.confluent.svc.cluster.local:8090}), version=0) assignment 1
[DEBUG] 2023-11-08 14:51:15,576 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataNodeManager run - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Wake up exception from poll
[INFO] 2023-11-08 14:51:15,576 [metadata-service-coordinator] io.confluent.security.auth.store.kafka.KafkaAuthWriter startWriter - Starting writer with generation 1
[INFO] 2023-11-08 14:51:15,584 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'auto.create.topics.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.recovery.threads.per.data.dir' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.server.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.mechanism.inter.broker.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'default.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.session.uuid' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.operator.managed' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,619 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'transaction.state.log.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.io.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'delete.topic.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.clientCnxnSocket' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.client.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.login.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.commit.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'authorizer.class.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'transaction.state.log.min.isr' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.request.max.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.retention.minutes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'inter.broker.protocol.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.security.protocol.map' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.retention.hours' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.partitions' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,620 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'inter.broker.listener.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.balancer.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'node.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.message.format.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,621 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.segment.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'group.max.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.dirs' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replica.lag.time.max.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.network.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.send.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.receive.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.rack' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.topic.compression.codec' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.retention.check.interval.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:15,622 [auth-writer-mgmt-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:15,623 [auth-writer-mgmt-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:15,623 [auth-writer-mgmt-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455075622
[DEBUG] 2023-11-08 14:51:15,671 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaUtils waitForTopic - Topic not found, attempting to create topic _confluent-metadata-auth
[INFO] 2023-11-08 14:51:15,673 [auth-writer-mgmt-1] io.confluent.security.auth.store.kafka.KafkaAuthWriter createAuthTopic - Creating auth topic (name=_confluent-metadata-auth, numPartitions=6, replicationFactor=3, replicasAssignments=null, configs={compression.type=producer, cleanup.policy=compact, min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false})
[INFO] 2023-11-08 14:51:15,727 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 7 from controller 1 for 6 partitions
[INFO] 2023-11-08 14:51:15,734 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-metadata-auth-4, _confluent-metadata-auth-1)
[INFO] 2023-11-08 14:51:15,734 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 7 from controller 1 epoch 1 as part of the become-leader transition for 2 partitions
[INFO] 2023-11-08 14:51:15,741 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,742 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-4 in /mnt/data/data0/logs/_confluent-metadata-auth-4 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,742 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-4 broker=0] No checkpointed highwatermark is found for partition _confluent-metadata-auth-4
[INFO] 2023-11-08 14:51:15,743 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-4 broker=0] Log loaded for partition _confluent-metadata-auth-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,743 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-metadata-auth-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,747 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,748 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-1 in /mnt/data/data0/logs/_confluent-metadata-auth-1 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,748 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-1 broker=0] No checkpointed highwatermark is found for partition _confluent-metadata-auth-1
[INFO] 2023-11-08 14:51:15,748 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-1 broker=0] Log loaded for partition _confluent-metadata-auth-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,749 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-metadata-auth-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,754 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,755 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-5 in /mnt/data/data0/logs/_confluent-metadata-auth-5 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,755 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-5 broker=0] No checkpointed highwatermark is found for partition _confluent-metadata-auth-5
[INFO] 2023-11-08 14:51:15,755 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-5 broker=0] Log loaded for partition _confluent-metadata-auth-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,755 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-metadata-auth-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,760 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,761 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-3 in /mnt/data/data0/logs/_confluent-metadata-auth-3 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,761 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-3 broker=0] No checkpointed highwatermark is found for partition _confluent-metadata-auth-3
[INFO] 2023-11-08 14:51:15,761 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-3 broker=0] Log loaded for partition _confluent-metadata-auth-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,761 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-metadata-auth-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:51:15,765 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaUtils waitForTopic - Topic _confluent-metadata-auth has the expected 6 partitions, returning
[INFO] 2023-11-08 14:51:15,766 [auth-reader-1] org.apache.kafka.clients.consumer.KafkaConsumer assign - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Subscribed to partition(s): _confluent-metadata-auth-1, _confluent-metadata-auth-0, _confluent-metadata-auth-3, _confluent-metadata-auth-2, _confluent-metadata-auth-5, _confluent-metadata-auth-4
[INFO] 2023-11-08 14:51:15,767 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,768 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-2 in /mnt/data/data0/logs/_confluent-metadata-auth-2 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,768 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-2 broker=0] No checkpointed highwatermark is found for partition _confluent-metadata-auth-2
[INFO] 2023-11-08 14:51:15,768 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-2 broker=0] Log loaded for partition _confluent-metadata-auth-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,768 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-metadata-auth-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,770 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-1
[INFO] 2023-11-08 14:51:15,770 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-0
[INFO] 2023-11-08 14:51:15,770 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-3
[INFO] 2023-11-08 14:51:15,771 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-2
[INFO] 2023-11-08 14:51:15,771 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-5
[INFO] 2023-11-08 14:51:15,771 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to LATEST offset of partition _confluent-metadata-auth-4
[INFO] 2023-11-08 14:51:15,773 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metadata-auth-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:15,774 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-metadata-auth-0 in /mnt/data/data0/logs/_confluent-metadata-auth-0 with properties {cleanup.policy=compact, compression.type="producer", min.insync.replicas=2, segment.bytes=10485760, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:15,774 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-0 broker=0] No checkpointed highwatermark is found for partition _confluent-metadata-auth-0
[INFO] 2023-11-08 14:51:15,774 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-metadata-auth-0 broker=0] Log loaded for partition _confluent-metadata-auth-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:15,774 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-metadata-auth-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:15,774 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-metadata-auth-0, _confluent-metadata-auth-3, _confluent-metadata-auth-2, _confluent-metadata-auth-5)
[INFO] 2023-11-08 14:51:15,774 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 7 for 4 partitions
[INFO] 2023-11-08 14:51:15,775 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions Map(_confluent-metadata-auth-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metadata-auth-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:15,776 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions Map(_confluent-metadata-auth-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metadata-auth-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:15,776 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 49ms correlationId 7 from controller 1 for 6 partitions
[INFO] 2023-11-08 14:51:15,779 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Add 6 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 8
[DEBUG] 2023-11-08 14:51:15,782 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaUtils waitForTopic - Topic _confluent-metadata-auth has the expected 6 partitions, returning
[INFO] 2023-11-08 14:51:15,783 [kafka-admin-client-thread | _confluent-metadata-admin-0] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for _confluent-metadata-admin-0 unregistered
[INFO] 2023-11-08 14:51:15,786 [kafka-admin-client-thread | _confluent-metadata-admin-0] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:15,787 [kafka-admin-client-thread | _confluent-metadata-admin-0] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:15,787 [kafka-admin-client-thread | _confluent-metadata-admin-0] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[DEBUG] 2023-11-08 14:51:15,799 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter start - [PartitionWriter _confluent-metadata-auth-0]Starting generation 1 for partition writer _confluent-metadata-auth-0
[DEBUG] 2023-11-08 14:51:15,799 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-0]Changing status from UNKNOWN to INITIALIZING
[DEBUG] 2023-11-08 14:51:15,800 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-0]writeStatus generation 1 status StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[INFO] 2023-11-08 14:51:15,819 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-metadata-auth-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:15,819 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metadata-auth-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:15,819 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-metadata-auth-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:15,819 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metadata-auth-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[DEBUG] 2023-11-08 14:51:15,826 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader initialize - auth topic partitions : [_confluent-metadata-auth-1, _confluent-metadata-auth-0, _confluent-metadata-auth-3, _confluent-metadata-auth-2, _confluent-metadata-auth-5, _confluent-metadata-auth-4]
[INFO] 2023-11-08 14:51:15,827 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-1
[INFO] 2023-11-08 14:51:15,827 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-0
[INFO] 2023-11-08 14:51:15,827 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-3
[INFO] 2023-11-08 14:51:15,827 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-2
[INFO] 2023-11-08 14:51:15,827 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-5
[INFO] 2023-11-08 14:51:15,827 [auth-reader-1] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-metadata-auth-consumer-0, groupId=null] Seeking to EARLIEST offset of partition _confluent-metadata-auth-4
[INFO] 2023-11-08 14:51:15,848 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-metadata-auth-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:15,848 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metadata-auth-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:15,848 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-metadata-auth-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:15,849 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metadata-auth-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[DEBUG] 2023-11-08 14:51:15,871 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter start - [PartitionWriter _confluent-metadata-auth-1]Starting generation 1 for partition writer _confluent-metadata-auth-1
[DEBUG] 2023-11-08 14:51:15,871 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-1]Changing status from UNKNOWN to INITIALIZING
[DEBUG] 2023-11-08 14:51:15,871 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-1]writeStatus generation 1 status StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,872 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter start - [PartitionWriter _confluent-metadata-auth-2]Starting generation 1 for partition writer _confluent-metadata-auth-2
[DEBUG] 2023-11-08 14:51:15,872 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-2]Changing status from UNKNOWN to INITIALIZING
[DEBUG] 2023-11-08 14:51:15,872 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-2]writeStatus generation 1 status StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,872 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter start - [PartitionWriter _confluent-metadata-auth-3]Starting generation 1 for partition writer _confluent-metadata-auth-3
[DEBUG] 2023-11-08 14:51:15,872 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-3]Changing status from UNKNOWN to INITIALIZING
[DEBUG] 2023-11-08 14:51:15,872 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-3]writeStatus generation 1 status StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,873 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter start - [PartitionWriter _confluent-metadata-auth-4]Starting generation 1 for partition writer _confluent-metadata-auth-4
[DEBUG] 2023-11-08 14:51:15,873 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-4]Changing status from UNKNOWN to INITIALIZING
[DEBUG] 2023-11-08 14:51:15,873 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-4]writeStatus generation 1 status StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,873 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter start - [PartitionWriter _confluent-metadata-auth-5]Starting generation 1 for partition writer _confluent-metadata-auth-5
[DEBUG] 2023-11-08 14:51:15,873 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-5]Changing status from UNKNOWN to INITIALIZING
[DEBUG] 2023-11-08 14:51:15,873 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-5]writeStatus generation 1 status StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[INFO] 2023-11-08 14:51:15,880 [auth-writer-mgmt-1] io.confluent.security.auth.provider.ldap.LdapGroupManager <init> - LDAP group manager created with config: LdapAuthorizerConfig: 
	ldap.sasl.login.read.timeout.ms=null%n	ldap.group.search.filter=%n	ldap.group.name.attribute=cn%n	ldap.group.name.attribute.pattern=%n	ldap.user.name.attribute.pattern=%n	ldap.user.search.filter=%n	ldap.ssl.trustmanager.algorithm=PKIX%n	ldap.ssl.endpoint.identification.algorithm=https%n	ldap.ssl.keystore.key=null%n	ldap.group.member.attribute=member%n	ldap.group.search.base=dc=test,dc=com%n	ldap.user.object.class=organizationalRole%n	ldap.user.memberof.attribute=memberof%n	ldap.group.member.attribute.pattern=CN=(.*),DC=test,DC=com%n	ldap.sasl.oauthbearer.sub.claim.name=sub%n	ldap.sasl.oauthbearer.clock.skew.seconds=30%n	ldap.ssl.engine.factory.class=null%n	ldap.ssl.truststore.certificates=null%n	ldap.ssl.protocol=TLSv1.3%n	ldap.ssl.truststore.location=null%n	ldap.user.dn.name.pattern=%n	ldap.sasl.jaas.config=null%n	ldap.sasl.login.callback.handler.class=null%n	ldap.sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms=10000%n	ldap.ssl.secure.random.implementation=null%n	ldap.sasl.kerberos.kinit.cmd=/usr/bin/kinit%n	ldap.retry.backoff.max.ms=1000%n	ldap.refresh.interval.ms=60000%n	ldap.group.search.scope=1%n	ldap.ssl.provider=null%n	ldap.sasl.login.connect.timeout.ms=null%n	ldap.sasl.kerberos.ticket.renew.window.factor=0.8%n	ldap.ssl.truststore.type=JKS%n	ldap.ssl.key.password=null%n	ldap.sasl.oauthbearer.jwks.endpoint.url=null%n	ldap.sasl.oauthbearer.jwks.endpoint.retry.backoff.ms=100%n	ldap.group.dn.name.pattern=%n	ldap.ssl.truststore.password=null%n	ldap.user.password.attribute=null%n	ldap.ssl.cipher.suites=null%n	ldap.retry.backoff.ms=100%n	ldap.group.object.class=group%n	ldap.sasl.login.class=null%n	ldap.ssl.keymanager.algorithm=SunX509%n	ldap.sasl.kerberos.min.time.before.relogin=60000%n	ldap.sasl.login.retry.backoff.ms=100%n	ldap.search.page.size=0%n	ldap.ssl.enabled.protocols=[TLSv1.2, TLSv1.3]%n	ldap.user.memberof.attribute.pattern=CN=(.*),DC=test,DC=com%n	ldap.ssl.keystore.location=null%n	ldap.ssl.keystore.certificate.chain=null%n	ldap.sasl.kerberos.ticket.renew.jitter=0.05%n	ldap.search.mode=GROUPS%n	ldap.user.search.base=dc=test,dc=com%n	ldap.sasl.login.retry.backoff.max.ms=10000%n	ldap.sasl.oauthbearer.expected.issuer=null%n	ldap.sasl.oauthbearer.token.endpoint.url=null%n	ldap.sasl.kerberos.service.name=ldap%n	ldap.ssl.keystore.type=JKS%n	ldap.user.search.scope=1%n	ldap.ssl.keystore.password=null%n	ldap.retry.timeout.ms=86400000%n	ldap.sasl.oauthbearer.expected.audience=null%n	ldap.sasl.oauthbearer.scope.claim.name=scope%n	ldap.sasl.oauthbearer.jwks.endpoint.refresh.ms=3600000%n	ldap.user.name.attribute=cn
	com.sun.jndi.ldap.connect.timeout=30000%n	java.naming.security.principal=cn=mds,dc=test,dc=com%n	java.naming.factory.initial=com.sun.jndi.ldap.LdapCtxFactory%n	java.naming.provider.url=ldap://ldap.confluent.svc.cluster.local:389%n	com.sun.jndi.ldap.read.timeout=30000%n	java.naming.security.credentials=[hidden]%n	java.naming.security.authentication=simple
[TRACE] 2023-11-08 14:51:15,880 [auth-writer-mgmt-1] io.confluent.security.auth.provider.ldap.LdapGroupManager start - Starting LDAP group manager
[DEBUG] 2023-11-08 14:51:15,941 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-2]Status record of generation 1 for partition _confluent-metadata-auth-2 written at offset 0
[DEBUG] 2023-11-08 14:51:15,943 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Status record of generation 1 for partition _confluent-metadata-auth-5 written at offset 0
[TRACE] 2023-11-08 14:51:15,944 [auth-writer-mgmt-1] io.confluent.security.auth.provider.ldap.LdapGroupManager search - Searching groups with base dc=test,dc=com filter (objectClass=group): 
[DEBUG] 2023-11-08 14:51:15,949 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader run - preparing latest auth record entries, size : 0
[DEBUG] 2023-11-08 14:51:15,951 [auth-writer-mgmt-1] io.confluent.security.auth.provider.ldap.LdapGroupManager searchAndProcessResults - Search completed, group cache is {}
[DEBUG] 2023-11-08 14:51:15,952 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,952 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:0 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:15,952 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-5]Received new generation id 1 for partition writer _confluent-metadata-auth-5 at offset 0 status INITIALIZING generation 1
[TRACE] 2023-11-08 14:51:15,953 [auth-writer-mgmt-1] io.confluent.security.auth.provider.ldap.LdapGroupManager schedulePeriodicSearch - Scheduling periodic search with initialDelayMs=60000, refreshIntervalMs 60000
[DEBUG] 2023-11-08 14:51:15,954 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-0]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,955 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-1]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,955 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-2]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,956 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-3]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,956 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-4]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,956 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,956 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:0 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:15,956 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-2]Received new generation id 1 for partition writer _confluent-metadata-auth-2 at offset 0 status INITIALIZING generation 1
[DEBUG] 2023-11-08 14:51:15,956 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-5]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,957 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-0]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,957 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-1]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,957 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-2]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,958 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-3]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,959 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-4]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,959 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-5]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,965 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-2]Status record of generation 1 for partition _confluent-metadata-auth-2 written at offset 1
[DEBUG] 2023-11-08 14:51:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:1 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-2]Received new generation id 1 for partition writer _confluent-metadata-auth-2 at offset 1 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-2]Changing status from INITIALIZING to INITIALIZED
[DEBUG] 2023-11-08 14:51:15,978 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-2]Status record of generation 1 for partition _confluent-metadata-auth-2 written at offset 2
[DEBUG] 2023-11-08 14:51:15,978 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,978 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Status record of generation 1 for partition _confluent-metadata-auth-5 written at offset 1
[DEBUG] 2023-11-08 14:51:15,978 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:1 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,978 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Status record of generation 1 for partition _confluent-metadata-auth-5 written at offset 2
[DEBUG] 2023-11-08 14:51:15,978 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-5]Received new generation id 1 for partition writer _confluent-metadata-auth-5 at offset 1 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:15,978 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-5]Changing status from INITIALIZING to INITIALIZED
[DEBUG] 2023-11-08 14:51:15,979 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,979 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:2 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,979 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-5]Received new generation id 1 for partition writer _confluent-metadata-auth-5 at offset 2 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:15,980 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,980 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:2 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:15,980 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-2]Received new generation id 1 for partition writer _confluent-metadata-auth-2 at offset 2 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:16,081 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,082 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:0 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:16,083 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-1]Received new generation id 1 for partition writer _confluent-metadata-auth-1 at offset 0 status INITIALIZING generation 1
[DEBUG] 2023-11-08 14:51:16,085 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,085 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:0 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:16,085 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-4]Received new generation id 1 for partition writer _confluent-metadata-auth-4 at offset 0 status INITIALIZING generation 1
[DEBUG] 2023-11-08 14:51:16,085 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Status record of generation 1 for partition _confluent-metadata-auth-1 written at offset 0
[DEBUG] 2023-11-08 14:51:16,086 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-4]Status record of generation 1 for partition _confluent-metadata-auth-4 written at offset 0
[DEBUG] 2023-11-08 14:51:16,094 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,094 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:1 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,094 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-1]Received new generation id 1 for partition writer _confluent-metadata-auth-1 at offset 1 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:16,094 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-1]Changing status from INITIALIZING to INITIALIZED
[DEBUG] 2023-11-08 14:51:16,094 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,094 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:2 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,094 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-1]Received new generation id 1 for partition writer _confluent-metadata-auth-1 at offset 2 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:16,095 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Status record of generation 1 for partition _confluent-metadata-auth-1 written at offset 1
[DEBUG] 2023-11-08 14:51:16,095 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Status record of generation 1 for partition _confluent-metadata-auth-1 written at offset 2
[DEBUG] 2023-11-08 14:51:16,095 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-4]Status record of generation 1 for partition _confluent-metadata-auth-4 written at offset 1
[DEBUG] 2023-11-08 14:51:16,095 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-4]Status record of generation 1 for partition _confluent-metadata-auth-4 written at offset 2
[DEBUG] 2023-11-08 14:51:16,096 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,096 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:1 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,096 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-4]Received new generation id 1 for partition writer _confluent-metadata-auth-4 at offset 1 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:16,096 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-4]Changing status from INITIALIZING to INITIALIZED
[DEBUG] 2023-11-08 14:51:16,096 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,096 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:2 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,096 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-4]Received new generation id 1 for partition writer _confluent-metadata-auth-4 at offset 2 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:16,167 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,167 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:0 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:16,167 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-3]Received new generation id 1 for partition writer _confluent-metadata-auth-3 at offset 0 status INITIALIZING generation 1
[DEBUG] 2023-11-08 14:51:16,169 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,169 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:0 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null) oldValue null
[DEBUG] 2023-11-08 14:51:16,169 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-0]Received new generation id 1 for partition writer _confluent-metadata-auth-0 at offset 0 status INITIALIZING generation 1
[DEBUG] 2023-11-08 14:51:16,170 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-0]Status record of generation 1 for partition _confluent-metadata-auth-0 written at offset 0
[DEBUG] 2023-11-08 14:51:16,170 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-3]Status record of generation 1 for partition _confluent-metadata-auth-3 written at offset 0
[DEBUG] 2023-11-08 14:51:16,181 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-0]Status record of generation 1 for partition _confluent-metadata-auth-0 written at offset 1
[DEBUG] 2023-11-08 14:51:16,181 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-0]Status record of generation 1 for partition _confluent-metadata-auth-0 written at offset 2
[DEBUG] 2023-11-08 14:51:16,181 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-3]Status record of generation 1 for partition _confluent-metadata-auth-3 written at offset 1
[DEBUG] 2023-11-08 14:51:16,181 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-3]Status record of generation 1 for partition _confluent-metadata-auth-3 written at offset 2
[DEBUG] 2023-11-08 14:51:16,182 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,183 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:1 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,183 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-3]Received new generation id 1 for partition writer _confluent-metadata-auth-3 at offset 1 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:16,183 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-3]Changing status from INITIALIZING to INITIALIZED
[DEBUG] 2023-11-08 14:51:16,183 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,183 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:2 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,183 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-3]Received new generation id 1 for partition writer _confluent-metadata-auth-3 at offset 2 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:16,185 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,185 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:1 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZING, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,185 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-0]Received new generation id 1 for partition writer _confluent-metadata-auth-0 at offset 1 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:51:16,185 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter status - [PartitionWriter _confluent-metadata-auth-0]Changing status from INITIALIZING to INITIALIZED
[INFO] 2023-11-08 14:51:16,185 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader lambda$null$0 - Kafka Auth reader initialized on all partitions
[INFO] 2023-11-08 14:51:16,186 [auth-reader-1] org.apache.kafka.server.http.MetadataServerConfig logAll - MetadataServerConfig values: 
	confluent.http.server.listeners = [http://0.0.0.0:8090]
	confluent.metadata.server.advertised.listeners = [https://kafka-0.kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.server.enable = false
	confluent.metadata.server.listeners = [https://0.0.0.0:8090]

[INFO] 2023-11-08 14:51:16,187 [auth-reader-1] io.confluent.security.store.kafka.KafkaStoreConfig logAll - KafkaStoreConfig values: 
	confluent.metadata.refresh.timeout.ms = 60000
	confluent.metadata.retry.timeout.ms = 86400000
	confluent.metadata.topic.create.timeout.ms = 600000
	confluent.metadata.topic.replication.factor = 3

[INFO] 2023-11-08 14:51:16,188 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:16,191 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,191 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,191 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'auto.create.topics.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,191 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.recovery.threads.per.data.dir' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.server.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.mechanism.inter.broker.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'default.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.session.uuid' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.operator.managed' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'transaction.state.log.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,192 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.io.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'delete.topic.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.clientCnxnSocket' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.client.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.login.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.commit.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'authorizer.class.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'transaction.state.log.min.isr' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.request.max.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,193 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.retention.minutes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'inter.broker.protocol.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.security.protocol.map' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.retention.hours' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.partitions' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'inter.broker.listener.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.balancer.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'node.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,194 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.message.format.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.segment.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'group.max.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,195 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.dirs' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replica.lag.time.max.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.network.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.send.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.receive.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.rack' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.topic.compression.codec' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.retention.check.interval.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,196 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,197 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:16,197 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:16,197 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:16,197 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455076197
[INFO] 2023-11-08 14:51:16,198 [auth-reader-1] io.confluent.security.store.kafka.KafkaStoreConfig logAll - KafkaStoreConfig values: 
	confluent.metadata.refresh.timeout.ms = 60000
	confluent.metadata.retry.timeout.ms = 86400000
	confluent.metadata.topic.create.timeout.ms = 600000
	confluent.metadata.topic.replication.factor = 3

[INFO] 2023-11-08 14:51:16,199 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-metadata-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'auto.create.topics.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.recovery.threads.per.data.dir' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,202 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.server.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.mechanism.inter.broker.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'default.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.session.uuid' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.operator.managed' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'transaction.state.log.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.io.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'delete.topic.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.clientCnxnSocket' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.client.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.oauthbearer.sasl.login.callback.handler.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.commit.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.connect' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,203 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'authorizer.class.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'transaction.state.log.min.isr' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.request.max.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.retention.minutes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'inter.broker.protocol.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.security.protocol.map' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.retention.hours' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.partitions' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,204 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'inter.broker.listener.name' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.balancer.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'node.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.external.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,205 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.message.format.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.plain.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.segment.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.token.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'group.max.session.timeout.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.dirs' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replica.lag.time.max.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,206 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.internal.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'num.network.threads' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.send.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'listener.name.replication.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'socket.receive.buffer.bytes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.rack' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'zookeeper.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'broker.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'offsets.topic.compression.codec' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'log.retention.check.interval.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455076207
[WARN] 2023-11-08 14:51:16,207 [auth-reader-1] org.apache.kafka.common.utils.AppInfoParser registerAppInfo - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.admin.client:type=app-info,id=_confluent-metadata-admin-0
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.admin.KafkaAdminClient.<init>(KafkaAdminClient.java:655)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:592)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:143)
	at io.confluent.security.auth.provider.ConfluentProvider.createMdsAdminClient(ConfluentProvider.java:336)
	at io.confluent.security.auth.provider.ConfluentProvider.lambda$start$0(ConfluentProvider.java:242)
	at java.base/java.util.concurrent.CompletableFuture$UniApply.tryFire(CompletableFuture.java:642)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at io.confluent.security.store.kafka.clients.KafkaReader.lambda$null$0(KafkaReader.java:96)
	at java.base/java.util.concurrent.CompletableFuture.uniWhenComplete(CompletableFuture.java:859)
	at java.base/java.util.concurrent.CompletableFuture$UniWhenComplete.tryFire(CompletableFuture.java:837)
	at java.base/java.util.concurrent.CompletableFuture.postComplete(CompletableFuture.java:506)
	at java.base/java.util.concurrent.CompletableFuture.complete(CompletableFuture.java:2073)
	at io.confluent.security.store.kafka.clients.KafkaReader$PartitionState.onConsume(KafkaReader.java:310)
	at io.confluent.security.store.kafka.clients.KafkaReader.processConsumerRecord(KafkaReader.java:283)
	at java.base/java.lang.Iterable.forEach(Iterable.java:75)
	at io.confluent.security.store.kafka.clients.KafkaReader.run(KafkaReader.java:217)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:16,210 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,210 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:2 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:51:16,210 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-0]Received new generation id 1 for partition writer _confluent-metadata-auth-0 at offset 2 status INITIALIZED generation 1
[INFO] 2023-11-08 14:51:16,213 [audit-init-1] io.confluent.security.audit.AuditLogConfig logAll - AuditLogConfig values: 
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.cloudevent.codec = structured
	confluent.security.event.logger.enable = true
	confluent.security.event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter
	confluent.security.event.logger.exporter.kafka.topic.create = true
	confluent.security.event.logger.exporter.kafka.topic.partitions = 12
	confluent.security.event.logger.exporter.kafka.topic.replicas = 3
	confluent.security.event.logger.exporter.kafka.topic.retention.bytes = -1
	confluent.security.event.logger.exporter.kafka.topic.retention.ms = 2592000000
	confluent.security.event.logger.exporter.kafka.topic.roll.ms = 14400000
	confluent.security.event.router.cache.entries = 10000
	confluent.security.event.router.config = 

[INFO] 2023-11-08 14:51:16,217 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(INTERNAL)
[INFO] 2023-11-08 14:51:16,220 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(EXTERNAL)
[INFO] 2023-11-08 14:51:16,222 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started data-plane acceptor and processor(s) for endpoint : ListenerName(TOKEN)
[INFO] 2023-11-08 14:51:16,223 [main] kafka.network.SocketServer info - [SocketServer listenerType=ZK_BROKER, nodeId=0] Started socket server acceptors and processors
[INFO] 2023-11-08 14:51:16,236 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:16,242 [audit-init-1] io.confluent.telemetry.events.EventLoggerConfig logAll - EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter

[INFO] 2023-11-08 14:51:16,243 [main] io.confluent.http.server.KafkaHttpServerConfig logAll - KafkaHttpServerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:16,322 [audit-init-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporterConfig logAll - NonBlockingKafkaExporterConfig values: 
	event.logger.exporter.kafka.blocking = false
	event.logger.exporter.kafka.bootstrap.servers = kafka-0.kafka.confluent.svc.cluster.local:9072
	event.logger.exporter.kafka.request.timeout.ms = 12000
	event.logger.exporter.kafka.topic.config = {"topics":[{"name":"confluent-audit-log-events","partitions":0,"replicationFactor":0,"config":{"retention.ms":"7776000000"}}]}
	event.logger.exporter.kafka.topic.create = true
	event.logger.exporter.kafka.topic.partitions = 12
	event.logger.exporter.kafka.topic.replicas = 3
	event.logger.exporter.kafka.topic.retention.bytes = -1
	event.logger.exporter.kafka.topic.retention.ms = 2592000000
	event.logger.exporter.kafka.topic.roll.ms = 14400000

[INFO] 2023-11-08 14:51:16,323 [audit-init-1] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-event-logger
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 0
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[INFO] 2023-11-08 14:51:16,326 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:16,327 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:16,327 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455076326
[INFO] 2023-11-08 14:51:16,327 [audit-init-1] io.confluent.telemetry.events.EventLoggerConfig logAll - EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter

[INFO] 2023-11-08 14:51:16,332 [audit-init-1] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:51:16,335 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:16,335 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:16,335 [audit-init-1] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455076335
[INFO] 2023-11-08 14:51:16,339 [audit-init-1] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-event-logger
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 0
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 10
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[INFO] 2023-11-08 14:51:16,347 [kafka-producer-network-thread | confluent-event-logger] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-event-logger] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:16,349 [main] io.confluent.rest.RestConfig logAll - RestConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = []
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8080
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = 
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = false
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = 
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:16,360 [main] org.eclipse.jetty.util.log initialized - Logging initialized @24486ms to org.eclipse.jetty.util.log.Slf4jLog
[INFO] 2023-11-08 14:51:16,363 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 9 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:51:16,378 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(confluent-audit-log-events-2, confluent-audit-log-events-5, confluent-audit-log-events-11, confluent-audit-log-events-8)
[INFO] 2023-11-08 14:51:16,384 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 9 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:51:16,389 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,391 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-2 in /mnt/data/data0/logs/confluent-audit-log-events-2 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,393 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-2 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-2
[INFO] 2023-11-08 14:51:16,393 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-2 broker=0] Log loaded for partition confluent-audit-log-events-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,393 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader confluent-audit-log-events-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,397 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,398 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-5 in /mnt/data/data0/logs/confluent-audit-log-events-5 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,398 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-5 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-5
[INFO] 2023-11-08 14:51:16,398 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-5 broker=0] Log loaded for partition confluent-audit-log-events-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,398 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader confluent-audit-log-events-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,400 [main] io.confluent.http.server.KafkaHttpApplicationLoader load - Application provider 'MetadataApiApplicationProvider' provided 1 instance(s).
[INFO] 2023-11-08 14:51:16,403 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,403 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-11 in /mnt/data/data0/logs/confluent-audit-log-events-11 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,403 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-11 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-11
[INFO] 2023-11-08 14:51:16,403 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-11 broker=0] Log loaded for partition confluent-audit-log-events-11 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,404 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader confluent-audit-log-events-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,408 [audit-init-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - all topics exist
[INFO] 2023-11-08 14:51:16,409 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,409 [audit-init-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - Event logger is waiting for metadata for topics: [confluent-audit-log-events]
[INFO] 2023-11-08 14:51:16,410 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-8 in /mnt/data/data0/logs/confluent-audit-log-events-8 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,410 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-8 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-8
[INFO] 2023-11-08 14:51:16,410 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-8 broker=0] Log loaded for partition confluent-audit-log-events-8 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,410 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader confluent-audit-log-events-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,414 [main] io.confluent.rbacapi.app.RbacApiAppConfig logAll - RbacApiAppConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	api.flavor = CP
	authentication.method = BEARER
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	cluster.registry.clusters = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	openapi.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	user.store = LDAP
	user.store.file.path = 
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:16,415 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,416 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-3 in /mnt/data/data0/logs/confluent-audit-log-events-3 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,416 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-3 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-3
[INFO] 2023-11-08 14:51:16,416 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-3 broker=0] Log loaded for partition confluent-audit-log-events-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,416 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower confluent-audit-log-events-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,422 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,422 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-4 in /mnt/data/data0/logs/confluent-audit-log-events-4 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,422 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-4 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-4
[INFO] 2023-11-08 14:51:16,422 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-4 broker=0] Log loaded for partition confluent-audit-log-events-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,423 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower confluent-audit-log-events-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[WARN] 2023-11-08 14:51:16,425 [kafka-producer-network-thread | confluent-event-logger] org.apache.kafka.clients.NetworkClient handleSuccessfulResponse - [Producer clientId=confluent-event-logger] Error while fetching metadata with correlation id 3 : {confluent-audit-log-events=UNKNOWN_TOPIC_OR_PARTITION}
[INFO] 2023-11-08 14:51:16,427 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,428 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-0 in /mnt/data/data0/logs/confluent-audit-log-events-0 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,428 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-0 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-0
[INFO] 2023-11-08 14:51:16,428 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-0 broker=0] Log loaded for partition confluent-audit-log-events-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,428 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower confluent-audit-log-events-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,434 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,435 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-1 in /mnt/data/data0/logs/confluent-audit-log-events-1 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,435 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-1 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-1
[INFO] 2023-11-08 14:51:16,435 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-1 broker=0] Log loaded for partition confluent-audit-log-events-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,436 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower confluent-audit-log-events-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,440 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,441 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-10 in /mnt/data/data0/logs/confluent-audit-log-events-10 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,441 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-10 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-10
[INFO] 2023-11-08 14:51:16,441 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-10 broker=0] Log loaded for partition confluent-audit-log-events-10 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,442 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower confluent-audit-log-events-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,446 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,447 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-6 in /mnt/data/data0/logs/confluent-audit-log-events-6 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,447 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-6 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-6
[INFO] 2023-11-08 14:51:16,447 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-6 broker=0] Log loaded for partition confluent-audit-log-events-6 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,447 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower confluent-audit-log-events-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,451 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,452 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-7 in /mnt/data/data0/logs/confluent-audit-log-events-7 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,452 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-7 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-7
[INFO] 2023-11-08 14:51:16,453 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-7 broker=0] Log loaded for partition confluent-audit-log-events-7 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,453 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower confluent-audit-log-events-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,457 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent-audit-log-events-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:16,458 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent-audit-log-events-9 in /mnt/data/data0/logs/confluent-audit-log-events-9 with properties {message.timestamp.type="CreateTime", retention.bytes=-1, retention.ms=7776000000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:16,458 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-9 broker=0] No checkpointed highwatermark is found for partition confluent-audit-log-events-9
[INFO] 2023-11-08 14:51:16,458 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent-audit-log-events-9 broker=0] Log loaded for partition confluent-audit-log-events-9 with initial high watermark 0
[INFO] 2023-11-08 14:51:16,458 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower confluent-audit-log-events-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:16,458 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(confluent-audit-log-events-0, confluent-audit-log-events-4, confluent-audit-log-events-3, confluent-audit-log-events-1, confluent-audit-log-events-7, confluent-audit-log-events-6, confluent-audit-log-events-10, confluent-audit-log-events-9)
[INFO] 2023-11-08 14:51:16,458 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 9 for 8 partitions
[INFO] 2023-11-08 14:51:16,459 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(confluent-audit-log-events-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:16,459 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(confluent-audit-log-events-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent-audit-log-events-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:16,460 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 97ms correlationId 9 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:51:16,462 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 10
[INFO] 2023-11-08 14:51:16,514 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent-audit-log-events-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,514 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,514 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent-audit-log-events-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,514 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,515 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent-audit-log-events-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,515 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,515 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent-audit-log-events-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,515 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,549 [main] org.hibernate.validator.internal.util.Version <clinit> - HV000001: Hibernate Validator 6.1.7.Final
[INFO] 2023-11-08 14:51:16,679 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent-audit-log-events-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,679 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,680 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent-audit-log-events-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,680 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,680 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent-audit-log-events-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,680 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:16,680 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent-audit-log-events-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:16,680 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent-audit-log-events-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:17,040 [main] io.confluent.auditlogapi.kafka.DestinationTopicManagerConfig logAll - DestinationTopicManagerConfig values: 
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 

[INFO] 2023-11-08 14:51:17,134 [main] io.confluent.http.server.KafkaHttpApplicationLoader load - Application provider 'RbacApplicationProvider' provided 1 instance(s).
[INFO] 2023-11-08 14:51:17,138 [main] io.confluent.http.server.KafkaHttpServerConfig logAll - KafkaHttpServerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:17,149 [main] io.confluent.http.server.KafkaHttpServerConfig logAll - KafkaHttpServerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:17,151 [main] kafka.server.KafkaConfig logAll - KafkaConfig values: 
	advertised.listeners = EXTERNAL://rb0.my.domain:9092,INTERNAL://kafka-0.kafka.confluent.svc.cluster.local:9071,REPLICATION://kafka-0.kafka.confluent.svc.cluster.local:9072,TOKEN://kafka-0.kafka.confluent.svc.cluster.local:9073
	alter.config.policy.class.name = null
	alter.log.dirs.replication.quota.window.num = 11
	alter.log.dirs.replication.quota.window.size.seconds = 1
	authorizer.class.name = io.confluent.kafka.security.authorizer.ConfluentServerAuthorizer
	auto.create.topics.enable = false
	auto.leader.rebalance.enable = true
	background.threads = 10
	broker.heartbeat.interval.ms = 2000
	broker.id = 0
	broker.id.generation.enable = true
	broker.interceptor.class = class org.apache.kafka.server.interceptor.DefaultBrokerInterceptor
	broker.rack = 0
	broker.session.timeout.ms = 9000
	broker.session.uuid = _xmBsq6vTSmXhoYfo9SXDw
	client.quota.callback.class = null
	compression.type = producer
	confluent.ansible.managed = false
	confluent.append.record.interceptor.classes = []
	confluent.apply.create.topic.policy.to.create.partitions = false
	confluent.authorizer.authority.name = 
	confluent.backpressure.disk.enable = false
	confluent.backpressure.disk.free.threshold.bytes = 21474836480
	confluent.backpressure.disk.produce.bytes.per.second = 131072
	confluent.backpressure.disk.threshold.recovery.factor = 1.5
	confluent.backpressure.request.min.broker.limit = 200
	confluent.backpressure.request.queue.size.percentile = p95
	confluent.backpressure.types = null
	confluent.balancer.api.state.persistence.store.startup.max.retry.ms = 300000
	confluent.balancer.api.state.persistence.store.startup.retry.wait.ms = 100
	confluent.balancer.api.state.topic = _confluent_balancer_api_state
	confluent.balancer.class = io.confluent.databalancer.KafkaDataBalanceManager
	confluent.balancer.disk.max.load = 0.85
	confluent.balancer.enable = true
	confluent.balancer.exclude.topic.names = []
	confluent.balancer.exclude.topic.prefixes = []
	confluent.balancer.heal.broker.failure.threshold.ms = 3600000
	confluent.balancer.heal.uneven.load.trigger = EMPTY_BROKER
	confluent.balancer.max.replicas = 2147483647
	confluent.balancer.network.in.max.bytes.per.second = 9223372036854775807
	confluent.balancer.network.out.max.bytes.per.second = 9223372036854775807
	confluent.balancer.task.history.retention.days = 30
	confluent.balancer.throttle.bytes.per.second = 10485760
	confluent.balancer.topic.replication.factor = 3
	confluent.basic.auth.credentials.source = null
	confluent.basic.auth.user.info = null
	confluent.bearer.auth.credentials.source = null
	confluent.bearer.auth.token = null
	confluent.broker.health.manager.enabled = true
	confluent.broker.health.manager.engine.request.handler.threads.stuck.criteria = AllThreadsStuck
	confluent.broker.health.manager.hard.kill.duration.ms = 60000
	confluent.broker.health.manager.mitigation.enabled = false
	confluent.broker.health.manager.num.samples.before.broker.unhealthy = 120
	confluent.broker.health.manager.sample.duration.ms = 1000
	confluent.broker.health.manager.storage.background.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.network.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.health.manager.storage.request.handler.threads.stuck.criteria = AnyThreadStuck
	confluent.broker.limit.consumer.bytes.per.second = 9223372036854775807
	confluent.broker.limit.producer.bytes.per.second = 9223372036854775807
	confluent.broker.load.average.service.request.time.ms = 0.1
	confluent.broker.load.delay.metric.start.ms = 180000
	confluent.broker.load.enabled = false
	confluent.broker.load.num.samples = 60
	confluent.broker.load.update.metric.tags.interval.ms = 60000
	confluent.broker.load.window.size.ms = 60000
	confluent.broker.load.workload.coefficient = 20.0
	confluent.broker.registration.delay.ms = 0
	confluent.cdc.api.keys.load.timeout.ms = 600000000000
	confluent.cdc.api.keys.topic = 
	confluent.cdc.lkc.metadata.topic = 
	confluent.checksum.enabled.files = [none]
	confluent.clm.enabled = false
	confluent.clm.frequency.in.hours = 6
	confluent.close.connections.on.credential.delete = false
	confluent.cluster.link.enable = true
	confluent.cluster.link.fetch.response.min.bytes = 1
	confluent.cluster.link.fetch.response.total.bytes = 2147483647
	confluent.cluster.link.io.max.bytes.per.second = 9223372036854775807
	confluent.cluster.link.metadata.topic.create.retry.delay.ms = 1000
	confluent.cluster.link.metadata.topic.enable = false
	confluent.cluster.link.metadata.topic.min.isr = 2
	confluent.cluster.link.metadata.topic.partitions = 50
	confluent.cluster.link.metadata.topic.replication.factor = 3
	confluent.cluster.link.replication.quota.mode = CLUSTER_LINK_ONLY
	confluent.cluster.link.replication.quota.window.num = 11
	confluent.cluster.link.replication.quota.window.size.seconds = 2
	confluent.connection.invalid.request.delay.enable = false
	confluent.consumer.lag.emitter.enabled = false
	confluent.consumer.lag.emitter.interval.ms = 60000
	confluent.defer.isr.shrink.enable = false
	confluent.durability.audit.batch.flush.frequency.ms = 900000
	confluent.durability.audit.enable = false
	confluent.durability.audit.initial.job.delay.ms = 900000
	confluent.durability.audit.reporting.batch.ms = 1800000
	confluent.durability.events.allowed = OffsetChangeType,EpochChangeType,IsrExpandType,DeleteRecordsType,RetentionChangeType,StartOffsetChangeType,DeletePartitionType,HealthCheckType
	confluent.durability.topic.partition.count = 50
	confluent.durability.topic.replication.factor = 3
	confluent.eligible.controllers = []
	confluent.enable.stray.partition.deletion = false
	confluent.encryption.key.manager.rotation.interval.ms = 31536000000
	confluent.fetch.partition.pruning.enable = true
	confluent.http.server.start.timeout.ms = 60000
	confluent.http.server.stop.timeout.ms = 30000
	confluent.internal.metrics.enable = false
	confluent.internal.rest.server.bind.port = null
	confluent.log.placement.constraints = 
	confluent.max.connection.creation.rate.per.ip = 2147483647
	confluent.max.connection.throttle.ms = null
	confluent.metadata.server.cluster.registry.clusters = []
	confluent.missing.id.cache.ttl.sec = 60
	confluent.missing.id.query.range = 200
	confluent.missing.schema.cache.ttl.sec = 60
	confluent.multitenant.interceptor.balancer.apis.enabled = false
	confluent.multitenant.listener.hostname.cluster.prefix.enable = false
	confluent.multitenant.listener.names = null
	confluent.multitenant.max.partitions.per.request = 2147483647
	confluent.multitenant.parse.sni.host.name.enable = false
	confluent.offsets.topic.placement.constraints = 
	confluent.operator.managed = true
	confluent.password.encoder.old.secret.ttl.ms = 9223372036854775807
	confluent.plugins.cluster.link.policy.max.destination.links.per.tenant = 5
	confluent.plugins.cluster.link.policy.max.source.links.per.tenant = 5
	confluent.plugins.topic.policy.max.partitions.per.tenant = 512
	confluent.prefer.tier.fetch.ms = -1
	confluent.proxy.protocol.fallback.enabled = false
	confluent.proxy.protocol.version = NONE
	confluent.quota.dynamic.reporting.enable = false
	confluent.quota.dynamic.reporting.interval.ms = 30000
	confluent.quota.tenant.broker.max.consumer.rate = 13107200
	confluent.quota.tenant.broker.max.producer.rate = 13107200
	confluent.quota.tenant.default.controller.mutation.rate = 2.147483647E9
	confluent.quota.tenant.fetch.multiplier = 1.0
	confluent.quota.tenant.follower.broker.min.consumer.rate = 10485760
	confluent.quota.tenant.follower.broker.min.producer.rate = 10485760
	confluent.quota.tenant.produce.multiplier = 1.0
	confluent.quota.tenant.user.quotas.enable = false
	confluent.replica.fetch.connections.mode = combined
	confluent.reporters.telemetry.auto.enable = true
	confluent.request.log.filter.class = class org.apache.kafka.common.requests.SamplingRequestLogFilter
	confluent.require.compatible.keystore.updates = true
	confluent.schema.registry.max.cache.size = 10000
	confluent.schema.registry.max.retries = 1
	confluent.schema.registry.retries.wait.ms = 0
	confluent.schema.registry.url = null
	confluent.schema.validator.interceptor.class = io.confluent.kafka.schemaregistry.validator.RecordSchemaValidator
	confluent.schema.validator.multitenant.enable = false
	confluent.schema.validator.samples.per.min = 0
	confluent.security.event.logger.authentication.enable = false
	confluent.security.event.logger.enable = true
	confluent.security.event.router.config = 
	confluent.segment.speculative.prefetch.enable = false
	confluent.ssl.key.password = null
	confluent.ssl.keystore.location = null
	confluent.ssl.keystore.password = null
	confluent.ssl.keystore.type = null
	confluent.ssl.protocol = null
	confluent.ssl.truststore.location = null
	confluent.ssl.truststore.password = null
	confluent.ssl.truststore.type = null
	confluent.storage.probe.period.ms = -1
	confluent.telemetry.enabled = false
	confluent.tier.archiver.num.threads = 2
	confluent.tier.azure.block.blob.auto.abort.threshold.bytes = 500000
	confluent.tier.azure.block.blob.container = null
	confluent.tier.azure.block.blob.cred.file.path = null
	confluent.tier.azure.block.blob.endpoint = null
	confluent.tier.azure.block.blob.prefix = 
	confluent.tier.backend = 
	confluent.tier.cleaner.compact.min.efficiency = 0.5
	confluent.tier.cleaner.compact.segment.min.bytes = 20971520
	confluent.tier.cleaner.dedupe.buffer.size = 134217728
	confluent.tier.cleaner.feature.enable = false
	confluent.tier.cleaner.io.buffer.load.factor = 0.9
	confluent.tier.cleaner.io.buffer.size = 10485760
	confluent.tier.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	confluent.tier.cleaner.min.cleanable.ratio = 0.75
	confluent.tier.cleaner.num.threads = 2
	confluent.tier.enable = false
	confluent.tier.feature = false
	confluent.tier.fenced.segment.delete.delay.ms = 600000
	confluent.tier.fetcher.memorypool.bytes = 0
	confluent.tier.fetcher.num.threads = 4
	confluent.tier.fetcher.offset.cache.expiration.ms = 1800000
	confluent.tier.fetcher.offset.cache.period.ms = 60000
	confluent.tier.fetcher.offset.cache.size = 200000
	confluent.tier.gcs.bucket = null
	confluent.tier.gcs.cred.file.path = null
	confluent.tier.gcs.prefix = 
	confluent.tier.gcs.region = null
	confluent.tier.gcs.sse.customer.encryption.key = null
	confluent.tier.gcs.write.chunk.size = 0
	confluent.tier.local.hotset.bytes = -1
	confluent.tier.local.hotset.ms = 86400000
	confluent.tier.max.partition.fetch.bytes.override = 0
	confluent.tier.metadata.bootstrap.servers = null
	confluent.tier.metadata.max.poll.ms = 100
	confluent.tier.metadata.namespace = null
	confluent.tier.metadata.num.partitions = 50
	confluent.tier.metadata.replication.factor = 3
	confluent.tier.metadata.request.timeout.ms = 30000
	confluent.tier.object.fetcher.num.threads = 1
	confluent.tier.partition.state.commit.interval.ms = 15000
	confluent.tier.s3.assumerole.arn = null
	confluent.tier.s3.auto.abort.threshold.bytes = 500000
	confluent.tier.s3.aws.endpoint.override = null
	confluent.tier.s3.aws.signer.override = null
	confluent.tier.s3.bucket = null
	confluent.tier.s3.cred.file.path = null
	confluent.tier.s3.force.path.style.access = false
	confluent.tier.s3.prefix = 
	confluent.tier.s3.region = null
	confluent.tier.s3.sse.algorithm = AES256
	confluent.tier.s3.sse.customer.encryption.key = null
	confluent.tier.s3.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	confluent.tier.s3.ssl.key.password = null
	confluent.tier.s3.ssl.keystore.location = null
	confluent.tier.s3.ssl.keystore.password = null
	confluent.tier.s3.ssl.keystore.type = null
	confluent.tier.s3.ssl.protocol = TLSv1.3
	confluent.tier.s3.ssl.truststore.location = null
	confluent.tier.s3.ssl.truststore.password = null
	confluent.tier.s3.ssl.truststore.type = null
	confluent.tier.s3.user.agent.prefix = APN/1.0 Confluent/1.0 TieredStorageS3/1.0
	confluent.tier.segment.hotset.roll.min.bytes = 104857600
	confluent.tier.topic.delete.backoff.ms = 21600000
	confluent.tier.topic.delete.check.interval.ms = 300000
	confluent.tier.topic.delete.max.inprogress.partitions = 100
	confluent.topic.replica.assignor.builder.class = 
	confluent.transaction.state.log.placement.constraints = 
	confluent.verify.group.subscription.prefix = false
	connection.failed.authentication.delay.ms = 100
	connection.min.expire.interval.ms = 250
	connections.max.age.ms = 3153600000000
	connections.max.idle.ms = 600000
	connections.max.reauth.ms = 0
	control.plane.listener.name = null
	controlled.shutdown.enable = true
	controlled.shutdown.max.retries = 3
	controlled.shutdown.retry.backoff.ms = 5000
	controller.listener.names = null
	controller.quorum.append.linger.ms = 25
	controller.quorum.election.backoff.max.ms = 1000
	controller.quorum.election.timeout.ms = 1000
	controller.quorum.fetch.timeout.ms = 2000
	controller.quorum.request.timeout.ms = 2000
	controller.quorum.retry.backoff.ms = 20
	controller.quorum.voters = []
	controller.quota.window.num = 11
	controller.quota.window.size.seconds = 1
	controller.socket.timeout.ms = 30000
	create.topic.policy.class.name = null
	default.replication.factor = 3
	delegation.token.expiry.check.interval.ms = 3600000
	delegation.token.expiry.time.ms = 86400000
	delegation.token.master.key = null
	delegation.token.max.lifetime.ms = 604800000
	delegation.token.secret.key = null
	delete.records.purgatory.purge.interval.requests = 1
	delete.topic.enable = true
	enable.fips = false
	fetch.max.bytes = 57671680
	fetch.purgatory.purge.interval.requests = 1000
	floor.max.connection.creation.rate = null
	follower.replication.throttled.rate = 9223372036854775807
	follower.replication.throttled.replicas = none
	group.initial.rebalance.delay.ms = 3000
	group.max.session.timeout.ms = 1200000
	group.max.size = 2147483647
	group.min.session.timeout.ms = 6000
	initial.broker.registration.timeout.ms = 60000
	inter.broker.listener.name = REPLICATION
	inter.broker.protocol.version = 2.6
	kafka.metrics.polling.interval.secs = 10
	kafka.metrics.reporters = []
	leader.imbalance.check.interval.seconds = 300
	leader.imbalance.per.broker.percentage = 10
	leader.replication.throttled.rate = 9223372036854775807
	leader.replication.throttled.replicas = none
	listener.security.protocol.map = EXTERNAL:SASL_SSL,INTERNAL:SASL_SSL,REPLICATION:SASL_SSL,TOKEN:SASL_SSL
	listeners = EXTERNAL://:9092,INTERNAL://:9071,REPLICATION://:9072,TOKEN://:9073
	log.cleaner.backoff.ms = 15000
	log.cleaner.dedupe.buffer.size = 134217728
	log.cleaner.delete.retention.ms = 86400000
	log.cleaner.enable = true
	log.cleaner.io.buffer.load.factor = 0.9
	log.cleaner.io.buffer.size = 524288
	log.cleaner.io.max.bytes.per.second = 1.7976931348623157E308
	log.cleaner.max.compaction.lag.ms = 9223372036854775807
	log.cleaner.min.cleanable.ratio = 0.5
	log.cleaner.min.compaction.lag.ms = 0
	log.cleaner.threads = 1
	log.cleanup.policy = [delete]
	log.deletion.max.segments.per.run = 2147483647
	log.dir = /tmp/kafka-logs
	log.dirs = /mnt/data/data0/logs
	log.flush.interval.messages = 9223372036854775807
	log.flush.interval.ms = null
	log.flush.offset.checkpoint.interval.ms = 60000
	log.flush.scheduler.interval.ms = 9223372036854775807
	log.flush.start.offset.checkpoint.interval.ms = 60000
	log.index.interval.bytes = 4096
	log.index.size.max.bytes = 10485760
	log.message.downconversion.enable = true
	log.message.format.version = 2.6
	log.message.timestamp.difference.max.ms = 9223372036854775807
	log.message.timestamp.type = CreateTime
	log.preallocate = false
	log.retention.bytes = -1
	log.retention.check.interval.ms = 300000
	log.retention.hours = 168
	log.retention.minutes = null
	log.retention.ms = null
	log.roll.hours = 168
	log.roll.jitter.hours = 0
	log.roll.jitter.ms = null
	log.roll.ms = null
	log.segment.bytes = 1073741824
	log.segment.delete.delay.ms = 60000
	max.connection.creation.rate = 2147483647
	max.connection.creation.rate.per.ip.enable.threshold = 0.0
	max.connections = 2147483647
	max.connections.per.ip = 2147483647
	max.connections.per.ip.overrides = 
	max.incremental.fetch.session.cache.slots = 1000
	message.max.bytes = 1048588
	metadata.log.dir = null
	metadata.log.max.record.bytes.between.snapshots = 20971520
	metadata.log.segment.bytes = 1073741824
	metadata.log.segment.min.bytes = 8388608
	metadata.log.segment.ms = 604800000
	metadata.max.retention.bytes = -1
	metadata.max.retention.ms = 604800000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	min.insync.replicas = 2
	multitenant.metadata.class = null
	multitenant.metadata.dir = null
	multitenant.metadata.reload.delay.ms = 120000
	multitenant.metadata.ssl.certs.path = null
	multitenant.tenant.delete.batch.size = 10
	multitenant.tenant.delete.delay = 604800000
	node.id = 0
	num.io.threads = 8
	num.network.threads = 4
	num.partitions = 1
	num.recovery.threads.per.data.dir = 1
	num.replica.alter.log.dirs.threads = null
	num.replica.fetchers = 1
	offset.metadata.max.bytes = 4096
	offsets.commit.required.acks = -1
	offsets.commit.timeout.ms = 15000
	offsets.load.buffer.size = 5242880
	offsets.retention.check.interval.ms = 600000
	offsets.retention.minutes = 10080
	offsets.topic.compression.codec = 3
	offsets.topic.num.partitions = 50
	offsets.topic.replication.factor = 3
	offsets.topic.segment.bytes = 104857600
	password.encoder.cipher.algorithm = AES/CBC/PKCS5Padding
	password.encoder.iterations = 4096
	password.encoder.key.length = 128
	password.encoder.keyfactory.algorithm = null
	password.encoder.old.secret = null
	password.encoder.secret = null
	principal.builder.class = class org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder
	process.roles = []
	producer.purgatory.purge.interval.requests = 1000
	queued.max.request.bytes = -1
	queued.max.requests = 500
	quota.coordinator.enable = false
	quota.window.num = 11
	quota.window.size.seconds = 1
	quotas.topic.append.timeout.ms = 5000
	quotas.topic.compression.codec = 3
	quotas.topic.load.buffer.size = 5242880
	quotas.topic.num.partitions = 50
	quotas.topic.placement.constraints = 
	quotas.topic.replication.factor = 3
	quotas.topic.segment.bytes = 104857600
	remote.log.index.file.cache.total.size.bytes = 1073741824
	remote.log.manager.task.interval.ms = 30000
	remote.log.manager.task.retry.backoff.max.ms = 30000
	remote.log.manager.task.retry.backoff.ms = 500
	remote.log.manager.task.retry.jitter = 0.2
	remote.log.manager.thread.pool.size = 10
	remote.log.metadata.manager.class.name = null
	remote.log.metadata.manager.class.path = null
	remote.log.metadata.manager.impl.prefix = null
	remote.log.metadata.manager.listener.name = null
	remote.log.reader.max.pending.tasks = 100
	remote.log.reader.threads = 10
	remote.log.storage.manager.class.name = null
	remote.log.storage.manager.class.path = null
	remote.log.storage.manager.impl.prefix = null
	remote.log.storage.system.enable = false
	replica.fetch.backoff.ms = 1000
	replica.fetch.max.bytes = 1048576
	replica.fetch.min.bytes = 1
	replica.fetch.response.max.bytes = 10485760
	replica.fetch.wait.max.ms = 500
	replica.high.watermark.checkpoint.interval.ms = 5000
	replica.lag.time.max.ms = 45000
	replica.selector.class = null
	replica.socket.receive.buffer.bytes = 65536
	replica.socket.timeout.ms = 30000
	replication.quota.window.num = 11
	replication.quota.window.size.seconds = 1
	request.timeout.ms = 30000
	reserved.broker.max.id = 1000
	sasl.client.callback.handler.class = null
	sasl.enabled.mechanisms = [PLAIN]
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.principal.to.local.rules = [DEFAULT]
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism.controller.protocol = GSSAPI
	sasl.mechanism.inter.broker.protocol = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	sasl.server.callback.handler.class = null
	security.inter.broker.protocol = PLAINTEXT
	security.providers = null
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	socket.listen.backlog.size = 50
	socket.receive.buffer.bytes = 102400
	socket.request.max.bytes = 104857600
	socket.send.buffer.bytes = 102400
	ssl.cipher.suites = []
	ssl.client.auth = none
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.principal.mapping.rules = DEFAULT
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.abort.timed.out.transaction.cleanup.interval.ms = 10000
	transaction.max.timeout.ms = 900000
	transaction.remove.expired.transaction.cleanup.interval.ms = 3600000
	transaction.state.log.load.buffer.size = 5242880
	transaction.state.log.min.isr = 2
	transaction.state.log.num.partitions = 50
	transaction.state.log.replication.factor = 3
	transaction.state.log.segment.bytes = 104857600
	transactional.id.expiration.ms = 604800000
	unclean.leader.election.enable = false
	zookeeper.clientCnxnSocket = org.apache.zookeeper.ClientCnxnSocketNetty
	zookeeper.connect = zookeeper.confluent.svc.cluster.local:2182/kafka-confluent
	zookeeper.connection.timeout.ms = null
	zookeeper.max.in.flight.requests = 10
	zookeeper.session.timeout.ms = 22500
	zookeeper.set.acl = false
	zookeeper.ssl.cipher.suites = null
	zookeeper.ssl.client.enable = true
	zookeeper.ssl.crl.enable = false
	zookeeper.ssl.enabled.protocols = null
	zookeeper.ssl.endpoint.identification.algorithm = HTTPS
	zookeeper.ssl.keystore.location = null
	zookeeper.ssl.keystore.password = null
	zookeeper.ssl.keystore.type = null
	zookeeper.ssl.ocsp.enable = false
	zookeeper.ssl.protocol = TLSv1.2
	zookeeper.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	zookeeper.ssl.truststore.password = [hidden]
	zookeeper.ssl.truststore.type = null
	zookeeper.sync.time.ms = 2000

[INFO] 2023-11-08 14:51:17,163 [main] io.confluent.kafkarest.KafkaRestConfig logAll - KafkaRestConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	advertised.listeners = []
	api.endpoints.allowlist = []
	api.endpoints.blocklist = []
	api.v2.enable = false
	api.v3.enable = true
	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
	api.v3.produce.rate.limit.enabled = false
	api.v3.produce.rate.limit.grace.period.ms = 30000
	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
	api.v3.produce.rate.limit.max.requests.per.sec = 10000
	api.v3.produce.response.thread.pool.size = 16
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	bootstrap.servers = kafka.confluent.svc.cluster.local:9073
	client.init.timeout.ms = 60000
	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	client.sasl.kerberos.min.time.before.relogin = 60000
	client.sasl.kerberos.service.name = 
	client.sasl.kerberos.ticket.renew.jitter = 0.05
	client.sasl.kerberos.ticket.renew.window.factor = 0.8
	client.sasl.mechanism = OAUTHBEARER
	client.security.protocol = SASL_SSL
	client.ssl.cipher.suites = 
	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
	client.ssl.endpoint.identification.algorithm = 
	client.ssl.key.password = [hidden]
	client.ssl.keymanager.algorithm = SunX509
	client.ssl.keystore.location = 
	client.ssl.keystore.password = [hidden]
	client.ssl.keystore.type = JKS
	client.ssl.protocol = TLS
	client.ssl.provider = 
	client.ssl.trustmanager.algorithm = PKIX
	client.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	client.ssl.truststore.password = [hidden]
	client.ssl.truststore.type = JKS
	client.timeout.ms = 500
	client.zk.session.timeout.ms = 30000
	compression.enable = true
	confluent.resource.name.authority = 
	consumer.instance.timeout.ms = 300000
	consumer.iterator.backoff.ms = 50
	consumer.iterator.timeout.ms = 1
	consumer.request.max.bytes = 67108864
	consumer.request.timeout.ms = 1000
	consumer.threads = 50
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	fetch.min.bytes = -1
	host.name = 
	http2.enabled = true
	id = 
	idle.timeout.ms = 30000
	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension, io.confluent.kafkarest.security.KafkaRestSecurityResourceExtension]
	listener.protocol.map = []
	listeners = []
	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.jmx.prefix = kafka.rest
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8082
	producer.threads = 5
	proxy.protocol.enabled = false
	rate.limit.backend = guava
	rate.limit.costs = 
	rate.limit.default.cost = 1
	rate.limit.enable = false
	rate.limit.permits.per.sec = 50
	rate.limit.timeout.ms = 0
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
	rest.servlet.initializor.classes = [io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler]
	schema.registry.url = http://localhost:8081
	shutdown.graceful.ms = 1000
	simpleconsumer.pool.size.max = 25
	simpleconsumer.pool.timeout.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []
	zookeeper.connect = 

[INFO] 2023-11-08 14:51:17,165 [main] io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig logAll - EventEmitterConfig values: 

[INFO] 2023-11-08 14:51:17,166 [main] io.confluent.shaded.io.confluent.telemetry.events.EventEmitterConfig logAll - EventEmitterConfig values: 

[INFO] 2023-11-08 14:51:17,167 [main] io.confluent.telemetry.ConfluentTelemetryConfig logAll - ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

[INFO] 2023-11-08 14:51:17,168 [main] io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig logAll - VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

[INFO] 2023-11-08 14:51:17,168 [main] io.confluent.telemetry.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[WARN] 2023-11-08 14:51:17,168 [main] io.confluent.telemetry.ConfluentTelemetryConfig <init> - no telemetry exporters are enabled
[WARN] 2023-11-08 14:51:17,169 [main] io.confluent.shaded.io.confluent.telemetry.ResourceBuilderFacade withLabel - Ignoring redefinition of existing telemetry label kafka_rest.version
[INFO] 2023-11-08 14:51:17,170 [main] io.confluent.telemetry.ConfluentTelemetryConfig logAll - ConfluentTelemetryConfig values: 
	confluent.telemetry.api.key = null
	confluent.telemetry.api.secret = null
	confluent.telemetry.debug.enabled = false
	confluent.telemetry.enabled = false
	confluent.telemetry.events.collector.include = .*transaction.remove.expired.transaction.cleanup.interval.ms.*|.*transaction.state.log.load.buffer.size.*|.*transaction.state.log.min.isr.*|.*transaction.state.log.num.partitions.*|.*transaction.state.log.replication.factor.*|.*transaction.state.log.segment.bytes.*|.*transactional.id.expiration.ms.*|.*controlled.shutdown.enable.*|.*controlled.shutdown.max.retries.*|.*controlled.shutdown.retry.backoff.ms.*|.*fetch.max.bytes.*|.*fetch.purgatory.purge.interval.requests.*|.*group.initial.rebalance.delay.ms.*|.*group.max.session.timeout.ms.*|.*group.max.size.*|.*group.min.session.timeout.ms.*|.*log.cleaner.backoff.ms.*|.*log.cleaner.dedupe.buffer.size.*|.*log.cleaner.delete.retention.ms.*|.*log.cleaner.enable.*|.*log.cleaner.io.buffer.load.factor.*|.*log.cleaner.io.buffer.size.*|.*log.cleaner.io.max.bytes.per.second.*|.*log.cleaner.max.compaction.lag.ms.*|.*log.cleaner.min.cleanable.ratio.*|.*log.cleaner.min.compaction.lag.ms.*|.*log.cleaner.threads.*|.*log.cleanup.policy.*|.*log.index.interval.bytes.*|.*log.index.size.max.bytes.*|.*log.message.downconversion.enable.*|.*log.message.format.version.*|.*log.message.timestamp.difference.max.ms.*|.*log.message.timestamp.type.*|.*max.connection.creation.rate.*|.*max.connections.*|.*max.connections.per.ip.*|.*max.incremental.fetch.session.cache.slots.*|.*replica.fetch.backoff.ms.*|.*replica.fetch.max.bytes.*|.*replica.fetch.min.bytes.*|.*replica.fetch.response.max.bytes.*|.*replica.fetch.wait.max.ms.*|.*replica.high.watermark.checkpoint.interval.ms.*|.*replica.lag.time.max.ms.*|.*replica.selector.class.*|.*replica.socket.receive.buffer.bytes.*|.*replica.socket.timeout.ms.*|.*alter.config.policy.class.name.*|.*alter.log.dirs.replication.quota.window.num.*|.*alter.log.dirs.replication.quota.window.size.seconds.*|.*metrics.num.samples.*|.*metrics.recording.level.*|.*metrics.sample.window.ms.*|.*quota.window.num.*|.*quota.window.size.seconds.*|.*replication.quota.window.num.*|.*replication.quota.window.size.seconds.*|.*confluent.balancer.enable.*|.*confluent.balancer.throttle.bytes.per.second.*|.*confluent.balancer.heal.uneven.load.trigger.*|.*confluent.balancer.heal.broker.failure.threshold.ms.*|.*confluent.balancer.disk.max.load.*|.*confluent.balancer.exclude.topic.prefixes.*|.*confluent.balancer.exclude.topic.names.*|.*confluent.tier.local.hotset.bytes.*|.*confluent.tier.local.hotset.ms.*|.*confluent.tier.archiver.num.threads.*|.*confluent.tier.backend.*|.*confluent.tier.enable.*|.*confluent.tier.feature.*|.*confluent.tier.fetcher.num.threads.*|.*confluent.tier.max.partition.fetch.bytes.override.*|.*confluent.tier.metadata.replication.factor.*|.*confluent.operator.managed.*|.*confluent.ansible.managed.*|.*confluent.license.*
	confluent.telemetry.events.enable = true
	confluent.telemetry.metrics.collector.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	confluent.telemetry.metrics.collector.interval.ms = 60000
	confluent.telemetry.metrics.collector.slo.enabled = false
	confluent.telemetry.proxy.password = null
	confluent.telemetry.proxy.url = null
	confluent.telemetry.proxy.username = null

[INFO] 2023-11-08 14:51:17,170 [main] io.confluent.telemetry.collector.VolumeMetricsCollector$VolumeMetricsCollectorConfig logAll - VolumeMetricsCollectorConfig values: 
	confluent.telemetry.metrics.collector.volume.update.ms = 15000

[INFO] 2023-11-08 14:51:17,170 [main] io.confluent.telemetry.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	metrics.include = .*io.confluent.telemetry/.*.*|.*io.confluent.system/.*(process_cpu_load|max_file_descriptor_count|open_file_descriptor_count|system_cpu_load|system_load_average|free_physical_memory_size|total_physical_memory_size|disk_total_bytes|disk_usable_bytes|committed|used).*|.*io.confluent.kafka.rest/.*(connections_active|connections_closed_rate|request_error_rate|request_latency_avg|request_latency_max|request_rate|response_size_avg|response_size_max).*
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[WARN] 2023-11-08 14:51:17,170 [main] io.confluent.telemetry.ConfluentTelemetryConfig <init> - no telemetry exporters are enabled
[INFO] 2023-11-08 14:51:17,170 [main] io.confluent.telemetry.reporter.TelemetryReporter initEventLogger - Initializing the event logger
[INFO] 2023-11-08 14:51:17,171 [main] io.confluent.shaded.io.confluent.telemetry.events.EventLoggerConfig logAll - EventLoggerConfig values: 
	event.logger.cloudevent.codec = structured
	event.logger.exporter.class = class io.confluent.shaded.io.confluent.telemetry.events.exporter.http.EventHttpExporter

[INFO] 2023-11-08 14:51:17,171 [main] io.confluent.shaded.io.confluent.telemetry.events.exporter.http.HttpExporterConfig logAll - HttpExporterConfig values: 
	api.key = null
	api.secret = null
	buffer.batch.duration.max.ms = null
	buffer.batch.items.max = null
	buffer.inflight.submissions.max = null
	buffer.pending.batches.max = null
	client.attempts.max = null
	client.base.url = https://collector.telemetry.confluent.cloud
	client.compression = null
	client.connect.timeout.ms = null
	client.request.timeout.ms = null
	client.retry.delay.seconds = null
	enabled = false
	events.enabled = true
	metrics.enabled = true
	proxy.password = null
	proxy.url = null
	proxy.username = null
	type = http

[INFO] 2023-11-08 14:51:17,177 [main] io.confluent.telemetry.reporter.TelemetryReporter startMetricCollectorTask - Starting Confluent telemetry reporter with an interval of 60000 ms for resource = (type = kafka_rest)
[INFO] 2023-11-08 14:51:17,180 [main] io.confluent.http.server.KafkaHttpApplicationLoader load - Application provider 'KafkaRestApplicationProvider' provided 1 instance(s).
[INFO] 2023-11-08 14:51:17,189 [main] io.confluent.rest.ApplicationServer createThreadPool - Initial capacity 128, increased by 64, maximum capacity 2147483647.
[INFO] 2023-11-08 14:51:17,277 [main] io.confluent.rest.FileWatcher onFileChange - Configure watch file change: /mnt/sslcerts/keystore.jks
[INFO] 2023-11-08 14:51:17,279 [main] io.confluent.rest.ApplicationServer createSslContextFactory - Enabled SSL cert auto reload for: /mnt/sslcerts/keystore.jks
[INFO] 2023-11-08 14:51:17,298 [main] io.confluent.rest.ApplicationServer getConnectionFactories - Adding listener with HTTP/2: https://0.0.0.0:8090
[INFO] 2023-11-08 14:51:17,369 [main] io.confluent.kafka.http.server.KafkaHttpServerLoader load - Loaded KafkaHttpServer implementation class io.confluent.http.server.KafkaHttpServerImpl
[INFO] 2023-11-08 14:51:17,370 [main] io.confluent.http.server.KafkaHttpServerImpl transition - KafkaHttpServer transitioned from NEW to STARTING..
[INFO] 2023-11-08 14:51:17,410 [audit-init-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - Event logger has metadata for all topics
[INFO] 2023-11-08 14:51:17,521 [ce-kafka-http-server-start-thread] io.confluent.rest.Application configureHandler - Binding MetadataApiApplication to all listeners.
[INFO] 2023-11-08 14:51:17,646 [ce-kafka-http-server-start-thread] io.confluent.tokenapi.jwt.JwtConfig logAll - JwtConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	api.flavor = CP
	authentication.method = BEARER
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	cluster.registry.clusters = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	openapi.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	token.issuer = Confluent
	token.max.lifetime.ms = 3600000
	token.roles = clusters
	user.store = LDAP
	user.store.file.path = 
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:17,648 [ce-kafka-http-server-start-thread] io.confluent.tokenapi.jwt.JwsConfig logAll - JwsConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	api.flavor = CP
	authentication.method = BEARER
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	cluster.registry.clusters = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = [https://0.0.0.0:8090]
	metric.reporters = []
	metrics.jmx.prefix = rest-utils
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	openapi.enable = false
	port = 8090
	proxy.protocol.enabled = false
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = []
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	token.issuer = Confluent
	token.key.path = /mnt/secrets/mds-token/mdsTokenKeyPair.pem
	token.max.lifetime.ms = 3600000
	token.roles = clusters
	token.signature.algorithm = RS256
	user.store = LDAP
	user.store.file.path = 
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:17,702 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 11 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:51:17,711 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-command-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:17,712 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-command-0 in /mnt/data/data0/logs/_confluent-command-0 with properties {cleanup.policy=compact, min.insync.replicas=2}
[INFO] 2023-11-08 14:51:17,717 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-command-0 broker=0] No checkpointed highwatermark is found for partition _confluent-command-0
[INFO] 2023-11-08 14:51:17,718 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-command-0 broker=0] Log loaded for partition _confluent-command-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:17,718 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-command-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:17,718 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-command-0)
[INFO] 2023-11-08 14:51:17,719 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 11 for 1 partitions
[INFO] 2023-11-08 14:51:17,719 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions Map(_confluent-command-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:17,720 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 18ms correlationId 11 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:51:17,723 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 12
[INFO] 2023-11-08 14:51:17,727 [ce-kafka-http-server-start-thread] io.confluent.rest.Application configureHandler - Binding RbacApiApplication to all listeners.
[WARN] 2023-11-08 14:51:17,842 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[INFO] 2023-11-08 14:51:17,843 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.config.SchemaRegistryConfig logAll - SchemaRegistryConfig values: 
	auto.register.schemas = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	context.name.strategy = class io.confluent.kafka.serializers.context.NullContextNameStrategy
	id.compatibility.strict = true
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	normalize.schemas = false
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	use.latest.version = false
	use.schema.id = -1
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

[INFO] 2023-11-08 14:51:17,876 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.security.config.ConfluentSecureKafkaRestConfig logAll - ConfluentSecureKafkaRestConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	advertised.listeners = []
	api.endpoints.allowlist = []
	api.endpoints.blocklist = []
	api.v2.enable = false
	api.v3.enable = true
	api.v3.produce.rate.limit.cache.expiry.ms = 3600000
	api.v3.produce.rate.limit.enabled = false
	api.v3.produce.rate.limit.grace.period.ms = 30000
	api.v3.produce.rate.limit.max.bytes.per.sec = 10000000
	api.v3.produce.rate.limit.max.requests.per.sec = 10000
	api.v3.produce.response.thread.pool.size = 16
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	bootstrap.servers = kafka.confluent.svc.cluster.local:9073
	client.init.timeout.ms = 60000
	client.sasl.kerberos.kinit.cmd = /usr/bin/kinit
	client.sasl.kerberos.min.time.before.relogin = 60000
	client.sasl.kerberos.service.name = 
	client.sasl.kerberos.ticket.renew.jitter = 0.05
	client.sasl.kerberos.ticket.renew.window.factor = 0.8
	client.sasl.mechanism = OAUTHBEARER
	client.security.protocol = SASL_SSL
	client.ssl.cipher.suites = 
	client.ssl.enabled.protocols = TLSv1.2,TLSv1.1,TLSv1
	client.ssl.endpoint.identification.algorithm = 
	client.ssl.key.password = [hidden]
	client.ssl.keymanager.algorithm = SunX509
	client.ssl.keystore.location = 
	client.ssl.keystore.password = [hidden]
	client.ssl.keystore.type = JKS
	client.ssl.protocol = TLS
	client.ssl.provider = 
	client.ssl.trustmanager.algorithm = PKIX
	client.ssl.truststore.location = /mnt/sslcerts/truststore.jks
	client.ssl.truststore.password = [hidden]
	client.ssl.truststore.type = JKS
	client.timeout.ms = 500
	client.zk.session.timeout.ms = 30000
	compression.enable = true
	confluent.license = 
	confluent.license.topic = _confluent-license
	confluent.metadata.bootstrap.server.urls = https://kafka.confluent.svc.cluster.local:8090
	confluent.resource.name.authority = 
	confluent.rest.auth.propagate.method = SSL
	confluent.rest.auth.ssl.principal.mapping.rules = DEFAULT
	consumer.instance.timeout.ms = 300000
	consumer.iterator.backoff.ms = 50
	consumer.iterator.timeout.ms = 1
	consumer.request.max.bytes = 67108864
	consumer.request.timeout.ms = 1000
	consumer.threads = 50
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	fetch.min.bytes = -1
	host.name = 
	http2.enabled = true
	id = 
	idle.timeout.ms = 30000
	kafka.rest.resource.extension.class = [io.confluent.kafkarest.KafkaRestResourceExtension, io.confluent.kafkarest.security.KafkaRestSecurityResourceExtension]
	listener.protocol.map = []
	listeners = []
	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.jmx.prefix = kafka.rest
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8082
	producer.threads = 5
	proxy.protocol.enabled = false
	rate.limit.backend = guava
	rate.limit.costs = 
	rate.limit.default.cost = 1
	rate.limit.enable = false
	rate.limit.permits.per.sec = 50
	rate.limit.timeout.ms = 0
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json, application/vnd.kafka.v2+json]
	rest.servlet.initializor.classes = [io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler]
	schema.registry.url = http://localhost:8081
	shutdown.graceful.ms = 1000
	simpleconsumer.pool.size.max = 25
	simpleconsumer.pool.timeout.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []
	zookeeper.connect = 

[WARN] 2023-11-08 14:51:17,896 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[INFO] 2023-11-08 14:51:17,938 [LicenseBackgroundFetcher STARTING] io.confluent.common.security.license.LicenseBackgroundFetcher startUp - Setting up License Manager...
[INFO] 2023-11-08 14:51:17,948 [ce-kafka-http-server-start-thread] io.confluent.rest.Application configureHandler - Binding EmbeddedKafkaRestApplication to all listeners.
[INFO] 2023-11-08 14:51:17,952 [ce-kafka-http-server-start-thread] io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler$BearerConfig logAll - BearerConfig values: 
	access.control.allow.headers = 
	access.control.allow.methods = 
	access.control.allow.origin = 
	access.control.skip.options = true
	authentication.method = NONE
	authentication.realm = 
	authentication.roles = [*]
	authentication.skip.paths = []
	compression.enable = true
	csrf.prevention.enable = false
	csrf.prevention.token.endpoint = /csrf
	csrf.prevention.token.expiration.minutes = 30
	csrf.prevention.token.max.entries = 10000
	debug = false
	dos.filter.delay.ms = 100
	dos.filter.enabled = false
	dos.filter.insert.headers = true
	dos.filter.ip.whitelist = []
	dos.filter.managed.attr = false
	dos.filter.max.idle.tracker.ms = 30000
	dos.filter.max.requests.ms = 30000
	dos.filter.max.requests.per.sec = 25
	dos.filter.max.wait.ms = 50
	dos.filter.remote.port = false
	dos.filter.throttle.ms = 30000
	dos.filter.throttled.requests = 5
	dos.filter.track.global = false
	expose.internal.connect.endpoints = false
	http2.enabled = true
	idle.timeout.ms = 30000
	listener.protocol.map = []
	listeners = []
	metric.reporters = [io.confluent.telemetry.reporter.TelemetryReporter]
	metrics.jmx.prefix = kafka.rest
	metrics.num.samples = 2
	metrics.sample.window.ms = 30000
	metrics.tag.map = []
	nosniff.prevention.enable = false
	port = 8080
	proxy.protocol.enabled = false
	public.key.path = /mnt/secrets/mds-token/mdsPublicKey.pem
	reject.options.request = false
	request.logger.name = io.confluent.rest-utils.requests
	request.queue.capacity = 2147483647
	request.queue.capacity.growby = 64
	request.queue.capacity.init = 128
	resource.extension.classes = []
	response.http.headers.config = 
	response.mediatype.default = application/json
	response.mediatype.preferred = [application/json]
	rest.servlet.initializor.classes = [io.confluent.common.security.jetty.initializer.InstallBearerOrBasicSecurityHandler]
	shutdown.graceful.ms = 1000
	ssl.cipher.suites = []
	ssl.client.auth = false
	ssl.client.authentication = NONE
	ssl.enabled.protocols = []
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = 
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.reload = true
	ssl.keystore.type = JKS
	ssl.keystore.watch.location = 
	ssl.protocol = TLS
	ssl.provider = 
	ssl.trustmanager.algorithm = 
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	thread.pool.max = 200
	thread.pool.min = 8
	token.issuer = Confluent
	websocket.path.prefix = /ws
	websocket.servlet.initializor.classes = []

[INFO] 2023-11-08 14:51:17,953 [ce-kafka-http-server-start-thread] io.confluent.kafka.clients.plugins.auth.jwt.JwtAuthenticatorConfig logAll - JwtAuthenticatorConfig values: 
	allowUnsafeURL = false
	audience = 
	issuer = Confluent
	jkuDomainWhiteList = []
	jwksLocation = /mnt/secrets/mds-token/mdsPublicKey.pem
	verificationKeyRefreshInterval = 3600000
	verificationKeyResolver = pemfile

[INFO] 2023-11-08 14:51:17,955 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:51:17,956 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:17,957 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[INFO] 2023-11-08 14:51:17,973 [ce-kafka-http-server-start-thread] io.confluent.security.auth.client.RestClientConfig logAll - RestClientConfig values: 
	confluent.metadata.basic.auth.credentials.path = null
	confluent.metadata.basic.auth.credentials.provider = USER_INFO
	confluent.metadata.basic.auth.user.info = [hidden]
	confluent.metadata.bootstrap.server.urls = [https://kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.enable.server.urls.refresh = true
	confluent.metadata.http.auth.credentials.provider = BASIC
	confluent.metadata.http.request.timeout.ms = 10000
	confluent.metadata.request.timeout.ms = 30000
	confluent.metadata.server.urls.fail.on.401 = false
	confluent.metadata.server.urls.max.age.ms = 600000
	confluent.metadata.server.urls.max.retries = 5
	confluent.metadata.token.auth.credential = [hidden]

[WARN] 2023-11-08 14:51:17,974 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,975 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:17,975 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:17,975 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:17,975 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455077975
[WARN] 2023-11-08 14:51:17,975 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser registerAppInfo - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=confluent-metrics-reporter
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:436)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:292)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:319)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:304)
	at io.confluent.metrics.reporter.ConfluentMetricsReporter.configure(ConfluentMetricsReporter.java:147)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstance(AbstractConfig.java:405)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:478)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:459)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:552)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:143)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:49)
	at io.confluent.license.LicenseStore.createTopicDescription(LicenseStore.java:413)
	at io.confluent.license.LicenseStore.<init>(LicenseStore.java:122)
	at io.confluent.license.LicenseStore.<init>(LicenseStore.java:104)
	at io.confluent.license.LicenseStore.<init>(LicenseStore.java:93)
	at io.confluent.license.LicenseManager.<init>(LicenseManager.java:151)
	at io.confluent.common.security.license.LicenseBackgroundFetcher.startUp(LicenseBackgroundFetcher.java:67)
	at com.google.common.util.concurrent.AbstractScheduledService$ServiceDelegate$2.run(AbstractScheduledService.java:258)
	at com.google.common.util.concurrent.Callables$4.run(Callables.java:119)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:17,975 [ce-kafka-http-server-start-thread] io.confluent.security.auth.client.RestClientConfig logAll - RestClientConfig values: 
	confluent.metadata.basic.auth.credentials.path = null
	confluent.metadata.basic.auth.credentials.provider = USER_INFO
	confluent.metadata.basic.auth.user.info = [hidden]
	confluent.metadata.bootstrap.server.urls = [https://kafka.confluent.svc.cluster.local:8090]
	confluent.metadata.enable.server.urls.refresh = true
	confluent.metadata.http.auth.credentials.provider = BASIC
	confluent.metadata.http.request.timeout.ms = 10000
	confluent.metadata.request.timeout.ms = 30000
	confluent.metadata.server.urls.fail.on.401 = false
	confluent.metadata.server.urls.max.age.ms = 600000
	confluent.metadata.server.urls.max.retries = 5
	confluent.metadata.token.auth.credential = [hidden]

[WARN] 2023-11-08 14:51:17,979 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,979 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,979 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,979 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,979 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,979 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,979 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,979 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,979 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,980 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:17,981 [ce-kafka-http-server-start-thread] io.confluent.security.auth.client.RestClientConfig$SslClientConfig logAll - SslClientConfig values: 
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = HTTPS
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:17,981 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:17,993 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455077993
[INFO] 2023-11-08 14:51:18,041 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-command-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:18,041 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-command-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:18,050 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,064 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:51:18,066 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,083 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:51:18,084 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:51:18,084 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:51:18,084 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:51:18,107 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,111 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,112 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,112 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:51:18,112 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,112 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[ERROR] 2023-11-08 14:51:18,114 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,118 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,146 [LicenseBackgroundFetcher STARTING] io.confluent.license.LicenseStore start - Starting License Store
[INFO] 2023-11-08 14:51:18,146 [LicenseBackgroundFetcher STARTING] org.apache.kafka.connect.util.KafkaBasedLog start - Starting KafkaBasedLog with topic _confluent-command
[INFO] 2023-11-08 14:51:18,146 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:51:18,153 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:18,154 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:18,158 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,158 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,158 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,159 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,159 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078158
[DEBUG] 2023-11-08 14:51:18,160 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 1, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,161 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,168 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[WARN] 2023-11-08 14:51:18,173 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,173 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,174 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,175 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,176 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,177 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,177 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,177 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,177 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078177
[INFO] 2023-11-08 14:51:18,197 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.Server doStart - jetty-9.4.44.v20210927; built: 2021-09-27T23:02:44.612Z; git: 8da83308eeca865e495e53ef315a249d63ba9332; jvm 11.0.14.1+1-LTS
[INFO] 2023-11-08 14:51:18,199 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:18,208 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 2, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,209 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,210 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,239 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:51:18,241 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,241 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:51:18,241 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[DEBUG] 2023-11-08 14:51:18,306 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 3, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,308 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,309 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,249 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:51:18,310 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:51:18,326 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,326 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,326 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,326 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:51:18,326 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,326 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,328 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 30
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[INFO] 2023-11-08 14:51:18,330 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:18,332 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:18,341 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,341 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,341 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,341 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,342 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078341
[DEBUG] 2023-11-08 14:51:18,349 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:18,363 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[WARN] 2023-11-08 14:51:18,365 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,365 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,365 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'auto.register.schemas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,365 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,365 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,365 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,365 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,365 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,366 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'schema.registry.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,367 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'use.latest.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,368 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078368
[INFO] 2023-11-08 14:51:18,382 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:18,389 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 4, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[INFO] 2023-11-08 14:51:18,390 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.session doStart - DefaultSessionIdManager workerName=node0
[ERROR] 2023-11-08 14:51:18,390 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,390 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.session doStart - No SessionScavenger set, using defaults
[INFO] 2023-11-08 14:51:18,391 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.Metadata update - [Consumer clientId=confluent-license-consumer, groupId=null] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:18,391 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,392 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporter onUpdate - Starting Confluent metrics reporter for cluster id 9PWH12e6ROOpezLRDGV6Ag with an interval of 30000 ms
[INFO] 2023-11-08 14:51:18,393 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,393 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:51:18,393 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:51:18,393 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:51:18,394 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:51:18,396 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:18,396 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,400 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,400 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:51:18,400 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:18,400 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:18,393 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.session startScavenging - node0 Scavenging every 660000ms
[INFO] 2023-11-08 14:51:18,402 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.consumer for confluent-license-consumer unregistered
[INFO] 2023-11-08 14:51:18,402 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-producer
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[INFO] 2023-11-08 14:51:18,403 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:18,404 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:18,413 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,414 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,414 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,414 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,414 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078414
[WARN] 2023-11-08 14:51:18,426 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,426 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,426 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,430 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,430 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,430 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,430 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,430 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,431 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,432 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,433 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,434 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,434 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,434 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,434 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,434 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,434 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,434 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,435 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,435 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,435 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,435 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,435 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,435 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,435 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078435
[INFO] 2023-11-08 14:51:18,436 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-consumer
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 30
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[INFO] 2023-11-08 14:51:18,437 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:18,438 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:18,441 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,441 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,441 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,442 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,442 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078441
[WARN] 2023-11-08 14:51:18,442 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser registerAppInfo - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=confluent-metrics-reporter
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:436)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:292)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:319)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:304)
	at io.confluent.metrics.reporter.ConfluentMetricsReporter.configure(ConfluentMetricsReporter.java:147)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstance(AbstractConfig.java:405)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:478)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:459)
	at org.apache.kafka.clients.consumer.KafkaConsumer.buildMetrics(KafkaConsumer.java:872)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:699)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:664)
	at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:612)
	at org.apache.kafka.connect.util.KafkaBasedLog.createConsumer(KafkaBasedLog.java:377)
	at org.apache.kafka.connect.util.KafkaBasedLog.start(KafkaBasedLog.java:221)
	at io.confluent.license.LicenseStore.startLog(LicenseStore.java:263)
	at io.confluent.license.LicenseStore.start(LicenseStore.java:249)
	at io.confluent.license.LicenseManager.<init>(LicenseManager.java:229)
	at io.confluent.license.LicenseManager.<init>(LicenseManager.java:211)
	at io.confluent.license.LicenseManager.<init>(LicenseManager.java:151)
	at io.confluent.common.security.license.LicenseBackgroundFetcher.startUp(LicenseBackgroundFetcher.java:67)
	at com.google.common.util.concurrent.AbstractScheduledService$ServiceDelegate$2.run(AbstractScheduledService.java:258)
	at com.google.common.util.concurrent.Callables$4.run(Callables.java:119)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:304)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[WARN] 2023-11-08 14:51:18,452 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,452 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,452 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'auto.register.schemas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,452 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,452 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,452 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,452 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,452 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,452 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,453 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,454 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,454 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,454 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,454 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,454 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,454 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,455 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,455 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,455 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,455 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,455 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,455 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,455 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,456 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,456 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,456 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,456 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,456 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,456 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,456 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,456 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,457 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,457 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,457 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,457 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,457 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'schema.registry.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,457 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,457 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,458 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,458 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,458 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,458 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,458 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,458 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,459 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,459 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,459 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,459 [kafka-producer-network-thread | confluent-license-producer] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-license-producer] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,459 [kafka-producer-network-thread | confluent-license-producer] io.confluent.metrics.reporter.ConfluentMetricsReporter onUpdate - Starting Confluent metrics reporter for cluster id 9PWH12e6ROOpezLRDGV6Ag with an interval of 30000 ms
[WARN] 2023-11-08 14:51:18,459 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,460 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,460 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'use.latest.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,460 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,460 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,460 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,460 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,460 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:18,460 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:18,461 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:18,461 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:18,461 [LicenseBackgroundFetcher STARTING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455078461
[INFO] 2023-11-08 14:51:18,473 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,481 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,492 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.Metadata update - [Consumer clientId=confluent-license-consumer, groupId=null] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:18,492 [LicenseBackgroundFetcher STARTING] io.confluent.metrics.reporter.ConfluentMetricsReporter onUpdate - Starting Confluent metrics reporter for cluster id 9PWH12e6ROOpezLRDGV6Ag with an interval of 30000 ms
[DEBUG] 2023-11-08 14:51:18,494 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 5, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,495 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,495 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,616 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 6, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,617 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,617 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,619 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.KafkaConsumer assign - [Consumer clientId=confluent-license-consumer, groupId=null] Subscribed to partition(s): _confluent-command-0
[INFO] 2023-11-08 14:51:18,619 [LicenseBackgroundFetcher STARTING] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=confluent-license-consumer, groupId=null] Seeking to EARLIEST offset of partition _confluent-command-0
[INFO] 2023-11-08 14:51:18,669 [LicenseBackgroundFetcher STARTING] org.apache.kafka.connect.util.KafkaBasedLog start - Finished reading KafkaBasedLog for topic _confluent-command
[INFO] 2023-11-08 14:51:18,669 [LicenseBackgroundFetcher STARTING] org.apache.kafka.connect.util.KafkaBasedLog start - Started KafkaBasedLog for topic _confluent-command
[INFO] 2023-11-08 14:51:18,669 [LicenseBackgroundFetcher STARTING] io.confluent.license.LicenseStore start - Started License Store
[INFO] 2023-11-08 14:51:18,669 [LicenseBackgroundFetcher STARTING] io.confluent.common.security.license.LicenseBackgroundFetcher startUp - Finished setting up License Manager.
[DEBUG] 2023-11-08 14:51:18,757 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 7, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,758 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,759 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[WARN] 2023-11-08 14:51:18,849 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.metadataapi.resources.MetadataResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.metadataapi.resources.MetadataResource will be ignored. 
[DEBUG] 2023-11-08 14:51:18,919 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 8, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:18,920 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:18,920 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:18,992 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@2e0a72fc{/v1/metadata,null,AVAILABLE}
[WARN] 2023-11-08 14:51:19,090 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.tokenapi.resources.v1.V1TokenResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.tokenapi.resources.v1.V1TokenResource will be ignored. 
[WARN] 2023-11-08 14:51:19,090 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1FeaturesResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1FeaturesResource will be ignored. 
[WARN] 2023-11-08 14:51:19,090 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1AuthorizeResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1AuthorizeResource will be ignored. 
[WARN] 2023-11-08 14:51:19,090 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1RolesResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1RolesResource will be ignored. 
[WARN] 2023-11-08 14:51:19,090 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1LookupResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1LookupResource will be ignored. 
[WARN] 2023-11-08 14:51:19,090 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1PrincipalsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1PrincipalsResource will be ignored. 
[WARN] 2023-11-08 14:51:19,091 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1MetadataServiceResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1MetadataServiceResource will be ignored. 
[WARN] 2023-11-08 14:51:19,091 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1AclResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1AclResource will be ignored. 
[WARN] 2023-11-08 14:51:19,091 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1OperationsResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1OperationsResource will be ignored. 
[WARN] 2023-11-08 14:51:19,091 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1UserGroupResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1UserGroupResource will be ignored. 
[WARN] 2023-11-08 14:51:19,091 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1ClusterRegistryResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1ClusterRegistryResource will be ignored. 
[WARN] 2023-11-08 14:51:19,091 [ce-kafka-http-server-start-thread] org.glassfish.jersey.internal.inject.Providers checkProviderRuntime - A provider io.confluent.rbacapi.resources.v1.V1AuditLogConfigResource registered in SERVER runtime does not implement any provider interfaces applicable in the SERVER runtime. Due to constraint configuration problems the provider io.confluent.rbacapi.resources.v1.V1AuditLogConfigResource will be ignored. 
[DEBUG] 2023-11-08 14:51:19,102 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 9, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:19,103 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:19,104 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:19,256 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:51:19,257 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:19,257 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:19,260 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,261 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:19,261 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,261 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,262 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079261
[WARN] 2023-11-08 14:51:19,262 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser registerAppInfo - Error registering AppInfo mbean
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=confluent-metrics-reporter
	at java.management/com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:436)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1855)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:955)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:890)
	at java.management/com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:320)
	at java.management/com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:436)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:292)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:319)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:304)
	at io.confluent.metrics.reporter.ConfluentMetricsReporter.configure(ConfluentMetricsReporter.java:147)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstance(AbstractConfig.java:405)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:478)
	at org.apache.kafka.common.config.AbstractConfig.getConfiguredInstances(AbstractConfig.java:459)
	at org.apache.kafka.clients.admin.KafkaAdminClient.createInternal(KafkaAdminClient.java:552)
	at org.apache.kafka.clients.admin.Admin.create(Admin.java:143)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:49)
	at io.confluent.license.LicenseManager$BasicClusterClient.clusterId(LicenseManager.java:743)
	at io.confluent.license.LicenseManager.registerOrValidateLicense(LicenseManager.java:458)
	at io.confluent.common.security.license.LicenseBackgroundFetcher.runOneIteration(LicenseBackgroundFetcher.java:88)
	at com.google.common.util.concurrent.AbstractScheduledService$ServiceDelegate$Task.run(AbstractScheduledService.java:221)
	at com.google.common.util.concurrent.Callables$4.run(Callables.java:119)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,266 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,267 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,268 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:19,268 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,268 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,268 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079268
[INFO] 2023-11-08 14:51:19,278 [ce-kafka-http-server-start-thread] org.eclipse.jetty.util.ssl.SslContextFactory load - x509=X509@35148dc1(ca,h=[testca],a=[],w=[]) for Server@4d0be5b4[provider=null,keyStore=file:///mnt/sslcerts/keystore.jks,trustStore=file:///mnt/sslcerts/truststore.jks]
[INFO] 2023-11-08 14:51:19,282 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:19,282 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@74290b85{/security,null,AVAILABLE}
[DEBUG] 2023-11-08 14:51:19,304 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 10, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:19,305 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:19,306 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:19,306 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:51:19,307 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:19,307 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:51:19,307 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:51:19,308 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:51:19,308 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:51:19,309 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:19,309 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:19,309 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:19,309 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:51:19,309 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:19,309 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:19,310 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[WARN] 2023-11-08 14:51:19,328 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[WARN] 2023-11-08 14:51:19,332 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[WARN] 2023-11-08 14:51:19,343 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[WARN] 2023-11-08 14:51:19,357 [ce-kafka-http-server-start-thread] io.confluent.kafkarest.KafkaRestConfig getSchemaRegistryConfigs - Using default value http://localhost:8081 for config schema.registry.url. In a future release this config won't have adefault value anymore. If you are using Schema Registry, please, specify schema.registry.urlexplicitly. Requests will fail in a future release if you try to use Schema Registry but have not specified a value for schema.registry.url.
[DEBUG] 2023-11-08 14:51:19,526 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 11, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:19,527 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:19,527 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:19,768 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 12, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[ERROR] 2023-11-08 14:51:19,769 [pool-23-thread-1] io.confluent.security.auth.client.rest.RestClient lambda$submit$0 - Unexpected exception sending HTTP Request.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[DEBUG] 2023-11-08 14:51:19,770 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - Http request to https://kafka.confluent.svc.cluster.local:8090 failed, selecting next url.
java.net.ConnectException: Connection refused (Connection refused)
	at java.base/java.net.PlainSocketImpl.socketConnect(Native Method)
	at java.base/java.net.AbstractPlainSocketImpl.doConnect(AbstractPlainSocketImpl.java:412)
	at java.base/java.net.AbstractPlainSocketImpl.connectToAddress(AbstractPlainSocketImpl.java:255)
	at java.base/java.net.AbstractPlainSocketImpl.connect(AbstractPlainSocketImpl.java:237)
	at java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:392)
	at java.base/java.net.Socket.connect(Socket.java:615)
	at java.base/sun.security.ssl.SSLSocketImpl.connect(SSLSocketImpl.java:300)
	at java.base/sun.net.NetworkClient.doConnect(NetworkClient.java:177)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:474)
	at java.base/sun.net.www.http.HttpClient.openServer(HttpClient.java:569)
	at java.base/sun.net.www.protocol.https.HttpsClient.<init>(HttpsClient.java:266)
	at java.base/sun.net.www.protocol.https.HttpsClient.New(HttpsClient.java:373)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.getNewHttpClient(AbstractDelegateHttpsURLConnection.java:203)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect0(HttpURLConnection.java:1192)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.plainConnect(HttpURLConnection.java:1086)
	at java.base/sun.net.www.protocol.https.AbstractDelegateHttpsURLConnection.connect(AbstractDelegateHttpsURLConnection.java:189)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream0(HttpURLConnection.java:1597)
	at java.base/sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1525)
	at java.base/java.net.HttpURLConnection.getResponseCode(HttpURLConnection.java:527)
	at java.base/sun.net.www.protocol.https.HttpsURLConnectionImpl.getResponseCode(HttpsURLConnectionImpl.java:334)
	at io.confluent.security.auth.client.rest.RestClient$HTTPRequestSender.lambda$submit$0(RestClient.java:381)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
[INFO] 2023-11-08 14:51:19,796 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@3ef0703d{/kafka,null,AVAILABLE}
[INFO] 2023-11-08 14:51:19,821 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@1ed1761e{/ws,null,AVAILABLE}
[INFO] 2023-11-08 14:51:19,823 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@63317aaa{/ws,null,AVAILABLE}
[INFO] 2023-11-08 14:51:19,824 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.handler.ContextHandler doStart - Started o.e.j.s.ServletContextHandler@5ff8206a{/ws,null,AVAILABLE}
[INFO] 2023-11-08 14:51:19,854 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.AbstractConnector doStart - Started NetworkTrafficServerConnector@3cae4518{SSL, (ssl, alpn, h2, http/1.1)}{0.0.0.0:8090}
[INFO] 2023-11-08 14:51:19,855 [ce-kafka-http-server-start-thread] org.eclipse.jetty.server.Server doStart - Started @27982ms
[INFO] 2023-11-08 14:51:19,856 [ce-kafka-http-server-start-thread] io.confluent.http.server.KafkaHttpServerImpl transition - KafkaHttpServer transitioned from STARTING to RUNNING..
[INFO] 2023-11-08 14:51:19,887 [main] kafka.server.KafkaServer info - [KafkaServer id=0] Skipping durability audit instantiation
[INFO] 2023-11-08 14:51:19,891 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,891 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,891 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079891
[INFO] 2023-11-08 14:51:19,891 [main] kafka.server.KafkaServer info - [KafkaServer id=0] started
[INFO] 2023-11-08 14:51:19,892 [main] io.confluent.license.validator.LicenseConfig logAll - LicenseConfig values: 
	confluent.license = 
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 3

[INFO] 2023-11-08 14:51:19,893 [main] io.confluent.license.validator.LicenseConfig logAll - LicenseConfig values: 
	confluent.license = 
	confluent.license.retry.backoff.max.ms = 100000
	confluent.license.retry.backoff.min.ms = 1000
	confluent.license.topic = _confluent-license
	confluent.license.topic.create.timeout.ms = 600000
	confluent.license.topic.replication.factor = 3

[INFO] 2023-11-08 14:51:19,894 [main] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:19,896 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,896 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,896 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,897 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,898 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,899 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,899 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,899 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,899 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,899 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,900 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,900 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,900 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,900 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,900 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,901 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,901 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,901 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,901 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,901 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,901 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,901 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,902 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,902 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,902 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,902 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,902 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,902 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,902 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,903 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,903 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,903 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,903 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,903 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,903 [main] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:19,903 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,904 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,904 [main] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079903
[INFO] 2023-11-08 14:51:19,942 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for _confluent-license-admin-0 unregistered
[INFO] 2023-11-08 14:51:19,943 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:19,943 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:19,943 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:19,945 [confluent-license-manager] io.confluent.license.LicenseStore start - Starting License Store
[INFO] 2023-11-08 14:51:19,945 [confluent-license-manager] org.apache.kafka.connect.util.KafkaBasedLog start - Starting KafkaBasedLog with topic _confluent-command
[INFO] 2023-11-08 14:51:19,946 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,949 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,950 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,951 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079951
[INFO] 2023-11-08 14:51:19,986 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for _confluent-license-admin-0 unregistered
[INFO] 2023-11-08 14:51:19,990 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:19,990 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:19,990 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:19,992 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = latest
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-consumer-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = true
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[WARN] 2023-11-08 14:51:19,997 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,997 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,997 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,997 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,997 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,997 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,997 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,997 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,998 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:19,999 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455079999
[INFO] 2023-11-08 14:51:20,019 [confluent-license-manager] org.apache.kafka.clients.Metadata update - [Consumer clientId=_confluent-license-consumer-0, groupId=null] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[DEBUG] 2023-11-08 14:51:20,033 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient sendRequest - HTTP Request: Failures: 13, Host: kafka.confluent.svc.cluster.local, Port: 8090, URI: https://kafka.confluent.svc.cluster.local:8090, requestTimeout: 30000
[DEBUG] 2023-11-08 14:51:20,082 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler searchForLdapUser - Searching for user with null LDAP context: Must create new context. LdapAuthenticateCallbackHandler@23d8a93c Thread:qtp1527668063-183 user:krp searchCounter:0 
[DEBUG] 2023-11-08 14:51:20,082 [qtp1527668063-189] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler searchForLdapUser - Searching for user with null LDAP context: Must create new context. LdapAuthenticateCallbackHandler@23d8a93c Thread:qtp1527668063-189 user:kafka searchCounter:3 
[DEBUG] 2023-11-08 14:51:20,082 [qtp1527668063-182] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler searchForLdapUser - Searching for user with null LDAP context: Must create new context. LdapAuthenticateCallbackHandler@23d8a93c Thread:qtp1527668063-182 user:krp searchCounter:2 
[DEBUG] 2023-11-08 14:51:20,082 [qtp1527668063-199] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler searchForLdapUser - Searching for user with null LDAP context: Must create new context. LdapAuthenticateCallbackHandler@23d8a93c Thread:qtp1527668063-199 user:krp searchCounter:1 
[TRACE] 2023-11-08 14:51:20,091 [qtp1527668063-189] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 189]
[TRACE] 2023-11-08 14:51:20,091 [qtp1527668063-182] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user krp with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 182]
[TRACE] 2023-11-08 14:51:20,091 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user krp with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 183]
[TRACE] 2023-11-08 14:51:20,101 [qtp1527668063-199] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user krp with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 199]
[TRACE] 2023-11-08 14:51:20,110 [qtp1527668063-182] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user krp with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=krp,dc=test,dc=com [Thread 182]
[TRACE] 2023-11-08 14:51:20,115 [qtp1527668063-189] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 189]
[TRACE] 2023-11-08 14:51:20,117 [qtp1527668063-199] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user krp with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=krp,dc=test,dc=com [Thread 199]
[DEBUG] 2023-11-08 14:51:20,119 [qtp1527668063-182] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for krp
[DEBUG] 2023-11-08 14:51:20,120 [qtp1527668063-189] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[TRACE] 2023-11-08 14:51:20,125 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user krp with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=krp,dc=test,dc=com [Thread 183]
[DEBUG] 2023-11-08 14:51:20,140 [qtp1527668063-183] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for krp
[DEBUG] 2023-11-08 14:51:20,140 [qtp1527668063-199] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for krp
[INFO] 2023-11-08 14:51:20,170 [confluent-license-manager] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:20,171 [confluent-license-manager] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:20,174 [confluent-license-manager] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:20,175 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.consumer for _confluent-license-consumer-0 unregistered
[INFO] 2023-11-08 14:51:20,176 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-producer-0
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[WARN] 2023-11-08 14:51:20,181 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,183 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,183 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,183 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,183 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,183 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,184 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,184 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,184 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,185 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,185 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,185 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,185 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,185 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,185 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,185 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,186 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455080187
[INFO] 2023-11-08 14:51:20,187 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logAll - ConsumerConfig values: 
	allow.auto.create.topics = true
	auto.commit.interval.ms = 5000
	auto.offset.reset = earliest
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	check.crcs = true
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-consumer-0
	client.rack = 
	connections.max.idle.ms = 540000
	default.api.timeout.ms = 60000
	enable.auto.commit = false
	exclude.internal.topics = true
	fetch.max.bytes = 52428800
	fetch.max.wait.ms = 500
	fetch.min.bytes = 1
	group.id = null
	group.instance.id = null
	heartbeat.interval.ms = 3000
	interceptor.classes = []
	internal.leave.group.on.close = true
	internal.throw.on.fetch.stable.offset.unsupported = false
	isolation.level = read_uncommitted
	key.deserializer = class io.confluent.license.LicenseStore$LicenseKeySerde
	max.partition.fetch.bytes = 1048576
	max.poll.interval.ms = 300000
	max.poll.records = 500
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partition.assignment.strategy = [class org.apache.kafka.clients.consumer.RangeAssignor, class org.apache.kafka.clients.consumer.CooperativeStickyAssignor]
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 120000
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	session.timeout.ms = 45000
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	value.deserializer = class io.confluent.license.LicenseStore$LicenseMessageSerde

[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,190 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,191 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,192 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,192 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,192 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,192 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,192 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,192 [confluent-license-manager] org.apache.kafka.clients.consumer.ConsumerConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:20,192 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:20,192 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:20,192 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455080192
[INFO] 2023-11-08 14:51:20,202 [kafka-producer-network-thread | _confluent-license-producer-0] org.apache.kafka.clients.Metadata update - [Producer clientId=_confluent-license-producer-0] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:20,214 [confluent-license-manager] org.apache.kafka.clients.Metadata update - [Consumer clientId=_confluent-license-consumer-0, groupId=null] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:20,215 [confluent-license-manager] org.apache.kafka.clients.consumer.KafkaConsumer assign - [Consumer clientId=_confluent-license-consumer-0, groupId=null] Subscribed to partition(s): _confluent-command-0
[INFO] 2023-11-08 14:51:20,216 [confluent-license-manager] org.apache.kafka.clients.consumer.internals.SubscriptionState lambda$requestOffsetReset$3 - [Consumer clientId=_confluent-license-consumer-0, groupId=null] Seeking to EARLIEST offset of partition _confluent-command-0
[INFO] 2023-11-08 14:51:20,219 [qtp1527668063-182] io.confluent.mds.request.logger log - 1 * Server has received a request on thread qtp1527668063-182null1 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull1 > User Principal: krp
1 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null1 > Cache-Control: no-cachenull1 > Connection: keep-alivenull1 > Content-Type: application/jsonnull1 > Host: kafka.confluent.svc.cluster.local:8090null1 > Pragma: no-cachenull1 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:20,224 [qtp1527668063-199] io.confluent.mds.request.logger log - 3 * Server has received a request on thread qtp1527668063-199null3 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull3 > User Principal: krp
3 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null3 > Cache-Control: no-cachenull3 > Connection: keep-alivenull3 > Content-Type: application/jsonnull3 > Host: kafka.confluent.svc.cluster.local:8090null3 > Pragma: no-cachenull3 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:20,219 [qtp1527668063-189] io.confluent.mds.request.logger log - 4 * Server has received a request on thread qtp1527668063-189null4 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull4 > User Principal: kafka
4 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null4 > Cache-Control: no-cachenull4 > Connection: keep-alivenull4 > Content-Type: application/jsonnull4 > Host: kafka.confluent.svc.cluster.local:8090null4 > Pragma: no-cachenull4 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:20,225 [qtp1527668063-183] io.confluent.mds.request.logger log - 2 * Server has received a request on thread qtp1527668063-183null2 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull2 > User Principal: krp
2 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null2 > Cache-Control: no-cachenull2 > Connection: keep-alivenull2 > Content-Type: application/jsonnull2 > Host: kafka.confluent.svc.cluster.local:8090null2 > Pragma: no-cachenull2 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:20,271 [qtp1527668063-182] io.confluent.mds.request.logger log - 1 * Server responded with a response on thread qtp1527668063-182null1 < 200null1 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:20,284 [confluent-license-manager] org.apache.kafka.connect.util.KafkaBasedLog start - Finished reading KafkaBasedLog for topic _confluent-command
[INFO] 2023-11-08 14:51:20,284 [confluent-license-manager] org.apache.kafka.connect.util.KafkaBasedLog start - Started KafkaBasedLog for topic _confluent-command
[INFO] 2023-11-08 14:51:20,284 [confluent-license-manager] io.confluent.license.LicenseStore start - Started License Store
[INFO] 2023-11-08 14:51:20,304 [qtp1527668063-189] io.confluent.mds.request.logger log - 4 * Server responded with a response on thread qtp1527668063-189null4 < 200null4 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:20,304 [qtp1527668063-183] io.confluent.mds.request.logger log - 2 * Server responded with a response on thread qtp1527668063-183null2 < 200null2 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:20,304 [qtp1527668063-199] io.confluent.mds.request.logger log - 3 * Server responded with a response on thread qtp1527668063-199null3 < 200null3 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[DEBUG] 2023-11-08 14:51:20,315 [pool-24-thread-1] io.confluent.security.auth.client.rest.RestClient run - Successfully fetched MDS URLs ([https://kafka-1.kafka.confluent.svc.cluster.local:8090, https://kafka-2.kafka.confluent.svc.cluster.local:8090, https://kafka-0.kafka.confluent.svc.cluster.local:8090, https://kafka.confluent.svc.cluster.local:8090])
[INFO] 2023-11-08 14:51:20,330 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.0.17 - kafka [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 258
[INFO] 2023-11-08 14:51:20,331 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.0.17 - kafka [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 259
[INFO] 2023-11-08 14:51:20,331 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.0.17 - kafka [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 259
[INFO] 2023-11-08 14:51:20,333 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 269
[INFO] 2023-11-08 14:51:20,333 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 269
[INFO] 2023-11-08 14:51:20,333 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 269
[INFO] 2023-11-08 14:51:20,334 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 270
[INFO] 2023-11-08 14:51:20,334 [qtp1527668063-182] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 634 "-" "Java/11.0.14.1" 270
[INFO] 2023-11-08 14:51:20,334 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 270
[INFO] 2023-11-08 14:51:20,334 [qtp1527668063-182] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 634 "-" "Java/11.0.14.1" 270
[INFO] 2023-11-08 14:51:20,334 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 270
[INFO] 2023-11-08 14:51:20,334 [qtp1527668063-182] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:20 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 634 "-" "Java/11.0.14.1" 270
[INFO] 2023-11-08 14:51:20,787 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = _confluent-license-admin-0
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:20,790 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,790 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,790 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,790 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,790 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,790 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,790 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'min.insync.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,791 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:20,792 [confluent-license-manager] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455080792
[INFO] 2023-11-08 14:51:20,878 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for _confluent-license-admin-0 unregistered
[INFO] 2023-11-08 14:51:20,880 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:20,880 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:20,880 [kafka-admin-client-thread | _confluent-license-admin-0] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:20,880 [confluent-license-manager] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[INFO] 2023-11-08 14:51:20,885 [main] kafka.metrics.BrokerLoad startMetric - Starting delay for broker load metric
[INFO] 2023-11-08 14:51:21,048 [data-plane-kafka-request-handler-0] kafka.authorizer.logger logAuthorization - Principal = User:krp is Denied Operation = Describe from host = 10.40.1.6 on resource = Topic:LITERAL:_confluent-license
[TRACE] 2023-11-08 14:51:21,099 [qtp1527668063-189] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user krp with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 189]
[TRACE] 2023-11-08 14:51:21,099 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user krp with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 183]
[TRACE] 2023-11-08 14:51:21,101 [qtp1527668063-189] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user krp with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=krp,dc=test,dc=com [Thread 189]
[TRACE] 2023-11-08 14:51:21,102 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user krp with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=krp,dc=test,dc=com [Thread 183]
[DEBUG] 2023-11-08 14:51:21,103 [qtp1527668063-189] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for krp
[DEBUG] 2023-11-08 14:51:21,106 [qtp1527668063-183] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for krp
[INFO] 2023-11-08 14:51:21,108 [qtp1527668063-189] io.confluent.mds.request.logger log - 5 * Server has received a request on thread qtp1527668063-189null5 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull5 > User Principal: krp
5 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null5 > Cache-Control: no-cachenull5 > Connection: keep-alivenull5 > Content-Type: application/jsonnull5 > Host: kafka.confluent.svc.cluster.local:8090null5 > Pragma: no-cachenull5 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:21,109 [qtp1527668063-183] io.confluent.mds.request.logger log - 6 * Server has received a request on thread qtp1527668063-183null6 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull6 > User Principal: krp
6 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null6 > Cache-Control: no-cachenull6 > Connection: keep-alivenull6 > Content-Type: application/jsonnull6 > Host: kafka.confluent.svc.cluster.local:8090null6 > Pragma: no-cachenull6 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:21,110 [qtp1527668063-189] io.confluent.mds.request.logger log - 5 * Server responded with a response on thread qtp1527668063-189null5 < 200null5 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:21,112 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:21,112 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:21,112 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:21,114 [qtp1527668063-183] io.confluent.mds.request.logger log - 6 * Server responded with a response on thread qtp1527668063-183null6 < 200null6 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:21,115 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 634 "-" "Java/11.0.14.1" 16
[INFO] 2023-11-08 14:51:21,115 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 634 "-" "Java/11.0.14.1" 16
[INFO] 2023-11-08 14:51:21,115 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.1.6 - krp [08/Nov/2023:14:51:21 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 634 "-" "Java/11.0.14.1" 16
[DEBUG] 2023-11-08 14:51:21,350 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:21,351 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:51:22,584 [data-plane-kafka-request-handler-4] kafka.authorizer.logger logAuthorization - Principal = User:c3 is Denied Operation = Describe from host = 10.40.2.11 on resource = Topic:LITERAL:_confluent-command
[DEBUG] 2023-11-08 14:51:24,351 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:24,352 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:51:25,528 [data-plane-kafka-request-handler-5] kafka.authorizer.logger logAuthorization - Principal = User:ksql is Denied Operation = Describe from host = 10.40.1.9 on resource = Topic:LITERAL:_confluent-ksql-confluent.ksqldb__command_topic
[DEBUG] 2023-11-08 14:51:27,352 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:27,353 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:51:30,359 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:30,361 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:30,757 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 186]
[TRACE] 2023-11-08 14:51:30,757 [qtp1527668063-190] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 190]
[TRACE] 2023-11-08 14:51:30,760 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 186]
[TRACE] 2023-11-08 14:51:30,760 [qtp1527668063-190] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 190]
[DEBUG] 2023-11-08 14:51:30,762 [qtp1527668063-186] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:30,763 [qtp1527668063-190] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:30,764 [qtp1527668063-186] io.confluent.mds.request.logger log - 7 * Server has received a request on thread qtp1527668063-186null7 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull7 > User Principal: connect
7 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null7 > Cache-Control: no-cachenull7 > Connection: keep-alivenull7 > Content-Type: application/jsonnull7 > Host: kafka.confluent.svc.cluster.local:8090null7 > Pragma: no-cachenull7 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:30,765 [qtp1527668063-190] io.confluent.mds.request.logger log - 8 * Server has received a request on thread qtp1527668063-190null8 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull8 > User Principal: connect
8 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null8 > Cache-Control: no-cachenull8 > Connection: keep-alivenull8 > Content-Type: application/jsonnull8 > Host: kafka.confluent.svc.cluster.local:8090null8 > Pragma: no-cachenull8 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:30,766 [qtp1527668063-186] io.confluent.mds.request.logger log - 7 * Server responded with a response on thread qtp1527668063-186null7 < 200null7 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:30,767 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:30 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:30,767 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:30 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:30,767 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:30 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:30,770 [qtp1527668063-190] io.confluent.mds.request.logger log - 8 * Server responded with a response on thread qtp1527668063-190null8 < 200null8 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:30,771 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:30 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:30,771 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:30 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:30,772 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:30 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 15
[TRACE] 2023-11-08 14:51:31,775 [qtp1527668063-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 181]
[TRACE] 2023-11-08 14:51:31,775 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 183]
[TRACE] 2023-11-08 14:51:31,777 [qtp1527668063-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 181]
[TRACE] 2023-11-08 14:51:31,778 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 183]
[DEBUG] 2023-11-08 14:51:31,779 [qtp1527668063-181] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:31,780 [qtp1527668063-183] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:31,781 [qtp1527668063-181] io.confluent.mds.request.logger log - 9 * Server has received a request on thread qtp1527668063-181null9 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull9 > User Principal: connect
9 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null9 > Cache-Control: no-cachenull9 > Connection: keep-alivenull9 > Content-Type: application/jsonnull9 > Host: kafka.confluent.svc.cluster.local:8090null9 > Pragma: no-cachenull9 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:31,781 [qtp1527668063-183] io.confluent.mds.request.logger log - 10 * Server has received a request on thread qtp1527668063-183null10 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull10 > User Principal: connect
10 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null10 > Cache-Control: no-cachenull10 > Connection: keep-alivenull10 > Content-Type: application/jsonnull10 > Host: kafka.confluent.svc.cluster.local:8090null10 > Pragma: no-cachenull10 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:31,782 [qtp1527668063-181] io.confluent.mds.request.logger log - 9 * Server responded with a response on thread qtp1527668063-181null9 < 200null9 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:31,783 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:31,783 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:31,783 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:31,787 [qtp1527668063-183] io.confluent.mds.request.logger log - 10 * Server responded with a response on thread qtp1527668063-183null10 < 200null10 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:31,789 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:31,789 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:51:31,789 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[TRACE] 2023-11-08 14:51:31,899 [qtp1527668063-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 181]
[TRACE] 2023-11-08 14:51:31,899 [qtp1527668063-187] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 187]
[TRACE] 2023-11-08 14:51:31,901 [qtp1527668063-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 181]
[TRACE] 2023-11-08 14:51:31,902 [qtp1527668063-187] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 187]
[DEBUG] 2023-11-08 14:51:31,903 [qtp1527668063-181] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:31,904 [qtp1527668063-187] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:31,905 [qtp1527668063-181] io.confluent.mds.request.logger log - 11 * Server has received a request on thread qtp1527668063-181null11 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull11 > User Principal: connect
11 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null11 > Cache-Control: no-cachenull11 > Connection: keep-alivenull11 > Content-Type: application/jsonnull11 > Host: kafka.confluent.svc.cluster.local:8090null11 > Pragma: no-cachenull11 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:31,905 [qtp1527668063-187] io.confluent.mds.request.logger log - 12 * Server has received a request on thread qtp1527668063-187null12 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull12 > User Principal: connect
12 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null12 > Cache-Control: no-cachenull12 > Connection: keep-alivenull12 > Content-Type: application/jsonnull12 > Host: kafka.confluent.svc.cluster.local:8090null12 > Pragma: no-cachenull12 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:31,906 [qtp1527668063-181] io.confluent.mds.request.logger log - 11 * Server responded with a response on thread qtp1527668063-181null11 < 200null11 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:31,907 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:31,908 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:31,908 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:31,910 [qtp1527668063-187] io.confluent.mds.request.logger log - 12 * Server responded with a response on thread qtp1527668063-187null12 < 200null12 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:31,912 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:31,912 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:31,912 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:31 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[TRACE] 2023-11-08 14:51:32,013 [qtp1527668063-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 181]
[TRACE] 2023-11-08 14:51:32,013 [qtp1527668063-198] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 198]
[TRACE] 2023-11-08 14:51:32,015 [qtp1527668063-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 181]
[TRACE] 2023-11-08 14:51:32,016 [qtp1527668063-198] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 198]
[DEBUG] 2023-11-08 14:51:32,017 [qtp1527668063-181] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:32,018 [qtp1527668063-198] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:32,019 [qtp1527668063-198] io.confluent.mds.request.logger log - 14 * Server has received a request on thread qtp1527668063-198null14 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull14 > User Principal: connect
14 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null14 > Cache-Control: no-cachenull14 > Connection: keep-alivenull14 > Content-Type: application/jsonnull14 > Host: kafka.confluent.svc.cluster.local:8090null14 > Pragma: no-cachenull14 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,019 [qtp1527668063-181] io.confluent.mds.request.logger log - 13 * Server has received a request on thread qtp1527668063-181null13 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull13 > User Principal: connect
13 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null13 > Cache-Control: no-cachenull13 > Connection: keep-alivenull13 > Content-Type: application/jsonnull13 > Host: kafka.confluent.svc.cluster.local:8090null13 > Pragma: no-cachenull13 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,020 [qtp1527668063-198] io.confluent.mds.request.logger log - 14 * Server responded with a response on thread qtp1527668063-198null14 < 200null14 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:32,022 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:32,022 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:32,022 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:32,024 [qtp1527668063-181] io.confluent.mds.request.logger log - 13 * Server responded with a response on thread qtp1527668063-181null13 < 200null13 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:32,026 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:32,026 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:32,026 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[TRACE] 2023-11-08 14:51:32,111 [qtp1527668063-194] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 194]
[TRACE] 2023-11-08 14:51:32,114 [qtp1527668063-194] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 194]
[TRACE] 2023-11-08 14:51:32,114 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 183]
[DEBUG] 2023-11-08 14:51:32,117 [qtp1527668063-194] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[TRACE] 2023-11-08 14:51:32,118 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 183]
[INFO] 2023-11-08 14:51:32,119 [qtp1527668063-194] io.confluent.mds.request.logger log - 15 * Server has received a request on thread qtp1527668063-194null15 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull15 > User Principal: connect
15 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null15 > Cache-Control: no-cachenull15 > Connection: keep-alivenull15 > Content-Type: application/jsonnull15 > Host: kafka.confluent.svc.cluster.local:8090null15 > Pragma: no-cachenull15 > User-Agent: Java/11.0.14.1null
[DEBUG] 2023-11-08 14:51:32,120 [qtp1527668063-183] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:32,122 [qtp1527668063-183] io.confluent.mds.request.logger log - 16 * Server has received a request on thread qtp1527668063-183null16 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull16 > User Principal: connect
16 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null16 > Cache-Control: no-cachenull16 > Connection: keep-alivenull16 > Content-Type: application/jsonnull16 > Host: kafka.confluent.svc.cluster.local:8090null16 > Pragma: no-cachenull16 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,123 [qtp1527668063-183] io.confluent.mds.request.logger log - 16 * Server responded with a response on thread qtp1527668063-183null16 < 200null16 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:32,123 [qtp1527668063-194] io.confluent.mds.request.logger log - 15 * Server responded with a response on thread qtp1527668063-194null15 < 200null15 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:32,124 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:32,124 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:32,124 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:32,124 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:32,124 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:32,125 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[TRACE] 2023-11-08 14:51:32,197 [qtp1527668063-187] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 187]
[TRACE] 2023-11-08 14:51:32,197 [qtp1527668063-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 180]
[TRACE] 2023-11-08 14:51:32,198 [qtp1527668063-187] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 187]
[TRACE] 2023-11-08 14:51:32,199 [qtp1527668063-180] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 180]
[DEBUG] 2023-11-08 14:51:32,200 [qtp1527668063-187] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:32,201 [qtp1527668063-180] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:32,204 [qtp1527668063-187] io.confluent.mds.request.logger log - 17 * Server has received a request on thread qtp1527668063-187null17 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull17 > User Principal: connect
17 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null17 > Cache-Control: no-cachenull17 > Connection: keep-alivenull17 > Content-Type: application/jsonnull17 > Host: kafka.confluent.svc.cluster.local:8090null17 > Pragma: no-cachenull17 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,204 [qtp1527668063-180] io.confluent.mds.request.logger log - 18 * Server has received a request on thread qtp1527668063-180null18 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull18 > User Principal: connect
18 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null18 > Cache-Control: no-cachenull18 > Connection: keep-alivenull18 > Content-Type: application/jsonnull18 > Host: kafka.confluent.svc.cluster.local:8090null18 > Pragma: no-cachenull18 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,205 [qtp1527668063-180] io.confluent.mds.request.logger log - 18 * Server responded with a response on thread qtp1527668063-180null18 < 200null18 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:32,206 [qtp1527668063-180] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:32,206 [qtp1527668063-180] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:32,207 [qtp1527668063-180] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:32,207 [qtp1527668063-187] io.confluent.mds.request.logger log - 17 * Server responded with a response on thread qtp1527668063-187null17 < 200null17 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:32,209 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:32,209 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:32,209 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[TRACE] 2023-11-08 14:51:32,296 [qtp1527668063-190] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 190]
[TRACE] 2023-11-08 14:51:32,297 [qtp1527668063-198] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 198]
[TRACE] 2023-11-08 14:51:32,298 [qtp1527668063-190] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 190]
[TRACE] 2023-11-08 14:51:32,299 [qtp1527668063-198] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 198]
[DEBUG] 2023-11-08 14:51:32,300 [qtp1527668063-190] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:32,301 [qtp1527668063-198] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:32,301 [qtp1527668063-190] io.confluent.mds.request.logger log - 19 * Server has received a request on thread qtp1527668063-190null19 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull19 > User Principal: connect
19 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null19 > Cache-Control: no-cachenull19 > Connection: keep-alivenull19 > Content-Type: application/jsonnull19 > Host: kafka.confluent.svc.cluster.local:8090null19 > Pragma: no-cachenull19 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,302 [qtp1527668063-198] io.confluent.mds.request.logger log - 20 * Server has received a request on thread qtp1527668063-198null20 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull20 > User Principal: connect
20 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null20 > Cache-Control: no-cachenull20 > Connection: keep-alivenull20 > Content-Type: application/jsonnull20 > Host: kafka.confluent.svc.cluster.local:8090null20 > Pragma: no-cachenull20 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,303 [qtp1527668063-198] io.confluent.mds.request.logger log - 20 * Server responded with a response on thread qtp1527668063-198null20 < 200null20 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:32,304 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:51:32,304 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:51:32,304 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:51:32,305 [qtp1527668063-190] io.confluent.mds.request.logger log - 19 * Server responded with a response on thread qtp1527668063-190null19 < 200null19 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:32,306 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:32,306 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:32,306 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[TRACE] 2023-11-08 14:51:32,377 [qtp1527668063-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 181]
[TRACE] 2023-11-08 14:51:32,377 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 183]
[TRACE] 2023-11-08 14:51:32,379 [qtp1527668063-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 181]
[TRACE] 2023-11-08 14:51:32,380 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 183]
[DEBUG] 2023-11-08 14:51:32,381 [qtp1527668063-181] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:51:32,382 [qtp1527668063-183] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:32,383 [qtp1527668063-181] io.confluent.mds.request.logger log - 21 * Server has received a request on thread qtp1527668063-181null21 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull21 > User Principal: connect
21 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null21 > Cache-Control: no-cachenull21 > Connection: keep-alivenull21 > Content-Type: application/jsonnull21 > Host: kafka.confluent.svc.cluster.local:8090null21 > Pragma: no-cachenull21 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,383 [qtp1527668063-183] io.confluent.mds.request.logger log - 22 * Server has received a request on thread qtp1527668063-183null22 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull22 > User Principal: connect
22 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null22 > Cache-Control: no-cachenull22 > Connection: keep-alivenull22 > Content-Type: application/jsonnull22 > Host: kafka.confluent.svc.cluster.local:8090null22 > Pragma: no-cachenull22 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,384 [qtp1527668063-181] io.confluent.mds.request.logger log - 21 * Server responded with a response on thread qtp1527668063-181null21 < 200null21 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:32,384 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:51:32,385 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:32,385 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:32,386 [qtp1527668063-183] io.confluent.mds.request.logger log - 22 * Server responded with a response on thread qtp1527668063-183null22 < 200null22 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:32,387 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:32,387 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:51:32,387 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 10
[TRACE] 2023-11-08 14:51:32,480 [qtp1527668063-187] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 187]
[TRACE] 2023-11-08 14:51:32,481 [qtp1527668063-187] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 187]
[DEBUG] 2023-11-08 14:51:32,483 [qtp1527668063-187] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:32,485 [qtp1527668063-187] io.confluent.mds.request.logger log - 23 * Server has received a request on thread qtp1527668063-187null23 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull23 > User Principal: connect
23 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null23 > Cache-Control: no-cachenull23 > Connection: keep-alivenull23 > Content-Type: application/jsonnull23 > Host: kafka.confluent.svc.cluster.local:8090null23 > Pragma: no-cachenull23 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,486 [qtp1527668063-187] io.confluent.mds.request.logger log - 23 * Server responded with a response on thread qtp1527668063-187null23 < 200null23 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:32,487 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:32,487 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:32,487 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:32,565 [data-plane-kafka-request-handler-5] kafka.authorizer.logger logAuthorization - Principal = User:connect is Denied Operation = Describe from host = 10.40.2.10 on resource = Topic:LITERAL:confluent.connect-offsets
[TRACE] 2023-11-08 14:51:32,579 [qtp1527668063-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 192]
[TRACE] 2023-11-08 14:51:32,581 [qtp1527668063-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 192]
[DEBUG] 2023-11-08 14:51:32,583 [qtp1527668063-192] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:51:32,585 [qtp1527668063-192] io.confluent.mds.request.logger log - 24 * Server has received a request on thread qtp1527668063-192null24 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull24 > User Principal: connect
24 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null24 > Cache-Control: no-cachenull24 > Connection: keep-alivenull24 > Content-Type: application/jsonnull24 > Host: kafka.confluent.svc.cluster.local:8090null24 > Pragma: no-cachenull24 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:32,586 [qtp1527668063-192] io.confluent.mds.request.logger log - 24 * Server responded with a response on thread qtp1527668063-192null24 < 200null24 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:32,587 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:32,587 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:32,587 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.10 - connect [08/Nov/2023:14:51:32 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[DEBUG] 2023-11-08 14:51:33,359 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:33,361 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:51:36,359 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:36,360 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:51:36,973 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:36,987 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:36,988 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:36,988 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:36,988 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:36,988 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455096988
[INFO] 2023-11-08 14:51:37,168 [kafka-admin-client-thread | adminclient-2] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for adminclient-2 unregistered
[INFO] 2023-11-08 14:51:37,170 [kafka-admin-client-thread | adminclient-2] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:37,170 [kafka-admin-client-thread | adminclient-2] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:37,170 [kafka-admin-client-thread | adminclient-2] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:37,170 [confluent-metrics-reporter-scheduler] io.confluent.metrics.reporter.ConfluentMetricsReporter createTopicIfNotPresent - Attempted to create metrics reporter topic _confluent-metrics but the topic was already created. This may happen the first time ConfluentMetricsReporter is started and multiple brokers attempt to create the topic simultaneously.
[WARN] 2023-11-08 14:51:37,279 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.NetworkClient handleSuccessfulResponse - [Producer clientId=confluent-metrics-reporter] Error while fetching metadata with correlation id 3 : {_confluent-metrics=UNKNOWN_TOPIC_OR_PARTITION}
[WARN] 2023-11-08 14:51:37,781 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.NetworkClient handleSuccessfulResponse - [Producer clientId=confluent-metrics-reporter] Error while fetching metadata with correlation id 4 : {_confluent-metrics=UNKNOWN_TOPIC_OR_PARTITION}
[INFO] 2023-11-08 14:51:37,908 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 13 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:51:37,914 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-metrics-5, _confluent-metrics-8, _confluent-metrics-11, _confluent-metrics-2)
[INFO] 2023-11-08 14:51:37,914 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 13 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:51:37,920 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,920 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-5 in /mnt/data/data0/logs/_confluent-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,921 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-5 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-5
[INFO] 2023-11-08 14:51:37,921 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-5 broker=0] Log loaded for partition _confluent-metrics-5 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,921 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Leader _confluent-metrics-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,925 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,926 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-8 in /mnt/data/data0/logs/_confluent-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,926 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-8 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-8
[INFO] 2023-11-08 14:51:37,926 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-8 broker=0] Log loaded for partition _confluent-metrics-8 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,926 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Leader _confluent-metrics-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,930 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,930 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-11 in /mnt/data/data0/logs/_confluent-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,930 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-11 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-11
[INFO] 2023-11-08 14:51:37,930 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-11 broker=0] Log loaded for partition _confluent-metrics-11 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,930 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Leader _confluent-metrics-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,934 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,934 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-2 in /mnt/data/data0/logs/_confluent-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,934 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-2 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-2
[INFO] 2023-11-08 14:51:37,934 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-2 broker=0] Log loaded for partition _confluent-metrics-2 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,934 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Leader _confluent-metrics-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,938 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,938 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-4 in /mnt/data/data0/logs/_confluent-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,938 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-4 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-4
[INFO] 2023-11-08 14:51:37,938 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-4 broker=0] Log loaded for partition _confluent-metrics-4 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,938 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-metrics-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,942 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,942 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-7 in /mnt/data/data0/logs/_confluent-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,942 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-7 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-7
[INFO] 2023-11-08 14:51:37,943 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-7 broker=0] Log loaded for partition _confluent-metrics-7 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,943 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-metrics-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,946 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,947 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-6 in /mnt/data/data0/logs/_confluent-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,947 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-6 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-6
[INFO] 2023-11-08 14:51:37,947 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-6 broker=0] Log loaded for partition _confluent-metrics-6 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,947 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-metrics-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,951 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,952 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-9 in /mnt/data/data0/logs/_confluent-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,952 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-9 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-9
[INFO] 2023-11-08 14:51:37,952 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-9 broker=0] Log loaded for partition _confluent-metrics-9 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,952 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-metrics-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,955 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,956 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-10 in /mnt/data/data0/logs/_confluent-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,956 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-10 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-10
[INFO] 2023-11-08 14:51:37,956 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-10 broker=0] Log loaded for partition _confluent-metrics-10 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,956 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-metrics-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,959 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,960 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-1 in /mnt/data/data0/logs/_confluent-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,960 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-1 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-1
[INFO] 2023-11-08 14:51:37,960 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-1 broker=0] Log loaded for partition _confluent-metrics-1 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,960 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-metrics-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,964 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,964 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-0 in /mnt/data/data0/logs/_confluent-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,965 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-0 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-0
[INFO] 2023-11-08 14:51:37,965 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-0 broker=0] Log loaded for partition _confluent-metrics-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,965 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-metrics-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,968 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-metrics-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:51:37,969 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-metrics-3 in /mnt/data/data0/logs/_confluent-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:51:37,969 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-3 broker=0] No checkpointed highwatermark is found for partition _confluent-metrics-3
[INFO] 2023-11-08 14:51:37,969 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-metrics-3 broker=0] Log loaded for partition _confluent-metrics-3 with initial high watermark 0
[INFO] 2023-11-08 14:51:37,969 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-metrics-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:37,969 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-metrics-9, _confluent-metrics-10, _confluent-metrics-7, _confluent-metrics-6, _confluent-metrics-3, _confluent-metrics-4, _confluent-metrics-1, _confluent-metrics-0)
[INFO] 2023-11-08 14:51:37,970 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 13 for 8 partitions
[INFO] 2023-11-08 14:51:37,970 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-metrics-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:37,971 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-metrics-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-metrics-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:51:37,971 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 63ms correlationId 13 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:51:37,973 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 14
[INFO] 2023-11-08 14:51:38,172 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-metrics-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,173 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,173 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-metrics-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,173 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,173 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-metrics-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,173 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,173 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-metrics-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,173 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,272 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-metrics-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,272 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-metrics-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-metrics-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-metrics-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:51:38,273 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-metrics-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[TRACE] 2023-11-08 14:51:38,568 [qtp1527668063-193] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 193]
[TRACE] 2023-11-08 14:51:38,568 [qtp1527668063-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 192]
[TRACE] 2023-11-08 14:51:38,570 [qtp1527668063-193] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=sr,dc=test,dc=com [Thread 193]
[TRACE] 2023-11-08 14:51:38,570 [qtp1527668063-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=sr,dc=test,dc=com [Thread 192]
[DEBUG] 2023-11-08 14:51:38,571 [qtp1527668063-193] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for sr
[DEBUG] 2023-11-08 14:51:38,572 [qtp1527668063-192] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for sr
[INFO] 2023-11-08 14:51:38,573 [qtp1527668063-193] io.confluent.mds.request.logger log - 25 * Server has received a request on thread qtp1527668063-193null25 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull25 > User Principal: sr
25 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null25 > Cache-Control: no-cachenull25 > Connection: keep-alivenull25 > Content-Type: application/jsonnull25 > Host: kafka.confluent.svc.cluster.local:8090null25 > Pragma: no-cachenull25 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:38,573 [qtp1527668063-192] io.confluent.mds.request.logger log - 26 * Server has received a request on thread qtp1527668063-192null26 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull26 > User Principal: sr
26 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null26 > Cache-Control: no-cachenull26 > Connection: keep-alivenull26 > Content-Type: application/jsonnull26 > Host: kafka.confluent.svc.cluster.local:8090null26 > Pragma: no-cachenull26 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:38,574 [qtp1527668063-192] io.confluent.mds.request.logger log - 26 * Server responded with a response on thread qtp1527668063-192null26 < 200null26 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:38,575 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:38 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:38,575 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:38 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:38,575 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:38 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:38,577 [qtp1527668063-193] io.confluent.mds.request.logger log - 25 * Server responded with a response on thread qtp1527668063-193null25 < 200null25 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:38,578 [qtp1527668063-193] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:38 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:51:38,578 [qtp1527668063-193] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:38 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:51:38,578 [qtp1527668063-193] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:38 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 11
[DEBUG] 2023-11-08 14:51:39,359 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:39,361 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:39,371 [qtp1527668063-199] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 199]
[TRACE] 2023-11-08 14:51:39,372 [qtp1527668063-253] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 253]
[TRACE] 2023-11-08 14:51:39,373 [qtp1527668063-199] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=sr,dc=test,dc=com [Thread 199]
[TRACE] 2023-11-08 14:51:39,374 [qtp1527668063-253] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user sr with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=sr,dc=test,dc=com [Thread 253]
[DEBUG] 2023-11-08 14:51:39,375 [qtp1527668063-199] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for sr
[DEBUG] 2023-11-08 14:51:39,376 [qtp1527668063-253] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for sr
[INFO] 2023-11-08 14:51:39,377 [qtp1527668063-253] io.confluent.mds.request.logger log - 27 * Server has received a request on thread qtp1527668063-253null27 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull27 > User Principal: sr
27 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null27 > Cache-Control: no-cachenull27 > Connection: keep-alivenull27 > Content-Type: application/jsonnull27 > Host: kafka.confluent.svc.cluster.local:8090null27 > Pragma: no-cachenull27 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:39,377 [qtp1527668063-199] io.confluent.mds.request.logger log - 28 * Server has received a request on thread qtp1527668063-199null28 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull28 > User Principal: sr
28 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null28 > Cache-Control: no-cachenull28 > Connection: keep-alivenull28 > Content-Type: application/jsonnull28 > Host: kafka.confluent.svc.cluster.local:8090null28 > Pragma: no-cachenull28 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:51:39,379 [qtp1527668063-253] io.confluent.mds.request.logger log - 27 * Server responded with a response on thread qtp1527668063-253null27 < 200null27 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:51:39,379 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:39 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:51:39,380 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:39 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:39,380 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:39 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:51:39,383 [qtp1527668063-199] io.confluent.mds.request.logger log - 28 * Server responded with a response on thread qtp1527668063-199null28 < 200null28 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:51:39,384 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:39 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:39,384 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:39 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:51:39,384 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.9 - sr [08/Nov/2023:14:51:39 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 13
[DEBUG] 2023-11-08 14:51:42,359 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:42,361 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:44,728 [qtp1527668063-249] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 249]
[TRACE] 2023-11-08 14:51:44,729 [qtp1527668063-249] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 249]
[DEBUG] 2023-11-08 14:51:44,731 [qtp1527668063-249] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:44,734 [qtp1527668063-249] io.confluent.mds.request.logger log - 29 * Server has received a request on thread qtp1527668063-249null29 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:c3/roles/SystemAdminnull29 > User Principal: kafka
29 > Accept: application/jsonnull29 > Content-Length: 56null29 > Content-Type: application/jsonnull29 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null29 > User-Agent: confluent-operatornull29 > Via: 1.1 kafka-2null29 > X-Forwarded-For: 10.40.2.7null29 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null29 > X-Forwarded-Proto: httpsnull29 > X-Forwarded-Server: 10.40.0.15null{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}}


[DEBUG] 2023-11-08 14:51:44,902 [qtp1527668063-249] io.confluent.security.auth.store.kafka.KafkaAuthWriter addClusterRoleBinding - addClusterRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:c3 role=SystemAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') reason=null
[DEBUG] 2023-11-08 14:51:44,902 [qtp1527668063-249] io.confluent.security.auth.store.kafka.KafkaAuthWriter replaceResourceRoleBinding - replaceResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:c3 role=SystemAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') resources=[] reason=null
[DEBUG] 2023-11-08 14:51:44,904 [qtp1527668063-249] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-5]Writing new record with key RoleBindingKey{principal=User:c3, role='SystemAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} to partition _confluent-metadata-auth-5 generation id 1
[DEBUG] 2023-11-08 14:51:44,916 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-5]Pending write completed metadata=_confluent-metadata-auth-5@3 exception=null
[DEBUG] 2023-11-08 14:51:44,916 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Send callback for record with partition _confluent-metadata-auth-5 generationId 1 offset 3
[DEBUG] 2023-11-08 14:51:44,926 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:3 key RoleBindingKey{principal=User:c3, role='SystemAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:51:44,927 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-5]Completing pending write since offset 3 has been consumed
[INFO] 2023-11-08 14:51:44,927 [qtp1527668063-249] io.confluent.mds.request.logger log - 29 * Server responded with a response on thread qtp1527668063-249null29 < 204null
[INFO] 2023-11-08 14:51:44,929 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:44 +0000] "POST /security/1.0/principals/User:c3/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 202
[INFO] 2023-11-08 14:51:44,929 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:44 +0000] "POST /security/1.0/principals/User:c3/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 202
[INFO] 2023-11-08 14:51:44,929 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:44 +0000] "POST /security/1.0/principals/User:c3/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 202
[TRACE] 2023-11-08 14:51:45,021 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 186]
[TRACE] 2023-11-08 14:51:45,022 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 186]
[DEBUG] 2023-11-08 14:51:45,024 [qtp1527668063-186] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:45,026 [qtp1527668063-186] io.confluent.mds.request.logger log - 30 * Server has received a request on thread qtp1527668063-186null30 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:ksql/roles/ResourceOwner/bindingsnull30 > User Principal: kafka
30 > Accept: application/jsonnull30 > Content-Length: 199null30 > Content-Type: application/jsonnull30 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null30 > User-Agent: confluent-operatornull30 > Via: 1.1 kafka-2null30 > X-Forwarded-For: 10.40.2.7null30 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null30 > X-Forwarded-Proto: httpsnull30 > X-Forwarded-Server: 10.40.0.15null{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","ksql-cluster":"confluent.ksqldb_"}},"resourcePatterns":[{"resourceType":"KsqlCluster","name":"ksql-cluster","patternType":"LITERAL"}]}


[DEBUG] 2023-11-08 14:51:45,049 [qtp1527668063-186] io.confluent.security.auth.store.kafka.KafkaAuthWriter addResourceRoleBinding - addResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:ksql role=ResourceOwner scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, ksql-cluster=confluent.ksqldb_}') resources=[KsqlCluster:LITERAL:ksql-cluster] reason=null
[DEBUG] 2023-11-08 14:51:45,050 [qtp1527668063-186] io.confluent.security.auth.store.kafka.KafkaAuthWriter lambda$addResourceRoleBinding$6 - New binding User:ksql ResourceOwner Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, ksql-cluster=confluent.ksqldb_}') [KsqlCluster:LITERAL:ksql-cluster]
[DEBUG] 2023-11-08 14:51:45,050 [qtp1527668063-186] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-5]Writing new record with key RoleBindingKey{principal=User:ksql, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, ksql-cluster=confluent.ksqldb_}')'} to partition _confluent-metadata-auth-5 generation id 1
[DEBUG] 2023-11-08 14:51:45,059 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:4 key RoleBindingKey{principal=User:ksql, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, ksql-cluster=confluent.ksqldb_}')'} newValue RoleBindingValue(resources=[KsqlCluster:LITERAL:ksql-cluster]) oldValue null
[DEBUG] 2023-11-08 14:51:45,060 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-5]Pending write completed metadata=_confluent-metadata-auth-5@4 exception=null
[DEBUG] 2023-11-08 14:51:45,060 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Send callback for record with partition _confluent-metadata-auth-5 generationId 1 offset 4
[DEBUG] 2023-11-08 14:51:45,060 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-5]Completing pending write since offset 4 has been consumed
[INFO] 2023-11-08 14:51:45,060 [qtp1527668063-186] io.confluent.mds.request.logger log - 30 * Server responded with a response on thread qtp1527668063-186null30 < 204null
[INFO] 2023-11-08 14:51:45,061 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 42
[INFO] 2023-11-08 14:51:45,061 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 42
[INFO] 2023-11-08 14:51:45,062 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 43
[TRACE] 2023-11-08 14:51:45,210 [qtp1527668063-241] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 241]
[TRACE] 2023-11-08 14:51:45,212 [qtp1527668063-241] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 241]
[DEBUG] 2023-11-08 14:51:45,214 [qtp1527668063-241] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:45,216 [qtp1527668063-241] io.confluent.mds.request.logger log - 31 * Server has received a request on thread qtp1527668063-241null31 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:ksql/roles/ResourceOwner/bindingsnull31 > User Principal: kafka
31 > Accept: application/jsonnull31 > Content-Length: 381null31 > Content-Type: application/jsonnull31 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null31 > User-Agent: confluent-operatornull31 > Via: 1.1 kafka-1null31 > X-Forwarded-For: 10.40.2.7null31 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null31 > X-Forwarded-Proto: httpsnull31 > X-Forwarded-Server: 10.40.1.7null{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourcePatterns":[{"resourceType":"Topic","name":"_confluent-ksql-confluent.ksqldb__command_topic","patternType":"LITERAL"},{"resourceType":"Topic","name":"confluent.ksqldb_ksql_processing_log","patternType":"LITERAL"},{"resourceType":"Group","name":"_confluent-ksql-confluent.ksqldb_","patternType":"PREFIXED"}]}


[DEBUG] 2023-11-08 14:51:45,222 [qtp1527668063-241] io.confluent.security.auth.store.kafka.KafkaAuthWriter addResourceRoleBinding - addResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:ksql role=ResourceOwner scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') resources=[Topic:LITERAL:_confluent-ksql-confluent.ksqldb__command_topic, Topic:LITERAL:confluent.ksqldb_ksql_processing_log, Group:PREFIXED:_confluent-ksql-confluent.ksqldb_] reason=null
[DEBUG] 2023-11-08 14:51:45,223 [qtp1527668063-241] io.confluent.security.auth.store.kafka.KafkaAuthWriter lambda$addResourceRoleBinding$6 - New binding User:ksql ResourceOwner Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') [Topic:LITERAL:_confluent-ksql-confluent.ksqldb__command_topic, Group:PREFIXED:_confluent-ksql-confluent.ksqldb_, Topic:LITERAL:confluent.ksqldb_ksql_processing_log]
[DEBUG] 2023-11-08 14:51:45,223 [qtp1527668063-241] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-5]Writing new record with key RoleBindingKey{principal=User:ksql, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} to partition _confluent-metadata-auth-5 generation id 1
[DEBUG] 2023-11-08 14:51:45,237 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:5 key RoleBindingKey{principal=User:ksql, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:LITERAL:_confluent-ksql-confluent.ksqldb__command_topic, Group:PREFIXED:_confluent-ksql-confluent.ksqldb_, Topic:LITERAL:confluent.ksqldb_ksql_processing_log]) oldValue null
[DEBUG] 2023-11-08 14:51:45,243 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-5]Pending write completed metadata=_confluent-metadata-auth-5@5 exception=null
[DEBUG] 2023-11-08 14:51:45,243 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Send callback for record with partition _confluent-metadata-auth-5 generationId 1 offset 5
[DEBUG] 2023-11-08 14:51:45,244 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-5]Completing pending write since offset 5 has been consumed
[INFO] 2023-11-08 14:51:45,244 [qtp1527668063-241] io.confluent.mds.request.logger log - 31 * Server responded with a response on thread qtp1527668063-241null31 < 204null
[INFO] 2023-11-08 14:51:45,245 [qtp1527668063-241] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 35
[INFO] 2023-11-08 14:51:45,245 [qtp1527668063-241] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 35
[INFO] 2023-11-08 14:51:45,245 [qtp1527668063-241] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 35
[TRACE] 2023-11-08 14:51:45,336 [qtp1527668063-259] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 259]
[TRACE] 2023-11-08 14:51:45,338 [qtp1527668063-259] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 259]
[DEBUG] 2023-11-08 14:51:45,339 [qtp1527668063-259] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:45,341 [qtp1527668063-259] io.confluent.mds.request.logger log - 32 * Server has received a request on thread qtp1527668063-259null32 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:ksql/roles/DeveloperWrite/bindingsnull32 > User Principal: kafka
32 > Accept: application/jsonnull32 > Content-Length: 248null32 > Content-Type: application/jsonnull32 > Host: kafka.confluent.svc.cluster.local:8090null32 > User-Agent: confluent-operatornull{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourcePatterns":[{"resourceType":"Cluster","name":"kafka-cluster","patternType":"LITERAL"},{"resourceType":"TransactionalId","name":"confluent.ksqldb_","patternType":"PREFIXED"}]}


[DEBUG] 2023-11-08 14:51:45,345 [qtp1527668063-259] io.confluent.security.auth.store.kafka.KafkaAuthWriter addResourceRoleBinding - addResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:ksql role=DeveloperWrite scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') resources=[Cluster:LITERAL:kafka-cluster, TransactionalId:PREFIXED:confluent.ksqldb_] reason=null
[DEBUG] 2023-11-08 14:51:45,346 [qtp1527668063-259] io.confluent.security.auth.store.kafka.KafkaAuthWriter lambda$addResourceRoleBinding$6 - New binding User:ksql DeveloperWrite Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') [Cluster:LITERAL:kafka-cluster, TransactionalId:PREFIXED:confluent.ksqldb_]
[DEBUG] 2023-11-08 14:51:45,346 [qtp1527668063-259] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-3]Writing new record with key RoleBindingKey{principal=User:ksql, role='DeveloperWrite', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} to partition _confluent-metadata-auth-3 generation id 1
[DEBUG] 2023-11-08 14:51:45,354 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:3 key RoleBindingKey{principal=User:ksql, role='DeveloperWrite', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Cluster:LITERAL:kafka-cluster, TransactionalId:PREFIXED:confluent.ksqldb_]) oldValue null
[DEBUG] 2023-11-08 14:51:45,354 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-3]Pending write completed metadata=_confluent-metadata-auth-3@3 exception=null
[DEBUG] 2023-11-08 14:51:45,354 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-3]Send callback for record with partition _confluent-metadata-auth-3 generationId 1 offset 3
[DEBUG] 2023-11-08 14:51:45,355 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-3]Completing pending write since offset 3 has been consumed
[INFO] 2023-11-08 14:51:45,355 [qtp1527668063-259] io.confluent.mds.request.logger log - 32 * Server responded with a response on thread qtp1527668063-259null32 < 204null
[INFO] 2023-11-08 14:51:45,356 [qtp1527668063-259] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 20
[INFO] 2023-11-08 14:51:45,356 [qtp1527668063-259] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 20
[INFO] 2023-11-08 14:51:45,356 [qtp1527668063-259] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:ksql/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 20
[DEBUG] 2023-11-08 14:51:45,359 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:45,360 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:51:45,937 [qtp1527668063-190] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 190]
[TRACE] 2023-11-08 14:51:45,939 [qtp1527668063-190] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 190]
[DEBUG] 2023-11-08 14:51:45,955 [qtp1527668063-190] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:45,956 [qtp1527668063-190] io.confluent.mds.request.logger log - 33 * Server has received a request on thread qtp1527668063-190null33 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:sr/roles/SecurityAdminnull33 > User Principal: kafka
33 > Accept: application/jsonnull33 > Content-Length: 112null33 > Content-Type: application/jsonnull33 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null33 > User-Agent: confluent-operatornull33 > Via: 1.1 kafka-2null33 > X-Forwarded-For: 10.40.2.7null33 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null33 > X-Forwarded-Proto: httpsnull33 > X-Forwarded-Server: 10.40.0.15null{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","schema-registry-cluster":"id_schemaregistry_confluent"}}


[DEBUG] 2023-11-08 14:51:45,962 [qtp1527668063-190] io.confluent.security.auth.store.kafka.KafkaAuthWriter addClusterRoleBinding - addClusterRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:sr role=SecurityAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, schema-registry-cluster=id_schemaregistry_confluent}') reason=null
[DEBUG] 2023-11-08 14:51:45,962 [qtp1527668063-190] io.confluent.security.auth.store.kafka.KafkaAuthWriter replaceResourceRoleBinding - replaceResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:sr role=SecurityAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, schema-registry-cluster=id_schemaregistry_confluent}') resources=[] reason=null
[DEBUG] 2023-11-08 14:51:45,963 [qtp1527668063-190] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-4]Writing new record with key RoleBindingKey{principal=User:sr, role='SecurityAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, schema-registry-cluster=id_schemaregistry_confluent}')'} to partition _confluent-metadata-auth-4 generation id 1
[DEBUG] 2023-11-08 14:51:45,970 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:3 key RoleBindingKey{principal=User:sr, role='SecurityAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, schema-registry-cluster=id_schemaregistry_confluent}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:51:45,970 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-4]Pending write completed metadata=_confluent-metadata-auth-4@3 exception=null
[DEBUG] 2023-11-08 14:51:45,970 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-4]Send callback for record with partition _confluent-metadata-auth-4 generationId 1 offset 3
[DEBUG] 2023-11-08 14:51:45,971 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-4]Completing pending write since offset 3 has been consumed
[INFO] 2023-11-08 14:51:45,971 [qtp1527668063-190] io.confluent.mds.request.logger log - 33 * Server responded with a response on thread qtp1527668063-190null33 < 204null
[INFO] 2023-11-08 14:51:45,972 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:sr/roles/SecurityAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 35
[INFO] 2023-11-08 14:51:45,972 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:sr/roles/SecurityAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 35
[INFO] 2023-11-08 14:51:45,972 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:45 +0000] "POST /security/1.0/principals/User:sr/roles/SecurityAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 35
[TRACE] 2023-11-08 14:51:46,228 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 186]
[TRACE] 2023-11-08 14:51:46,229 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 186]
[DEBUG] 2023-11-08 14:51:46,233 [qtp1527668063-186] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:46,234 [qtp1527668063-186] io.confluent.mds.request.logger log - 34 * Server has received a request on thread qtp1527668063-186null34 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:sr/roles/ResourceOwner/bindingsnull34 > User Principal: kafka
34 > Accept: application/jsonnull34 > Content-Length: 570null34 > Content-Type: application/jsonnull34 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null34 > User-Agent: confluent-operatornull34 > Via: 1.1 kafka-1null34 > X-Forwarded-For: 10.40.2.7null34 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null34 > X-Forwarded-Proto: httpsnull34 > X-Forwarded-Server: 10.40.1.7null{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourcePatterns":[{"resourceType":"Topic","name":"_confluent-license","patternType":"LITERAL"},{"resourceType":"Topic","name":"_confluent-command","patternType":"LITERAL"},{"resourceType":"Topic","name":"_exporter_configs","patternType":"LITERAL"},{"resourceType":"Topic","name":"_exporter_states","patternType":"LITERAL"},{"resourceType":"Topic","name":"_schemas_schemaregistry_confluent","patternType":"LITERAL"},{"resourceType":"Group","name":"id_schemaregistry_confluent","patternType":"LITERAL"}]}


[DEBUG] 2023-11-08 14:51:46,243 [qtp1527668063-186] io.confluent.security.auth.store.kafka.KafkaAuthWriter addResourceRoleBinding - addResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:sr role=ResourceOwner scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') resources=[Topic:LITERAL:_confluent-license, Topic:LITERAL:_confluent-command, Topic:LITERAL:_exporter_configs, Topic:LITERAL:_exporter_states, Topic:LITERAL:_schemas_schemaregistry_confluent, Group:LITERAL:id_schemaregistry_confluent] reason=null
[DEBUG] 2023-11-08 14:51:46,243 [qtp1527668063-186] io.confluent.security.auth.store.kafka.KafkaAuthWriter lambda$addResourceRoleBinding$6 - New binding User:sr ResourceOwner Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') [Topic:LITERAL:_confluent-license, Topic:LITERAL:_confluent-command, Group:LITERAL:id_schemaregistry_confluent, Topic:LITERAL:_exporter_states, Topic:LITERAL:_schemas_schemaregistry_confluent, Topic:LITERAL:_exporter_configs]
[DEBUG] 2023-11-08 14:51:46,243 [qtp1527668063-186] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-3]Writing new record with key RoleBindingKey{principal=User:sr, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} to partition _confluent-metadata-auth-3 generation id 1
[DEBUG] 2023-11-08 14:51:46,250 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:4 key RoleBindingKey{principal=User:sr, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:LITERAL:_confluent-license, Topic:LITERAL:_confluent-command, Group:LITERAL:id_schemaregistry_confluent, Topic:LITERAL:_exporter_states, Topic:LITERAL:_schemas_schemaregistry_confluent, Topic:LITERAL:_exporter_configs]) oldValue null
[DEBUG] 2023-11-08 14:51:46,251 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-3]Pending write completed metadata=_confluent-metadata-auth-3@4 exception=null
[DEBUG] 2023-11-08 14:51:46,251 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-3]Send callback for record with partition _confluent-metadata-auth-3 generationId 1 offset 4
[DEBUG] 2023-11-08 14:51:46,251 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-3]Completing pending write since offset 4 has been consumed
[INFO] 2023-11-08 14:51:46,251 [qtp1527668063-186] io.confluent.mds.request.logger log - 34 * Server responded with a response on thread qtp1527668063-186null34 < 204null
[INFO] 2023-11-08 14:51:46,252 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:sr/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 25
[INFO] 2023-11-08 14:51:46,252 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:sr/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 25
[INFO] 2023-11-08 14:51:46,252 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:sr/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 25
[TRACE] 2023-11-08 14:51:46,424 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 186]
[TRACE] 2023-11-08 14:51:46,426 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 186]
[DEBUG] 2023-11-08 14:51:46,428 [qtp1527668063-186] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:46,429 [qtp1527668063-186] io.confluent.mds.request.logger log - 35 * Server has received a request on thread qtp1527668063-186null35 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:connect/roles/SecurityAdminnull35 > User Principal: kafka
35 > Accept: application/jsonnull35 > Content-Length: 94null35 > Content-Type: application/jsonnull35 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null35 > User-Agent: confluent-operatornull35 > Via: 1.1 kafka-1null35 > X-Forwarded-For: 10.40.2.7null35 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null35 > X-Forwarded-Proto: httpsnull35 > X-Forwarded-Server: 10.40.1.7null{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-cluster":"confluent.connect"}}


[DEBUG] 2023-11-08 14:51:46,432 [qtp1527668063-186] io.confluent.security.auth.store.kafka.KafkaAuthWriter addClusterRoleBinding - addClusterRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:connect role=SecurityAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, connect-cluster=confluent.connect}') reason=null
[DEBUG] 2023-11-08 14:51:46,432 [qtp1527668063-186] io.confluent.security.auth.store.kafka.KafkaAuthWriter replaceResourceRoleBinding - replaceResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:connect role=SecurityAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, connect-cluster=confluent.connect}') resources=[] reason=null
[DEBUG] 2023-11-08 14:51:46,432 [qtp1527668063-186] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-1]Writing new record with key RoleBindingKey{principal=User:connect, role='SecurityAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, connect-cluster=confluent.connect}')'} to partition _confluent-metadata-auth-1 generation id 1
[DEBUG] 2023-11-08 14:51:46,441 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-1]Pending write completed metadata=_confluent-metadata-auth-1@3 exception=null
[DEBUG] 2023-11-08 14:51:46,441 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Send callback for record with partition _confluent-metadata-auth-1 generationId 1 offset 3
[DEBUG] 2023-11-08 14:51:46,441 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:3 key RoleBindingKey{principal=User:connect, role='SecurityAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, connect-cluster=confluent.connect}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:51:46,442 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-1]Completing pending write since offset 3 has been consumed
[INFO] 2023-11-08 14:51:46,442 [qtp1527668063-186] io.confluent.mds.request.logger log - 35 * Server responded with a response on thread qtp1527668063-186null35 < 204null
[INFO] 2023-11-08 14:51:46,443 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/SecurityAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 19
[INFO] 2023-11-08 14:51:46,443 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/SecurityAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 19
[INFO] 2023-11-08 14:51:46,443 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/SecurityAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 19
[TRACE] 2023-11-08 14:51:46,624 [qtp1527668063-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 181]
[TRACE] 2023-11-08 14:51:46,625 [qtp1527668063-181] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 181]
[DEBUG] 2023-11-08 14:51:46,627 [qtp1527668063-181] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:46,629 [qtp1527668063-181] io.confluent.mds.request.logger log - 36 * Server has received a request on thread qtp1527668063-181null36 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:connect/roles/ResourceOwner/bindingsnull36 > User Principal: kafka
36 > Accept: application/jsonnull36 > Content-Length: 241null36 > Content-Type: application/jsonnull36 > Host: kafka.confluent.svc.cluster.local:8090null36 > User-Agent: confluent-operatornull{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourcePatterns":[{"resourceType":"Group","name":"confluent.connect","patternType":"LITERAL"},{"resourceType":"Topic","name":"confluent.connect-","patternType":"PREFIXED"}]}


[DEBUG] 2023-11-08 14:51:46,634 [qtp1527668063-181] io.confluent.security.auth.store.kafka.KafkaAuthWriter addResourceRoleBinding - addResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:connect role=ResourceOwner scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') resources=[Group:LITERAL:confluent.connect, Topic:PREFIXED:confluent.connect-] reason=null
[DEBUG] 2023-11-08 14:51:46,634 [qtp1527668063-181] io.confluent.security.auth.store.kafka.KafkaAuthWriter lambda$addResourceRoleBinding$6 - New binding User:connect ResourceOwner Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') [Topic:PREFIXED:confluent.connect-, Group:LITERAL:confluent.connect]
[DEBUG] 2023-11-08 14:51:46,635 [qtp1527668063-181] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-4]Writing new record with key RoleBindingKey{principal=User:connect, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} to partition _confluent-metadata-auth-4 generation id 1
[DEBUG] 2023-11-08 14:51:46,641 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-4]Pending write completed metadata=_confluent-metadata-auth-4@4 exception=null
[DEBUG] 2023-11-08 14:51:46,641 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-4]Send callback for record with partition _confluent-metadata-auth-4 generationId 1 offset 4
[DEBUG] 2023-11-08 14:51:46,642 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:4 key RoleBindingKey{principal=User:connect, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:PREFIXED:confluent.connect-, Group:LITERAL:confluent.connect]) oldValue null
[DEBUG] 2023-11-08 14:51:46,642 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-4]Completing pending write since offset 4 has been consumed
[INFO] 2023-11-08 14:51:46,642 [qtp1527668063-181] io.confluent.mds.request.logger log - 36 * Server responded with a response on thread qtp1527668063-181null36 < 204null
[INFO] 2023-11-08 14:51:46,643 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 20
[INFO] 2023-11-08 14:51:46,643 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 20
[INFO] 2023-11-08 14:51:46,643 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 20
[TRACE] 2023-11-08 14:51:46,730 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 186]
[TRACE] 2023-11-08 14:51:46,731 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 186]
[DEBUG] 2023-11-08 14:51:46,733 [qtp1527668063-186] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:46,735 [qtp1527668063-186] io.confluent.mds.request.logger log - 37 * Server has received a request on thread qtp1527668063-186null37 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:connect/roles/DeveloperWrite/bindingsnull37 > User Principal: kafka
37 > Accept: application/jsonnull37 > Content-Length: 168null37 > Content-Type: application/jsonnull37 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null37 > User-Agent: confluent-operatornull37 > Via: 1.1 kafka-2null37 > X-Forwarded-For: 10.40.2.7null37 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null37 > X-Forwarded-Proto: httpsnull37 > X-Forwarded-Server: 10.40.0.15null{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourcePatterns":[{"resourceType":"Topic","name":"_confluent-monitoring","patternType":"PREFIXED"}]}


[DEBUG] 2023-11-08 14:51:46,738 [qtp1527668063-186] io.confluent.security.auth.store.kafka.KafkaAuthWriter addResourceRoleBinding - addResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:connect role=DeveloperWrite scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') resources=[Topic:PREFIXED:_confluent-monitoring] reason=null
[DEBUG] 2023-11-08 14:51:46,738 [qtp1527668063-186] io.confluent.security.auth.store.kafka.KafkaAuthWriter lambda$addResourceRoleBinding$6 - New binding User:connect DeveloperWrite Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') [Topic:PREFIXED:_confluent-monitoring]
[DEBUG] 2023-11-08 14:51:46,738 [qtp1527668063-186] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-2]Writing new record with key RoleBindingKey{principal=User:connect, role='DeveloperWrite', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} to partition _confluent-metadata-auth-2 generation id 1
[DEBUG] 2023-11-08 14:51:46,747 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:3 key RoleBindingKey{principal=User:connect, role='DeveloperWrite', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:PREFIXED:_confluent-monitoring]) oldValue null
[DEBUG] 2023-11-08 14:51:46,749 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-2]Pending write completed metadata=_confluent-metadata-auth-2@3 exception=null
[DEBUG] 2023-11-08 14:51:46,749 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-2]Send callback for record with partition _confluent-metadata-auth-2 generationId 1 offset 3
[DEBUG] 2023-11-08 14:51:46,750 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-2]Completing pending write since offset 3 has been consumed
[INFO] 2023-11-08 14:51:46,750 [qtp1527668063-186] io.confluent.mds.request.logger log - 37 * Server responded with a response on thread qtp1527668063-186null37 < 204null
[INFO] 2023-11-08 14:51:46,751 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 22
[INFO] 2023-11-08 14:51:46,751 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 22
[INFO] 2023-11-08 14:51:46,751 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:46 +0000] "POST /security/1.0/principals/User:connect/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 22
[DEBUG] 2023-11-08 14:51:48,360 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:48,361 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:51:48,460 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:48,462 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,462 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:48,462 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:48,462 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:48,462 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455108462
[INFO] 2023-11-08 14:51:48,489 [kafka-admin-client-thread | adminclient-3] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for adminclient-3 unregistered
[INFO] 2023-11-08 14:51:48,491 [kafka-admin-client-thread | adminclient-3] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:48,491 [kafka-admin-client-thread | adminclient-3] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:48,491 [kafka-admin-client-thread | adminclient-3] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:48,493 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	client.dns.lookup = use_all_dns_ips
	client.id = 
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:51:48,495 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,495 [confluent-metrics-reporter-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:48,495 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:48,495 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:48,495 [confluent-metrics-reporter-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455108495
[TRACE] 2023-11-08 14:51:48,516 [qtp1527668063-260] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 260]
[TRACE] 2023-11-08 14:51:48,518 [qtp1527668063-260] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 260]
[DEBUG] 2023-11-08 14:51:48,521 [qtp1527668063-260] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:48,524 [qtp1527668063-260] io.confluent.mds.request.logger log - 38 * Server has received a request on thread qtp1527668063-260null38 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:krp/roles/DeveloperWrite/bindingsnull38 > User Principal: kafka
38 > Accept: application/jsonnull38 > Content-Length: 168null38 > Content-Type: application/jsonnull38 > Host: kafka.confluent.svc.cluster.local:8090null38 > User-Agent: confluent-operatornull{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourcePatterns":[{"resourceType":"Topic","name":"_confluent-monitoring","patternType":"PREFIXED"}]}


[DEBUG] 2023-11-08 14:51:48,526 [qtp1527668063-260] io.confluent.security.auth.store.kafka.KafkaAuthWriter addResourceRoleBinding - addResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:krp role=DeveloperWrite scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') resources=[Topic:PREFIXED:_confluent-monitoring] reason=null
[DEBUG] 2023-11-08 14:51:48,527 [qtp1527668063-260] io.confluent.security.auth.store.kafka.KafkaAuthWriter lambda$addResourceRoleBinding$6 - New binding User:krp DeveloperWrite Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') [Topic:PREFIXED:_confluent-monitoring]
[DEBUG] 2023-11-08 14:51:48,527 [qtp1527668063-260] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-3]Writing new record with key RoleBindingKey{principal=User:krp, role='DeveloperWrite', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} to partition _confluent-metadata-auth-3 generation id 1
[INFO] 2023-11-08 14:51:48,530 [kafka-admin-client-thread | adminclient-4] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for adminclient-4 unregistered
[INFO] 2023-11-08 14:51:48,531 [kafka-admin-client-thread | adminclient-4] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:48,531 [kafka-admin-client-thread | adminclient-4] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:48,531 [kafka-admin-client-thread | adminclient-4] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[DEBUG] 2023-11-08 14:51:48,534 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:5 key RoleBindingKey{principal=User:krp, role='DeveloperWrite', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:PREFIXED:_confluent-monitoring]) oldValue null
[DEBUG] 2023-11-08 14:51:48,535 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-3]Pending write completed metadata=_confluent-metadata-auth-3@5 exception=null
[DEBUG] 2023-11-08 14:51:48,535 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-3]Send callback for record with partition _confluent-metadata-auth-3 generationId 1 offset 5
[DEBUG] 2023-11-08 14:51:48,535 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-3]Completing pending write since offset 5 has been consumed
[INFO] 2023-11-08 14:51:48,536 [qtp1527668063-260] io.confluent.mds.request.logger log - 38 * Server responded with a response on thread qtp1527668063-260null38 < 204null
[INFO] 2023-11-08 14:51:48,537 [qtp1527668063-260] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:48 +0000] "POST /security/1.0/principals/User:krp/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 22
[INFO] 2023-11-08 14:51:48,537 [qtp1527668063-260] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:48 +0000] "POST /security/1.0/principals/User:krp/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 22
[INFO] 2023-11-08 14:51:48,537 [qtp1527668063-260] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:51:48 +0000] "POST /security/1.0/principals/User:krp/roles/DeveloperWrite/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 22
[TRACE] 2023-11-08 14:51:48,708 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 183]
[TRACE] 2023-11-08 14:51:48,710 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 183]
[DEBUG] 2023-11-08 14:51:48,712 [qtp1527668063-183] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:51:48,713 [qtp1527668063-183] io.confluent.mds.request.logger log - 39 * Server has received a request on thread qtp1527668063-183null39 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:krp/roles/ResourceOwner/bindingsnull39 > User Principal: kafka
39 > Accept: application/jsonnull39 > Content-Length: 241null39 > Content-Type: application/jsonnull39 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null39 > User-Agent: confluent-operatornull39 > Via: 1.1 kafka-2null39 > X-Forwarded-For: 10.40.2.7null39 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null39 > X-Forwarded-Proto: httpsnull39 > X-Forwarded-Server: 10.40.0.15null{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourcePatterns":[{"resourceType":"Topic","name":"_confluent-license","patternType":"LITERAL"},{"resourceType":"Topic","name":"_confluent-command","patternType":"LITERAL"}]}


[DEBUG] 2023-11-08 14:51:48,718 [qtp1527668063-183] io.confluent.security.auth.store.kafka.KafkaAuthWriter addResourceRoleBinding - addResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:krp role=ResourceOwner scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') resources=[Topic:LITERAL:_confluent-license, Topic:LITERAL:_confluent-command] reason=null
[DEBUG] 2023-11-08 14:51:48,718 [qtp1527668063-183] io.confluent.security.auth.store.kafka.KafkaAuthWriter lambda$addResourceRoleBinding$6 - New binding User:krp ResourceOwner Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') [Topic:LITERAL:_confluent-license, Topic:LITERAL:_confluent-command]
[DEBUG] 2023-11-08 14:51:48,718 [qtp1527668063-183] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-5]Writing new record with key RoleBindingKey{principal=User:krp, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} to partition _confluent-metadata-auth-5 generation id 1
[INFO] 2023-11-08 14:51:48,720 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 15 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:51:48,721 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-ksql-confluent.ksqldb__command_topic-0)
[INFO] 2023-11-08 14:51:48,721 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 15 from controller 1 epoch 1 as part of the become-leader transition for 1 partitions
[INFO] 2023-11-08 14:51:48,726 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-ksql-confluent.ksqldb__command_topic-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[DEBUG] 2023-11-08 14:51:48,727 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-5]Pending write completed metadata=_confluent-metadata-auth-5@6 exception=null
[DEBUG] 2023-11-08 14:51:48,727 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Send callback for record with partition _confluent-metadata-auth-5 generationId 1 offset 6
[DEBUG] 2023-11-08 14:51:48,727 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:6 key RoleBindingKey{principal=User:krp, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[Topic:LITERAL:_confluent-license, Topic:LITERAL:_confluent-command]) oldValue null
[DEBUG] 2023-11-08 14:51:48,727 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-5]Completing pending write since offset 6 has been consumed
[INFO] 2023-11-08 14:51:48,727 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-ksql-confluent.ksqldb__command_topic-0 in /mnt/data/data0/logs/_confluent-ksql-confluent.ksqldb__command_topic-0 with properties {cleanup.policy=delete, min.insync.replicas=1, retention.ms=-1, unclean.leader.election.enable=false}
[INFO] 2023-11-08 14:51:48,728 [qtp1527668063-183] io.confluent.mds.request.logger log - 39 * Server responded with a response on thread qtp1527668063-183null39 < 204null
[INFO] 2023-11-08 14:51:48,728 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-ksql-confluent.ksqldb__command_topic-0 broker=0] No checkpointed highwatermark is found for partition _confluent-ksql-confluent.ksqldb__command_topic-0
[INFO] 2023-11-08 14:51:48,728 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-ksql-confluent.ksqldb__command_topic-0 broker=0] Log loaded for partition _confluent-ksql-confluent.ksqldb__command_topic-0 with initial high watermark 0
[INFO] 2023-11-08 14:51:48,728 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-ksql-confluent.ksqldb__command_topic-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:51:48,728 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:48 +0000] "POST /security/1.0/principals/User:krp/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 21
[INFO] 2023-11-08 14:51:48,729 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:48 +0000] "POST /security/1.0/principals/User:krp/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 22
[INFO] 2023-11-08 14:51:48,729 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:51:48 +0000] "POST /security/1.0/principals/User:krp/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 22
[INFO] 2023-11-08 14:51:48,729 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 9ms correlationId 15 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:51:48,731 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 16
[INFO] 2023-11-08 14:51:48,823 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:51:48,824 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:51:48,824 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:51:48,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:48,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:48,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:48,827 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455108827
[WARN] 2023-11-08 14:51:48,830 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,830 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,830 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,830 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,830 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,830 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,830 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,830 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,830 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,831 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:51:48,832 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455108832
[INFO] 2023-11-08 14:51:48,842 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:51:48,858 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:51:48,859 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:48,860 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:51:48,860 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:51:48,860 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:51:48,860 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:51:48,861 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:51:48,861 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:48,861 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:48,861 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:51:48,861 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:51:48,861 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:51:48,861 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:51:51,361 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:51,363 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:51:54,362 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:54,363 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:51:57,362 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:51:57,363 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:00,362 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:00,363 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:01,856 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 17 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:52:01,863 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_schemas_schemaregistry_confluent-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:01,863 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _schemas_schemaregistry_confluent-0 in /mnt/data/data0/logs/_schemas_schemaregistry_confluent-0 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:01,864 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _schemas_schemaregistry_confluent-0 broker=0] No checkpointed highwatermark is found for partition _schemas_schemaregistry_confluent-0
[INFO] 2023-11-08 14:52:01,864 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _schemas_schemaregistry_confluent-0 broker=0] Log loaded for partition _schemas_schemaregistry_confluent-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:01,864 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _schemas_schemaregistry_confluent-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:01,864 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_schemas_schemaregistry_confluent-0)
[INFO] 2023-11-08 14:52:01,865 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 17 for 1 partitions
[INFO] 2023-11-08 14:52:01,865 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions Map(_schemas_schemaregistry_confluent-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:01,866 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 9ms correlationId 17 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:52:01,868 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 18
[INFO] 2023-11-08 14:52:02,135 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _schemas_schemaregistry_confluent-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:02,135 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_schemas_schemaregistry_confluent-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[DEBUG] 2023-11-08 14:52:03,368 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:03,391 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:04,124 [qtp1527668063-185] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 185]
[TRACE] 2023-11-08 14:52:04,124 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 183]
[TRACE] 2023-11-08 14:52:04,128 [qtp1527668063-185] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 185]
[TRACE] 2023-11-08 14:52:04,132 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 183]
[DEBUG] 2023-11-08 14:52:04,142 [qtp1527668063-185] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:52:04,143 [qtp1527668063-185] io.confluent.mds.request.logger log - 40 * Server has received a request on thread qtp1527668063-185null40 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull40 > User Principal: c3
40 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null40 > Cache-Control: no-cachenull40 > Connection: keep-alivenull40 > Content-Type: application/jsonnull40 > Host: kafka.confluent.svc.cluster.local:8090null40 > Pragma: no-cachenull40 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:04,145 [qtp1527668063-185] io.confluent.mds.request.logger log - 40 * Server responded with a response on thread qtp1527668063-185null40 < 200null40 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:04,146 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:04 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 23
[INFO] 2023-11-08 14:52:04,146 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:04 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 23
[INFO] 2023-11-08 14:52:04,147 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:04 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 24
[DEBUG] 2023-11-08 14:52:04,147 [qtp1527668063-183] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:52:04,148 [qtp1527668063-183] io.confluent.mds.request.logger log - 41 * Server has received a request on thread qtp1527668063-183null41 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull41 > User Principal: c3
41 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null41 > Cache-Control: no-cachenull41 > Connection: keep-alivenull41 > Content-Type: application/jsonnull41 > Host: kafka.confluent.svc.cluster.local:8090null41 > Pragma: no-cachenull41 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:04,154 [qtp1527668063-183] io.confluent.mds.request.logger log - 41 * Server responded with a response on thread qtp1527668063-183null41 < 200null41 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:04,156 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:04 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 32
[INFO] 2023-11-08 14:52:04,156 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:04 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 32
[INFO] 2023-11-08 14:52:04,156 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:04 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 32
[DEBUG] 2023-11-08 14:52:06,368 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:06,369 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:07,152 [qtp1527668063-185] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 185]
[TRACE] 2023-11-08 14:52:07,154 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 183]
[TRACE] 2023-11-08 14:52:07,155 [qtp1527668063-185] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 185]
[TRACE] 2023-11-08 14:52:07,157 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 183]
[DEBUG] 2023-11-08 14:52:07,157 [qtp1527668063-185] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:07,158 [qtp1527668063-185] io.confluent.mds.request.logger log - 42 * Server has received a request on thread qtp1527668063-185null42 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull42 > User Principal: connect
42 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null42 > Cache-Control: no-cachenull42 > Connection: keep-alivenull42 > Content-Type: application/jsonnull42 > Host: kafka.confluent.svc.cluster.local:8090null42 > Pragma: no-cachenull42 > User-Agent: Java/11.0.14.1null
[DEBUG] 2023-11-08 14:52:07,159 [qtp1527668063-183] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:07,160 [qtp1527668063-185] io.confluent.mds.request.logger log - 42 * Server responded with a response on thread qtp1527668063-185null42 < 200null42 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:07,160 [qtp1527668063-183] io.confluent.mds.request.logger log - 43 * Server has received a request on thread qtp1527668063-183null43 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull43 > User Principal: connect
43 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null43 > Cache-Control: no-cachenull43 > Connection: keep-alivenull43 > Content-Type: application/jsonnull43 > Host: kafka.confluent.svc.cluster.local:8090null43 > Pragma: no-cachenull43 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:07,161 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:07,161 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:07,162 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:07,166 [qtp1527668063-183] io.confluent.mds.request.logger log - 43 * Server responded with a response on thread qtp1527668063-183null43 < 200null43 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:07,167 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:07,169 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 15
[INFO] 2023-11-08 14:52:07,169 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 15
[INFO] 2023-11-08 14:52:07,375 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 19 from controller 1 for 25 partitions
[INFO] 2023-11-08 14:52:07,391 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(confluent.connect-offsets-3, confluent.connect-offsets-0, confluent.connect-offsets-18, confluent.connect-offsets-15, confluent.connect-offsets-6, confluent.connect-offsets-12, confluent.connect-offsets-24, confluent.connect-offsets-21, confluent.connect-offsets-9)
[INFO] 2023-11-08 14:52:07,392 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 19 from controller 1 epoch 1 as part of the become-leader transition for 9 partitions
[INFO] 2023-11-08 14:52:07,400 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-15, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,401 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-15 in /mnt/data/data0/logs/confluent.connect-offsets-15 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,402 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-15 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-15
[INFO] 2023-11-08 14:52:07,402 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-15 broker=0] Log loaded for partition confluent.connect-offsets-15 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,403 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader confluent.connect-offsets-15 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,407 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-12, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,408 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-12 in /mnt/data/data0/logs/confluent.connect-offsets-12 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,408 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-12 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-12
[INFO] 2023-11-08 14:52:07,409 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-12 broker=0] Log loaded for partition confluent.connect-offsets-12 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,409 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader confluent.connect-offsets-12 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,413 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,414 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-3 in /mnt/data/data0/logs/confluent.connect-offsets-3 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,414 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-3 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-3
[INFO] 2023-11-08 14:52:07,414 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-3 broker=0] Log loaded for partition confluent.connect-offsets-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,415 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader confluent.connect-offsets-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,419 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-18, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,419 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-18 in /mnt/data/data0/logs/confluent.connect-offsets-18 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,420 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-18 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-18
[INFO] 2023-11-08 14:52:07,420 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-18 broker=0] Log loaded for partition confluent.connect-offsets-18 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,420 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader confluent.connect-offsets-18 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,424 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,425 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-0 in /mnt/data/data0/logs/confluent.connect-offsets-0 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,425 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-0 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-0
[INFO] 2023-11-08 14:52:07,425 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-0 broker=0] Log loaded for partition confluent.connect-offsets-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,425 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader confluent.connect-offsets-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,429 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,429 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-6 in /mnt/data/data0/logs/confluent.connect-offsets-6 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,429 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-6 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-6
[INFO] 2023-11-08 14:52:07,430 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-6 broker=0] Log loaded for partition confluent.connect-offsets-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,430 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader confluent.connect-offsets-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,433 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-21, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,434 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-21 in /mnt/data/data0/logs/confluent.connect-offsets-21 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,434 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-21 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-21
[INFO] 2023-11-08 14:52:07,434 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-21 broker=0] Log loaded for partition confluent.connect-offsets-21 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,434 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader confluent.connect-offsets-21 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,438 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,439 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-9 in /mnt/data/data0/logs/confluent.connect-offsets-9 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,439 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-9 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-9
[INFO] 2023-11-08 14:52:07,439 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-9 broker=0] Log loaded for partition confluent.connect-offsets-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,439 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader confluent.connect-offsets-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,443 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-24, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,443 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-24 in /mnt/data/data0/logs/confluent.connect-offsets-24 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,444 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-24 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-24
[INFO] 2023-11-08 14:52:07,444 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-24 broker=0] Log loaded for partition confluent.connect-offsets-24 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,444 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader confluent.connect-offsets-24 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,449 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-14, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,450 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-14 in /mnt/data/data0/logs/confluent.connect-offsets-14 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,450 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-14 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-14
[INFO] 2023-11-08 14:52:07,450 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-14 broker=0] Log loaded for partition confluent.connect-offsets-14 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,450 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-14 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,455 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,456 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-1 in /mnt/data/data0/logs/confluent.connect-offsets-1 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,456 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-1 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-1
[INFO] 2023-11-08 14:52:07,456 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-1 broker=0] Log loaded for partition confluent.connect-offsets-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,456 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,461 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-16, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,462 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-16 in /mnt/data/data0/logs/confluent.connect-offsets-16 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,462 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-16 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-16
[INFO] 2023-11-08 14:52:07,462 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-16 broker=0] Log loaded for partition confluent.connect-offsets-16 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,462 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-16 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,467 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-23, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,468 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-23 in /mnt/data/data0/logs/confluent.connect-offsets-23 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,468 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-23 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-23
[INFO] 2023-11-08 14:52:07,468 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-23 broker=0] Log loaded for partition confluent.connect-offsets-23 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,468 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-23 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,473 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,473 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-4 in /mnt/data/data0/logs/confluent.connect-offsets-4 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,474 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-4 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-4
[INFO] 2023-11-08 14:52:07,474 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-4 broker=0] Log loaded for partition confluent.connect-offsets-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,474 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,478 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,479 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-10 in /mnt/data/data0/logs/confluent.connect-offsets-10 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,479 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-10 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-10
[INFO] 2023-11-08 14:52:07,479 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-10 broker=0] Log loaded for partition confluent.connect-offsets-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,479 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,483 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,484 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-8 in /mnt/data/data0/logs/confluent.connect-offsets-8 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,484 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-8 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-8
[INFO] 2023-11-08 14:52:07,484 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-8 broker=0] Log loaded for partition confluent.connect-offsets-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,485 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,489 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-13, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,489 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-13 in /mnt/data/data0/logs/confluent.connect-offsets-13 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,490 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-13 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-13
[INFO] 2023-11-08 14:52:07,490 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-13 broker=0] Log loaded for partition confluent.connect-offsets-13 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,490 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-13 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,494 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-19, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,495 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-19 in /mnt/data/data0/logs/confluent.connect-offsets-19 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,495 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-19 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-19
[INFO] 2023-11-08 14:52:07,495 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-19 broker=0] Log loaded for partition confluent.connect-offsets-19 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,495 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-19 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,503 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,504 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-2 in /mnt/data/data0/logs/confluent.connect-offsets-2 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,504 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-2 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-2
[INFO] 2023-11-08 14:52:07,504 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-2 broker=0] Log loaded for partition confluent.connect-offsets-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,504 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,508 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-17, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,509 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-17 in /mnt/data/data0/logs/confluent.connect-offsets-17 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,509 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-17 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-17
[INFO] 2023-11-08 14:52:07,509 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-17 broker=0] Log loaded for partition confluent.connect-offsets-17 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,509 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-17 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,513 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,514 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-7 in /mnt/data/data0/logs/confluent.connect-offsets-7 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,514 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-7 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-7
[INFO] 2023-11-08 14:52:07,515 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-7 broker=0] Log loaded for partition confluent.connect-offsets-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,515 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,519 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-22, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,519 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-22 in /mnt/data/data0/logs/confluent.connect-offsets-22 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,519 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-22 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-22
[INFO] 2023-11-08 14:52:07,520 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-22 broker=0] Log loaded for partition confluent.connect-offsets-22 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,520 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-22 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,524 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,524 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-5 in /mnt/data/data0/logs/confluent.connect-offsets-5 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,525 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-5 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-5
[INFO] 2023-11-08 14:52:07,525 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-5 broker=0] Log loaded for partition confluent.connect-offsets-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,525 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,529 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-20, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,529 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-20 in /mnt/data/data0/logs/confluent.connect-offsets-20 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,530 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-20 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-20
[INFO] 2023-11-08 14:52:07,530 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-20 broker=0] Log loaded for partition confluent.connect-offsets-20 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,530 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-20 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,533 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-offsets-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,534 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition confluent.connect-offsets-11 in /mnt/data/data0/logs/confluent.connect-offsets-11 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,534 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-11 broker=0] No checkpointed highwatermark is found for partition confluent.connect-offsets-11
[INFO] 2023-11-08 14:52:07,534 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition confluent.connect-offsets-11 broker=0] Log loaded for partition confluent.connect-offsets-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,534 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower confluent.connect-offsets-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,535 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(confluent.connect-offsets-19, confluent.connect-offsets-20, confluent.connect-offsets-22, confluent.connect-offsets-16, confluent.connect-offsets-17, confluent.connect-offsets-23, confluent.connect-offsets-4, confluent.connect-offsets-5, confluent.connect-offsets-1, confluent.connect-offsets-2, confluent.connect-offsets-11, confluent.connect-offsets-13, confluent.connect-offsets-14, confluent.connect-offsets-7, confluent.connect-offsets-8, confluent.connect-offsets-10)
[INFO] 2023-11-08 14:52:07,535 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 19 for 16 partitions
[INFO] 2023-11-08 14:52:07,536 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(confluent.connect-offsets-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-17 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-14 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-20 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-23 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,537 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(confluent.connect-offsets-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-19 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-16 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-13 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-offsets-22 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,537 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 162ms correlationId 19 from controller 1 for 25 partitions
[INFO] 2023-11-08 14:52:07,539 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Add 25 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 20
[INFO] 2023-11-08 14:52:07,542 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 21 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:07,551 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-telemetry-metrics-10, _confluent-telemetry-metrics-7, _confluent-telemetry-metrics-4, _confluent-telemetry-metrics-1)
[INFO] 2023-11-08 14:52:07,551 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 21 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:07,556 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,557 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-10 in /mnt/data/data0/logs/_confluent-telemetry-metrics-10 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,558 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-10 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-10
[INFO] 2023-11-08 14:52:07,558 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-10 broker=0] Log loaded for partition _confluent-telemetry-metrics-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,558 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-telemetry-metrics-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,562 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,563 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-7 in /mnt/data/data0/logs/_confluent-telemetry-metrics-7 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,563 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-7 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-7
[INFO] 2023-11-08 14:52:07,563 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-7 broker=0] Log loaded for partition _confluent-telemetry-metrics-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,563 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-telemetry-metrics-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,567 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,567 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-4 in /mnt/data/data0/logs/_confluent-telemetry-metrics-4 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,567 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-4 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-4
[INFO] 2023-11-08 14:52:07,568 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-4 broker=0] Log loaded for partition _confluent-telemetry-metrics-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,568 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-telemetry-metrics-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,571 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,572 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-1 in /mnt/data/data0/logs/_confluent-telemetry-metrics-1 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,572 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-1 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-1
[INFO] 2023-11-08 14:52:07,572 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-1 broker=0] Log loaded for partition _confluent-telemetry-metrics-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,572 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-telemetry-metrics-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,576 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,577 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-11 in /mnt/data/data0/logs/_confluent-telemetry-metrics-11 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,577 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-11 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-11
[INFO] 2023-11-08 14:52:07,577 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-11 broker=0] Log loaded for partition _confluent-telemetry-metrics-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,577 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-telemetry-metrics-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,580 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,581 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-9 in /mnt/data/data0/logs/_confluent-telemetry-metrics-9 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,581 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-9 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-9
[INFO] 2023-11-08 14:52:07,581 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-9 broker=0] Log loaded for partition _confluent-telemetry-metrics-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,581 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-telemetry-metrics-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,585 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,585 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-8 in /mnt/data/data0/logs/_confluent-telemetry-metrics-8 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,585 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-8 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-8
[INFO] 2023-11-08 14:52:07,585 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-8 broker=0] Log loaded for partition _confluent-telemetry-metrics-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,585 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-telemetry-metrics-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,589 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,589 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-6 in /mnt/data/data0/logs/_confluent-telemetry-metrics-6 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,589 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-6 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-6
[INFO] 2023-11-08 14:52:07,589 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-6 broker=0] Log loaded for partition _confluent-telemetry-metrics-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,589 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-telemetry-metrics-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,592 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,593 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-5 in /mnt/data/data0/logs/_confluent-telemetry-metrics-5 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,593 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-5 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-5
[INFO] 2023-11-08 14:52:07,593 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-5 broker=0] Log loaded for partition _confluent-telemetry-metrics-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,593 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-telemetry-metrics-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,596 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,597 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-3 in /mnt/data/data0/logs/_confluent-telemetry-metrics-3 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,597 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-3 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-3
[INFO] 2023-11-08 14:52:07,597 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-3 broker=0] Log loaded for partition _confluent-telemetry-metrics-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,597 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-telemetry-metrics-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,601 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,602 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-2 in /mnt/data/data0/logs/_confluent-telemetry-metrics-2 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,602 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-2 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-2
[INFO] 2023-11-08 14:52:07,602 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-2 broker=0] Log loaded for partition _confluent-telemetry-metrics-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,602 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-telemetry-metrics-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,606 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-telemetry-metrics-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,606 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-telemetry-metrics-0 in /mnt/data/data0/logs/_confluent-telemetry-metrics-0 with properties {max.message.bytes=10485760, message.timestamp.type="CreateTime", min.insync.replicas=1, retention.bytes=-1, retention.ms=259200000, segment.ms=14400000}
[INFO] 2023-11-08 14:52:07,606 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-0 broker=0] No checkpointed highwatermark is found for partition _confluent-telemetry-metrics-0
[INFO] 2023-11-08 14:52:07,607 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-telemetry-metrics-0 broker=0] Log loaded for partition _confluent-telemetry-metrics-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,607 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-telemetry-metrics-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,607 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-telemetry-metrics-3, _confluent-telemetry-metrics-5, _confluent-telemetry-metrics-6, _confluent-telemetry-metrics-8, _confluent-telemetry-metrics-9, _confluent-telemetry-metrics-11, _confluent-telemetry-metrics-0, _confluent-telemetry-metrics-2)
[INFO] 2023-11-08 14:52:07,607 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 21 for 8 partitions
[INFO] 2023-11-08 14:52:07,608 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-telemetry-metrics-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,608 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-telemetry-metrics-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-telemetry-metrics-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,609 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 67ms correlationId 21 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:07,611 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 22
[INFO] 2023-11-08 14:52:07,671 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-telemetry-metrics-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,671 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,671 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent.connect-offsets-20 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,671 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-20, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,671 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-telemetry-metrics-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent.connect-offsets-17 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-17, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-telemetry-metrics-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent.connect-offsets-23 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-23, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent.connect-offsets-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent.connect-offsets-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent.connect-offsets-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent.connect-offsets-14 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-14, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent.connect-offsets-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-telemetry-metrics-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,672 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,686 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-19 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,686 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-19, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,686 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-telemetry-metrics-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-22 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-22, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-16 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-16, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-telemetry-metrics-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-telemetry-metrics-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,687 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,688 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,688 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-13 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,688 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-13, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,688 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,688 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,688 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent.connect-offsets-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,688 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-offsets-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,688 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-telemetry-metrics-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:07,688 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-telemetry-metrics-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:07,835 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 23 from controller 1 for 5 partitions
[INFO] 2023-11-08 14:52:07,839 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(confluent.connect-status-3, confluent.connect-status-0)
[INFO] 2023-11-08 14:52:07,839 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 23 from controller 1 epoch 1 as part of the become-leader transition for 2 partitions
[INFO] 2023-11-08 14:52:07,846 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-status-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,846 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-status-3 in /mnt/data/data0/logs/confluent.connect-status-3 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,847 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-status-3 broker=0] No checkpointed highwatermark is found for partition confluent.connect-status-3
[INFO] 2023-11-08 14:52:07,847 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-status-3 broker=0] Log loaded for partition confluent.connect-status-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,847 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader confluent.connect-status-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,855 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-status-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,856 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-status-0 in /mnt/data/data0/logs/confluent.connect-status-0 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,856 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-status-0 broker=0] No checkpointed highwatermark is found for partition confluent.connect-status-0
[INFO] 2023-11-08 14:52:07,856 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-status-0 broker=0] Log loaded for partition confluent.connect-status-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,856 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader confluent.connect-status-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,864 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-status-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,865 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-status-2 in /mnt/data/data0/logs/confluent.connect-status-2 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,865 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-status-2 broker=0] No checkpointed highwatermark is found for partition confluent.connect-status-2
[INFO] 2023-11-08 14:52:07,865 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-status-2 broker=0] Log loaded for partition confluent.connect-status-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,865 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower confluent.connect-status-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,873 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-status-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,874 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-status-1 in /mnt/data/data0/logs/confluent.connect-status-1 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,874 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-status-1 broker=0] No checkpointed highwatermark is found for partition confluent.connect-status-1
[INFO] 2023-11-08 14:52:07,874 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-status-1 broker=0] Log loaded for partition confluent.connect-status-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,874 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower confluent.connect-status-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,879 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-status-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:07,879 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition confluent.connect-status-4 in /mnt/data/data0/logs/confluent.connect-status-4 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:07,880 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-status-4 broker=0] No checkpointed highwatermark is found for partition confluent.connect-status-4
[INFO] 2023-11-08 14:52:07,880 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition confluent.connect-status-4 broker=0] Log loaded for partition confluent.connect-status-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:07,880 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower confluent.connect-status-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:07,880 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(confluent.connect-status-1, confluent.connect-status-2, confluent.connect-status-4)
[INFO] 2023-11-08 14:52:07,880 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 23 for 3 partitions
[INFO] 2023-11-08 14:52:07,881 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions Map(confluent.connect-status-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,881 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions Map(confluent.connect-status-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), confluent.connect-status-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:07,882 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 47ms correlationId 23 from controller 1 for 5 partitions
[INFO] 2023-11-08 14:52:07,884 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Add 5 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 24
[INFO] 2023-11-08 14:52:07,949 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-telemetry-reporter-local-producer
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[WARN] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'compression.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'enable.idempotence' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'acks' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'key.serializer' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'max.request.size' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'value.serializer' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'interceptor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'max.in.flight.requests.per.connection' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'linger.ms' was supplied but isn't a known config.
[INFO] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:52:07,952 [confluent-telemetry-metrics-collector-task-scheduler] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455127952
[INFO] 2023-11-08 14:52:07,980 [kafka-admin-client-thread | confluent-telemetry-reporter-local-producer] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-telemetry-reporter-local-producer unregistered
[INFO] 2023-11-08 14:52:07,981 [kafka-admin-client-thread | confluent-telemetry-reporter-local-producer] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:52:07,981 [kafka-admin-client-thread | confluent-telemetry-reporter-local-producer] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:52:07,981 [kafka-admin-client-thread | confluent-telemetry-reporter-local-producer] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:52:08,003 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 25 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:52:08,009 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=confluent.connect-configs-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,009 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition confluent.connect-configs-0 in /mnt/data/data0/logs/confluent.connect-configs-0 with properties {cleanup.policy=compact}
[INFO] 2023-11-08 14:52:08,010 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent.connect-configs-0 broker=0] No checkpointed highwatermark is found for partition confluent.connect-configs-0
[INFO] 2023-11-08 14:52:08,010 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition confluent.connect-configs-0 broker=0] Log loaded for partition confluent.connect-configs-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,010 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower confluent.connect-configs-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,010 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(confluent.connect-configs-0)
[INFO] 2023-11-08 14:52:08,010 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 25 for 1 partitions
[INFO] 2023-11-08 14:52:08,011 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions Map(confluent.connect-configs-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,012 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 9ms correlationId 25 from controller 1 for 1 partitions
[INFO] 2023-11-08 14:52:08,013 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Add 1 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 26
[INFO] 2023-11-08 14:52:08,058 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent.connect-status-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,059 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-status-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,060 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition confluent.connect-status-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,060 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-status-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,119 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent.connect-status-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,119 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-status-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,119 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition confluent.connect-configs-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,119 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=confluent.connect-configs-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,173 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Dynamic member with unknown member id joins group confluent.connect in Empty state. Created a new member id connect-1-fba34a8e-0b61-49b4-a33b-788e9f50280a and request the member to rejoin with this id.
[INFO] 2023-11-08 14:52:08,177 [data-plane-kafka-request-handler-3] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Preparing to rebalance group confluent.connect in state PreparingRebalance with old generation 0 (__consumer_offsets-8) (reason: Adding new member connect-1-fba34a8e-0b61-49b4-a33b-788e9f50280a with group instance id None)
[INFO] 2023-11-08 14:52:08,191 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 27 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,200 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6)
[INFO] 2023-11-08 14:52:08,201 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 27 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,207 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,208 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,209 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3
[INFO] 2023-11-08 14:52:08,209 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,209 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,216 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,217 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,218 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0
[INFO] 2023-11-08 14:52:08,218 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,218 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,224 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,225 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,226 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9
[INFO] 2023-11-08 14:52:08,226 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,227 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,232 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,233 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,233 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6
[INFO] 2023-11-08 14:52:08,233 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,233 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,239 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,241 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,242 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2
[INFO] 2023-11-08 14:52:08,242 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,242 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,250 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,250 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,251 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1
[INFO] 2023-11-08 14:52:08,251 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,251 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,262 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,263 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,263 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4
[INFO] 2023-11-08 14:52:08,263 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,264 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,269 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,270 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,270 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10
[INFO] 2023-11-08 14:52:08,270 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,271 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,275 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,276 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,276 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11
[INFO] 2023-11-08 14:52:08,276 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,276 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,280 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,281 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,281 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5
[INFO] 2023-11-08 14:52:08,281 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,281 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,284 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,285 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,286 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8
[INFO] 2023-11-08 14:52:08,286 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,286 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,290 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,291 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,291 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7
[INFO] 2023-11-08 14:52:08,291 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,291 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,291 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11)
[INFO] 2023-11-08 14:52:08,291 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 27 for 8 partitions
[INFO] 2023-11-08 14:52:08,292 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,292 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,293 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 101ms correlationId 27 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,294 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 28
[INFO] 2023-11-08 14:52:08,304 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,304 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,305 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,305 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,305 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,305 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,305 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,305 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,318 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 29 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,332 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9)
[INFO] 2023-11-08 14:52:08,332 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 29 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,337 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,343 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,343 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0
[INFO] 2023-11-08 14:52:08,344 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,344 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,349 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,349 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,350 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6
[INFO] 2023-11-08 14:52:08,350 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,350 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,355 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,361 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,363 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3
[INFO] 2023-11-08 14:52:08,363 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,363 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,368 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,368 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,368 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,368 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,368 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,368 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,369 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,369 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,368 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,370 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,370 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9
[INFO] 2023-11-08 14:52:08,370 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,370 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,376 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,377 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,377 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11
[INFO] 2023-11-08 14:52:08,377 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,377 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,381 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,382 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,384 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1
[INFO] 2023-11-08 14:52:08,384 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,385 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,390 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,390 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,390 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2
[INFO] 2023-11-08 14:52:08,391 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,391 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,396 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,397 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,398 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5
[INFO] 2023-11-08 14:52:08,398 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,398 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,402 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,403 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,403 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4
[INFO] 2023-11-08 14:52:08,404 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,404 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,408 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,409 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,409 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10
[INFO] 2023-11-08 14:52:08,409 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,410 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,414 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,415 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,416 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7
[INFO] 2023-11-08 14:52:08,416 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,416 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,421 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,422 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,422 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8
[INFO] 2023-11-08 14:52:08,422 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,423 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,423 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10)
[INFO] 2023-11-08 14:52:08,423 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 29 for 8 partitions
[INFO] 2023-11-08 14:52:08,425 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,425 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,426 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 108ms correlationId 29 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,428 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 30
[INFO] 2023-11-08 14:52:08,430 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 31 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,440 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,441 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,441 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,441 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,441 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,442 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,442 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,442 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,447 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2)
[INFO] 2023-11-08 14:52:08,447 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 31 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,454 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,456 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,457 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11
[INFO] 2023-11-08 14:52:08,457 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,457 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,466 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,467 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,467 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5
[INFO] 2023-11-08 14:52:08,467 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,472 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,476 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,476 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,477 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,477 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,478 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,478 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,478 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,479 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,477 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,482 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,482 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8
[INFO] 2023-11-08 14:52:08,482 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,482 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,487 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,488 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,488 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2
[INFO] 2023-11-08 14:52:08,488 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,489 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,493 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,493 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,493 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9
[INFO] 2023-11-08 14:52:08,493 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,494 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,497 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,498 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,498 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10
[INFO] 2023-11-08 14:52:08,498 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,498 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,502 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,502 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,503 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6
[INFO] 2023-11-08 14:52:08,503 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,503 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,506 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,507 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,507 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7
[INFO] 2023-11-08 14:52:08,507 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,507 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,510 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,511 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,511 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1
[INFO] 2023-11-08 14:52:08,511 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,511 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,515 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,516 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,516 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3
[INFO] 2023-11-08 14:52:08,516 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,516 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,519 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,520 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,520 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4
[INFO] 2023-11-08 14:52:08,521 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,521 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,526 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,527 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,527 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0
[INFO] 2023-11-08 14:52:08,527 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,527 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,528 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4, _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3)
[INFO] 2023-11-08 14:52:08,528 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 31 for 8 partitions
[INFO] 2023-11-08 14:52:08,529 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,529 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,530 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 100ms correlationId 31 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,533 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 32
[INFO] 2023-11-08 14:52:08,537 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 33 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,544 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4)
[INFO] 2023-11-08 14:52:08,545 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 33 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,550 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,551 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,551 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7
[INFO] 2023-11-08 14:52:08,551 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,552 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,556 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,556 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,557 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10
[INFO] 2023-11-08 14:52:08,557 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,557 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,562 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,562 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,562 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1
[INFO] 2023-11-08 14:52:08,562 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,563 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,567 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,568 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,568 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4
[INFO] 2023-11-08 14:52:08,568 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,568 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,572 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,572 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,573 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8
[INFO] 2023-11-08 14:52:08,573 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,573 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,578 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,578 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,578 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9
[INFO] 2023-11-08 14:52:08,579 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,579 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,583 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,584 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,584 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11
[INFO] 2023-11-08 14:52:08,584 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,584 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,589 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,589 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,589 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0
[INFO] 2023-11-08 14:52:08,589 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,590 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,593 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,594 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,594 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2
[INFO] 2023-11-08 14:52:08,594 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,594 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,598 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,598 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,599 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3
[INFO] 2023-11-08 14:52:08,599 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,599 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,604 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,604 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,604 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6
[INFO] 2023-11-08 14:52:08,604 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,605 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,609 [data-plane-kafka-request-handler-2] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,610 [data-plane-kafka-request-handler-2] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,610 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5
[INFO] 2023-11-08 14:52:08,610 [data-plane-kafka-request-handler-2] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,610 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,610 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2)
[INFO] 2023-11-08 14:52:08,610 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 33 for 8 partitions
[INFO] 2023-11-08 14:52:08,611 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,612 [data-plane-kafka-request-handler-2] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,612 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 75ms correlationId 33 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,615 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 34
[TRACE] 2023-11-08 14:52:08,628 [qtp1527668063-251] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 251]
[INFO] 2023-11-08 14:52:08,629 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,629 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,629 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,629 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,629 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,630 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,630 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,630 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,630 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,630 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,630 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,630 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,630 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,630 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[TRACE] 2023-11-08 14:52:08,631 [qtp1527668063-251] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 251]
[DEBUG] 2023-11-08 14:52:08,633 [qtp1527668063-251] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[TRACE] 2023-11-08 14:52:08,633 [qtp1527668063-252] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 252]
[INFO] 2023-11-08 14:52:08,634 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,634 [qtp1527668063-251] io.confluent.mds.request.logger log - 44 * Server has received a request on thread qtp1527668063-251null44 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull44 > User Principal: connect
44 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null44 > Cache-Control: no-cachenull44 > Connection: keep-alivenull44 > Content-Type: application/jsonnull44 > Host: kafka.confluent.svc.cluster.local:8090null44 > Pragma: no-cachenull44 > User-Agent: Java/11.0.14.1null
[TRACE] 2023-11-08 14:52:08,635 [qtp1527668063-252] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 252]
[DEBUG] 2023-11-08 14:52:08,637 [qtp1527668063-252] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:08,635 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,639 [qtp1527668063-252] io.confluent.mds.request.logger log - 45 * Server has received a request on thread qtp1527668063-252null45 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull45 > User Principal: connect
45 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null45 > Cache-Control: no-cachenull45 > Connection: keep-alivenull45 > Content-Type: application/jsonnull45 > Host: kafka.confluent.svc.cluster.local:8090null45 > Pragma: no-cachenull45 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:08,640 [qtp1527668063-252] io.confluent.mds.request.logger log - 45 * Server responded with a response on thread qtp1527668063-252null45 < 200null45 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:08,641 [qtp1527668063-252] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:08,641 [qtp1527668063-251] io.confluent.mds.request.logger log - 44 * Server responded with a response on thread qtp1527668063-251null44 < 200null44 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:08,641 [qtp1527668063-252] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:08,641 [qtp1527668063-252] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:08,644 [qtp1527668063-251] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 17
[INFO] 2023-11-08 14:52:08,644 [qtp1527668063-251] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 17
[INFO] 2023-11-08 14:52:08,644 [qtp1527668063-251] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 17
[INFO] 2023-11-08 14:52:08,646 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,646 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,646 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,646 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,646 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,646 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,646 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,646 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,646 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,647 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,647 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,647 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,647 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,647 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,647 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,647 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-stream-extension-rekey-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,648 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 35 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,757 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2)
[INFO] 2023-11-08 14:52:08,757 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 35 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,772 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,772 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,773 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11
[INFO] 2023-11-08 14:52:08,774 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,775 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,780 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,781 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,781 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8
[INFO] 2023-11-08 14:52:08,781 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,781 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,785 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,786 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,786 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5
[INFO] 2023-11-08 14:52:08,786 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,786 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,790 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,790 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,790 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2
[INFO] 2023-11-08 14:52:08,790 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,790 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,794 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,795 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,795 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10
[INFO] 2023-11-08 14:52:08,795 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,795 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,799 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,799 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,799 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9
[INFO] 2023-11-08 14:52:08,799 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,800 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,803 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,804 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,804 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7
[INFO] 2023-11-08 14:52:08,804 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,804 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,808 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,809 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,809 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6
[INFO] 2023-11-08 14:52:08,809 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,809 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,812 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,813 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,813 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4
[INFO] 2023-11-08 14:52:08,813 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,813 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,817 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,817 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,817 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3
[INFO] 2023-11-08 14:52:08,817 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,817 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,821 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,821 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,821 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1
[INFO] 2023-11-08 14:52:08,822 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,822 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,825 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,825 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:08,825 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0
[INFO] 2023-11-08 14:52:08,825 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,826 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,826 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3, _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4)
[INFO] 2023-11-08 14:52:08,826 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 35 for 8 partitions
[INFO] 2023-11-08 14:52:08,827 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,827 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:08,828 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 180ms correlationId 35 from controller 1 for 12 partitions
[TRACE] 2023-11-08 14:52:08,870 [qtp1527668063-247] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 247]
[TRACE] 2023-11-08 14:52:08,870 [qtp1527668063-249] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 249]
[TRACE] 2023-11-08 14:52:08,871 [qtp1527668063-247] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 247]
[INFO] 2023-11-08 14:52:08,872 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 36
[INFO] 2023-11-08 14:52:08,873 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,873 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,873 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,873 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,873 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[TRACE] 2023-11-08 14:52:08,873 [qtp1527668063-249] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 249]
[INFO] 2023-11-08 14:52:08,873 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,874 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,874 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[DEBUG] 2023-11-08 14:52:08,875 [qtp1527668063-247] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:52:08,875 [qtp1527668063-249] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:08,876 [qtp1527668063-249] io.confluent.mds.request.logger log - 46 * Server has received a request on thread qtp1527668063-249null46 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull46 > User Principal: connect
46 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null46 > Cache-Control: no-cachenull46 > Connection: keep-alivenull46 > Content-Type: application/jsonnull46 > Host: kafka.confluent.svc.cluster.local:8090null46 > Pragma: no-cachenull46 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:08,876 [qtp1527668063-247] io.confluent.mds.request.logger log - 47 * Server has received a request on thread qtp1527668063-247null47 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull47 > User Principal: connect
47 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null47 > Cache-Control: no-cachenull47 > Connection: keep-alivenull47 > Content-Type: application/jsonnull47 > Host: kafka.confluent.svc.cluster.local:8090null47 > Pragma: no-cachenull47 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:08,878 [qtp1527668063-247] io.confluent.mds.request.logger log - 47 * Server responded with a response on thread qtp1527668063-247null47 < 200null47 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:08,879 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:08,879 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:08,879 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:08,881 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 37 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:08,883 [qtp1527668063-249] io.confluent.mds.request.logger log - 46 * Server responded with a response on thread qtp1527668063-249null46 < 200null46 < Content-Type: application/jsonnull
[WARN] 2023-11-08 14:52:08,885 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-6. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-9. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-0. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-repartition-3. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:08,886 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-expected-group-consumption-rekey-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:08,888 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 18
[INFO] 2023-11-08 14:52:08,888 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 18
[INFO] 2023-11-08 14:52:08,888 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:08 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 18
[INFO] 2023-11-08 14:52:08,892 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0)
[INFO] 2023-11-08 14:52:08,892 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 37 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:08,901 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,906 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,907 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6
[INFO] 2023-11-08 14:52:08,908 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,908 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,914 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,915 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,916 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3
[INFO] 2023-11-08 14:52:08,916 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,917 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,923 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,924 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,924 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9
[INFO] 2023-11-08 14:52:08,924 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,924 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,931 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,931 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,932 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0
[INFO] 2023-11-08 14:52:08,932 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,932 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,939 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,939 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,940 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5
[INFO] 2023-11-08 14:52:08,941 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,941 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,950 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,951 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,951 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4
[INFO] 2023-11-08 14:52:08,952 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,952 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,959 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,961 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,961 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10
[INFO] 2023-11-08 14:52:08,961 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,961 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,969 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,973 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,973 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8
[INFO] 2023-11-08 14:52:08,974 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,974 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,988 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:08,988 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:08,989 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7
[INFO] 2023-11-08 14:52:08,989 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:08,989 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:08,999 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,000 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,000 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11
[INFO] 2023-11-08 14:52:09,001 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,001 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,009 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,010 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,010 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2
[INFO] 2023-11-08 14:52:09,010 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,011 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,019 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,024 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,024 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1
[INFO] 2023-11-08 14:52:09,024 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,024 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,025 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2)
[INFO] 2023-11-08 14:52:09,025 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 37 for 8 partitions
[INFO] 2023-11-08 14:52:09,026 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,026 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,029 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 148ms correlationId 37 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,041 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 38
[INFO] 2023-11-08 14:52:09,046 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 39 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:09,074 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10)
[INFO] 2023-11-08 14:52:09,074 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 39 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[INFO] 2023-11-08 14:52:09,079 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,080 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[TRACE] 2023-11-08 14:52:09,080 [qtp1527668063-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 246]
[INFO] 2023-11-08 14:52:09,081 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4
[INFO] 2023-11-08 14:52:09,081 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,081 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:09,082 [qtp1527668063-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 246]
[DEBUG] 2023-11-08 14:52:09,086 [qtp1527668063-246] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:09,088 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,090 [qtp1527668063-246] io.confluent.mds.request.logger log - 48 * Server has received a request on thread qtp1527668063-246null48 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull48 > User Principal: connect
48 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null48 > Cache-Control: no-cachenull48 > Connection: keep-alivenull48 > Content-Type: application/jsonnull48 > Host: kafka.confluent.svc.cluster.local:8090null48 > Pragma: no-cachenull48 > User-Agent: Java/11.0.14.1null
[TRACE] 2023-11-08 14:52:09,090 [qtp1527668063-259] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 259]
[TRACE] 2023-11-08 14:52:09,092 [qtp1527668063-259] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 259]
[INFO] 2023-11-08 14:52:09,094 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,094 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10
[INFO] 2023-11-08 14:52:09,095 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,095 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,095 [qtp1527668063-246] io.confluent.mds.request.logger log - 48 * Server responded with a response on thread qtp1527668063-246null48 < 200null48 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:09,097 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 17
[INFO] 2023-11-08 14:52:09,097 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 17
[INFO] 2023-11-08 14:52:09,097 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 17
[DEBUG] 2023-11-08 14:52:09,097 [qtp1527668063-259] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:09,099 [qtp1527668063-259] io.confluent.mds.request.logger log - 49 * Server has received a request on thread qtp1527668063-259null49 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull49 > User Principal: connect
49 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null49 > Cache-Control: no-cachenull49 > Connection: keep-alivenull49 > Content-Type: application/jsonnull49 > Host: kafka.confluent.svc.cluster.local:8090null49 > Pragma: no-cachenull49 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:09,101 [qtp1527668063-259] io.confluent.mds.request.logger log - 49 * Server responded with a response on thread qtp1527668063-259null49 < 200null49 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:09,103 [qtp1527668063-259] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:09,103 [qtp1527668063-259] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:09,103 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,103 [qtp1527668063-259] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:09,105 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,105 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7
[INFO] 2023-11-08 14:52:09,105 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,105 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,109 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,110 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,110 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1
[INFO] 2023-11-08 14:52:09,110 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,110 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,114 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,115 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,115 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4
[INFO] 2023-11-08 14:52:09,115 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,115 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,120 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,121 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,121 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1
[INFO] 2023-11-08 14:52:09,121 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,121 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,125 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,126 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,126 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7
[INFO] 2023-11-08 14:52:09,126 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,127 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,131 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,131 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,132 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10
[INFO] 2023-11-08 14:52:09,132 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,132 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,136 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,137 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,137 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3
[INFO] 2023-11-08 14:52:09,137 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,137 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,140 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,141 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,141 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5
[INFO] 2023-11-08 14:52:09,141 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,141 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,147 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,147 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,150 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8
[INFO] 2023-11-08 14:52:09,150 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,150 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,155 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,155 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,156 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6
[INFO] 2023-11-08 14:52:09,156 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,156 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,164 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,164 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,165 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3
[INFO] 2023-11-08 14:52:09,165 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,165 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,170 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,170 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,171 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8
[INFO] 2023-11-08 14:52:09,171 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,171 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,175 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,175 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,175 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11
[INFO] 2023-11-08 14:52:09,175 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,175 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,179 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,180 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,180 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6
[INFO] 2023-11-08 14:52:09,180 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,180 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,184 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,184 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,184 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9
[INFO] 2023-11-08 14:52:09,185 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,185 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,188 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,189 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,189 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0
[INFO] 2023-11-08 14:52:09,189 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,189 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,194 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,194 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,195 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2
[INFO] 2023-11-08 14:52:09,195 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,195 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,198 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,199 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,199 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5
[INFO] 2023-11-08 14:52:09,199 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,199 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,202 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,202 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,203 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11
[INFO] 2023-11-08 14:52:09,203 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,203 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,206 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,206 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,206 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2
[INFO] 2023-11-08 14:52:09,207 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,207 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,210 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,210 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,210 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0
[INFO] 2023-11-08 14:52:09,211 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,211 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,213 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,214 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,214 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9
[INFO] 2023-11-08 14:52:09,214 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,214 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,215 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5, _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9)
[INFO] 2023-11-08 14:52:09,215 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 39 for 16 partitions
[INFO] 2023-11-08 14:52:09,216 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,216 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,217 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 171ms correlationId 39 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:09,222 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 40
[INFO] 2023-11-08 14:52:09,225 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 41 from controller 1 for 24 partitions
[TRACE] 2023-11-08 14:52:09,236 [qtp1527668063-245] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 245]
[TRACE] 2023-11-08 14:52:09,236 [qtp1527668063-260] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 260]
[TRACE] 2023-11-08 14:52:09,238 [qtp1527668063-245] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 245]
[INFO] 2023-11-08 14:52:09,238 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-cluster-rekey-4, _confluent-controlcenter-7-1-0-0-cluster-rekey-10, _confluent-controlcenter-7-1-0-0-cluster-rekey-1, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8, _confluent-controlcenter-7-1-0-0-cluster-rekey-7, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5)
[INFO] 2023-11-08 14:52:09,239 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 41 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[TRACE] 2023-11-08 14:52:09,239 [qtp1527668063-260] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 260]
[DEBUG] 2023-11-08 14:52:09,240 [qtp1527668063-245] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:09,241 [qtp1527668063-245] io.confluent.mds.request.logger log - 50 * Server has received a request on thread qtp1527668063-245null50 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull50 > User Principal: connect
50 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null50 > Cache-Control: no-cachenull50 > Connection: keep-alivenull50 > Content-Type: application/jsonnull50 > Host: kafka.confluent.svc.cluster.local:8090null50 > Pragma: no-cachenull50 > User-Agent: Java/11.0.14.1null
[DEBUG] 2023-11-08 14:52:09,241 [qtp1527668063-260] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:09,242 [qtp1527668063-245] io.confluent.mds.request.logger log - 50 * Server responded with a response on thread qtp1527668063-245null50 < 200null50 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:09,242 [qtp1527668063-260] io.confluent.mds.request.logger log - 51 * Server has received a request on thread qtp1527668063-260null51 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull51 > User Principal: connect
51 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null51 > Cache-Control: no-cachenull51 > Connection: keep-alivenull51 > Content-Type: application/jsonnull51 > Host: kafka.confluent.svc.cluster.local:8090null51 > Pragma: no-cachenull51 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:09,243 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,244 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,243 [qtp1527668063-245] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 7
[INFO] 2023-11-08 14:52:09,245 [qtp1527668063-245] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:09,245 [qtp1527668063-245] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:52:09,245 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1
[INFO] 2023-11-08 14:52:09,245 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,246 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-cluster-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,249 [qtp1527668063-260] io.confluent.mds.request.logger log - 51 * Server responded with a response on thread qtp1527668063-260null51 < 200null51 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:09,250 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,250 [qtp1527668063-260] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:52:09,250 [qtp1527668063-260] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:52:09,250 [qtp1527668063-260] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 14
[INFO] 2023-11-08 14:52:09,250 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,250 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8
[INFO] 2023-11-08 14:52:09,251 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,251 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,254 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,254 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,255 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4
[INFO] 2023-11-08 14:52:09,255 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,255 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-cluster-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,260 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,261 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,261 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11
[INFO] 2023-11-08 14:52:09,261 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,261 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,266 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,266 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,266 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10
[INFO] 2023-11-08 14:52:09,266 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,267 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-cluster-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,270 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,271 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,271 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2
[INFO] 2023-11-08 14:52:09,271 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,271 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,274 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,275 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,275 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7
[INFO] 2023-11-08 14:52:09,275 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,275 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-cluster-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,278 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,279 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,279 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5
[INFO] 2023-11-08 14:52:09,279 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,279 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,283 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,283 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,283 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2
[INFO] 2023-11-08 14:52:09,283 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,283 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,287 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,287 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,287 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10
[INFO] 2023-11-08 14:52:09,287 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,287 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,290 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,291 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,291 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0
[INFO] 2023-11-08 14:52:09,291 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,291 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,295 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,295 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,296 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1
[INFO] 2023-11-08 14:52:09,296 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,296 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,300 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,300 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,300 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6
[INFO] 2023-11-08 14:52:09,300 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,301 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,304 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,305 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,305 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3
[INFO] 2023-11-08 14:52:09,305 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,305 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,308 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,308 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,309 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8
[INFO] 2023-11-08 14:52:09,309 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,309 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,312 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,313 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,313 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7
[INFO] 2023-11-08 14:52:09,313 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,313 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,316 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,316 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,316 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3
[INFO] 2023-11-08 14:52:09,316 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,316 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,319 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,320 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,320 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9
[INFO] 2023-11-08 14:52:09,320 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,320 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,324 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,324 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,324 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9
[INFO] 2023-11-08 14:52:09,324 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,324 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,328 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,328 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,328 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0
[INFO] 2023-11-08 14:52:09,328 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,328 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,340 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,340 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,341 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11
[INFO] 2023-11-08 14:52:09,341 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,341 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:09,346 [qtp1527668063-254] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 254]
[TRACE] 2023-11-08 14:52:09,346 [qtp1527668063-258] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 258]
[INFO] 2023-11-08 14:52:09,346 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,347 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-cluster-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,347 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5
[INFO] 2023-11-08 14:52:09,347 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,347 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-cluster-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:09,348 [qtp1527668063-254] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 254]
[TRACE] 2023-11-08 14:52:09,349 [qtp1527668063-258] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 258]
[DEBUG] 2023-11-08 14:52:09,350 [qtp1527668063-254] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[DEBUG] 2023-11-08 14:52:09,351 [qtp1527668063-258] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:09,351 [qtp1527668063-254] io.confluent.mds.request.logger log - 52 * Server has received a request on thread qtp1527668063-254null52 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull52 > User Principal: connect
52 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null52 > Cache-Control: no-cachenull52 > Connection: keep-alivenull52 > Content-Type: application/jsonnull52 > Host: kafka.confluent.svc.cluster.local:8090null52 > Pragma: no-cachenull52 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:09,351 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,351 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,352 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4
[INFO] 2023-11-08 14:52:09,352 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,352 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,353 [qtp1527668063-254] io.confluent.mds.request.logger log - 52 * Server responded with a response on thread qtp1527668063-254null52 < 200null52 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:09,353 [qtp1527668063-258] io.confluent.mds.request.logger log - 53 * Server has received a request on thread qtp1527668063-258null53 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull53 > User Principal: connect
53 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null53 > Cache-Control: no-cachenull53 > Connection: keep-alivenull53 > Content-Type: application/jsonnull53 > Host: kafka.confluent.svc.cluster.local:8090null53 > Pragma: no-cachenull53 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:09,354 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:09,354 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:09,354 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 8
[INFO] 2023-11-08 14:52:09,357 [qtp1527668063-258] io.confluent.mds.request.logger log - 53 * Server responded with a response on thread qtp1527668063-258null53 < 200null53 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:09,357 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,358 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,358 [qtp1527668063-258] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:52:09,358 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6
[INFO] 2023-11-08 14:52:09,358 [qtp1527668063-258] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:52:09,358 [qtp1527668063-258] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:52:09,358 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,359 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,359 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-cluster-rekey-0, _confluent-controlcenter-7-1-0-0-cluster-rekey-5, _confluent-controlcenter-7-1-0-0-cluster-rekey-3, _confluent-controlcenter-7-1-0-0-cluster-rekey-2, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4, _confluent-controlcenter-7-1-0-0-cluster-rekey-9, _confluent-controlcenter-7-1-0-0-cluster-rekey-8, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-1-0-0-cluster-rekey-6, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1, _confluent-controlcenter-7-1-0-0-cluster-rekey-11)
[INFO] 2023-11-08 14:52:09,359 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 41 for 16 partitions
[INFO] 2023-11-08 14:52:09,360 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-cluster-rekey-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,361 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-cluster-rekey-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,361 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 136ms correlationId 41 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:09,364 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 42
[INFO] 2023-11-08 14:52:09,367 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 43 from controller 1 for 12 partitions
[DEBUG] 2023-11-08 14:52:09,372 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[INFO] 2023-11-08 14:52:09,373 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3)
[INFO] 2023-11-08 14:52:09,374 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 43 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[DEBUG] 2023-11-08 14:52:09,376 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:09,377 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,378 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,379 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,380 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,380 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,380 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,380 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,380 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,378 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,380 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,380 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,380 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,380 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,380 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,381 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,382 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0
[INFO] 2023-11-08 14:52:09,382 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,382 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,386 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,387 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,387 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6
[INFO] 2023-11-08 14:52:09,387 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,387 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,391 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,391 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,391 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,391 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,391 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,391 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,391 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,391 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,392 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,393 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,393 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,393 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,393 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9
[INFO] 2023-11-08 14:52:09,393 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,393 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,393 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,393 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,394 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,394 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,394 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-cluster-rekey-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,394 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-cluster-rekey-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,394 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,394 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,394 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,394 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,394 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,394 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-message-rekey-store-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,398 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,399 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,399 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3
[INFO] 2023-11-08 14:52:09,399 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,399 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,403 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,403 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,403 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1
[INFO] 2023-11-08 14:52:09,404 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,404 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,406 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,407 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,407 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11
[INFO] 2023-11-08 14:52:09,407 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,407 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,410 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,411 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,411 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10
[INFO] 2023-11-08 14:52:09,412 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,412 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,416 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,416 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,416 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7
[INFO] 2023-11-08 14:52:09,417 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,417 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,420 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,420 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,421 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8
[INFO] 2023-11-08 14:52:09,421 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,421 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,424 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,424 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,425 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2
[INFO] 2023-11-08 14:52:09,425 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,425 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,427 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,428 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,428 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5
[INFO] 2023-11-08 14:52:09,428 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,428 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,431 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,431 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,431 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4
[INFO] 2023-11-08 14:52:09,431 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,431 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,431 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10)
[INFO] 2023-11-08 14:52:09,432 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 43 for 8 partitions
[INFO] 2023-11-08 14:52:09,432 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,433 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,433 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 66ms correlationId 43 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,436 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 44
[INFO] 2023-11-08 14:52:09,439 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 45 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,448 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10)
[INFO] 2023-11-08 14:52:09,449 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 45 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,456 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,457 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,458 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4
[INFO] 2023-11-08 14:52:09,458 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,459 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,463 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,464 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,464 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1
[INFO] 2023-11-08 14:52:09,464 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,464 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,471 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,471 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,472 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7
[INFO] 2023-11-08 14:52:09,472 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,472 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,475 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,476 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,476 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10
[INFO] 2023-11-08 14:52:09,476 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,476 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,481 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,481 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,482 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0
[INFO] 2023-11-08 14:52:09,482 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,482 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:09,491 [qtp1527668063-252] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 252]
[INFO] 2023-11-08 14:52:09,491 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,491 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[TRACE] 2023-11-08 14:52:09,492 [qtp1527668063-252] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 252]
[INFO] 2023-11-08 14:52:09,493 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3
[INFO] 2023-11-08 14:52:09,494 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,494 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[DEBUG] 2023-11-08 14:52:09,496 [qtp1527668063-252] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:09,497 [qtp1527668063-252] io.confluent.mds.request.logger log - 54 * Server has received a request on thread qtp1527668063-252null54 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull54 > User Principal: connect
54 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null54 > Cache-Control: no-cachenull54 > Connection: keep-alivenull54 > Content-Type: application/jsonnull54 > Host: kafka.confluent.svc.cluster.local:8090null54 > Pragma: no-cachenull54 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:09,497 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,498 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,498 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2
[INFO] 2023-11-08 14:52:09,498 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,498 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,503 [qtp1527668063-252] io.confluent.mds.request.logger log - 54 * Server responded with a response on thread qtp1527668063-252null54 < 200null54 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:09,504 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,505 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,505 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8
[INFO] 2023-11-08 14:52:09,505 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,505 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:09,505 [qtp1527668063-251] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 251]
[INFO] 2023-11-08 14:52:09,505 [qtp1527668063-252] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 15
[INFO] 2023-11-08 14:52:09,505 [qtp1527668063-252] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 15
[INFO] 2023-11-08 14:52:09,506 [qtp1527668063-252] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 16
[INFO] 2023-11-08 14:52:09,508 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,509 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,509 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5
[INFO] 2023-11-08 14:52:09,509 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,509 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,512 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,512 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,513 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6
[INFO] 2023-11-08 14:52:09,513 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,513 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,515 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,516 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,516 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11
[INFO] 2023-11-08 14:52:09,516 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,516 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:09,513 [qtp1527668063-251] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 251]
[INFO] 2023-11-08 14:52:09,519 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[DEBUG] 2023-11-08 14:52:09,519 [qtp1527668063-251] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:09,519 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:09,520 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9
[INFO] 2023-11-08 14:52:09,520 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,520 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,520 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8)
[INFO] 2023-11-08 14:52:09,520 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 45 for 8 partitions
[INFO] 2023-11-08 14:52:09,521 [qtp1527668063-251] io.confluent.mds.request.logger log - 55 * Server has received a request on thread qtp1527668063-251null55 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull55 > User Principal: connect
55 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null55 > Cache-Control: no-cachenull55 > Connection: keep-alivenull55 > Content-Type: application/jsonnull55 > Host: kafka.confluent.svc.cluster.local:8090null55 > Pragma: no-cachenull55 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:09,521 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,521 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,522 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 84ms correlationId 45 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,522 [qtp1527668063-251] io.confluent.mds.request.logger log - 55 * Server responded with a response on thread qtp1527668063-251null55 < 200null55 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:09,523 [qtp1527668063-251] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 18
[INFO] 2023-11-08 14:52:09,523 [qtp1527668063-251] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 18
[INFO] 2023-11-08 14:52:09,523 [qtp1527668063-251] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 18
[INFO] 2023-11-08 14:52:09,523 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 46
[INFO] 2023-11-08 14:52:09,525 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 47 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,530 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10)
[INFO] 2023-11-08 14:52:09,530 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 47 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,535 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,536 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,537 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7
[INFO] 2023-11-08 14:52:09,537 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,537 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,541 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,541 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,542 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4
[INFO] 2023-11-08 14:52:09,542 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,542 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,552 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,553 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,553 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1
[INFO] 2023-11-08 14:52:09,554 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,554 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,558 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,559 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,559 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10
[INFO] 2023-11-08 14:52:09,559 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,559 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,563 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,564 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,564 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9
[INFO] 2023-11-08 14:52:09,564 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,564 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,568 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,568 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,568 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8
[INFO] 2023-11-08 14:52:09,569 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,569 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,574 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,575 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,575 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6
[INFO] 2023-11-08 14:52:09,575 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,575 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,583 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,584 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,584 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5
[INFO] 2023-11-08 14:52:09,584 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,584 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,593 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,596 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,596 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3
[INFO] 2023-11-08 14:52:09,596 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,596 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,601 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,602 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,602 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2
[INFO] 2023-11-08 14:52:09,602 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,602 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:09,602 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 183]
[TRACE] 2023-11-08 14:52:09,605 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 183]
[INFO] 2023-11-08 14:52:09,606 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,607 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,607 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0
[INFO] 2023-11-08 14:52:09,607 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,607 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:09,608 [qtp1527668063-194] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 194]
[DEBUG] 2023-11-08 14:52:09,608 [qtp1527668063-183] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:09,609 [qtp1527668063-183] io.confluent.mds.request.logger log - 56 * Server has received a request on thread qtp1527668063-183null56 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull56 > User Principal: connect
56 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null56 > Cache-Control: no-cachenull56 > Connection: keep-alivenull56 > Content-Type: application/jsonnull56 > Host: kafka.confluent.svc.cluster.local:8090null56 > Pragma: no-cachenull56 > User-Agent: Java/11.0.14.1null
[TRACE] 2023-11-08 14:52:09,610 [qtp1527668063-194] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 194]
[INFO] 2023-11-08 14:52:09,611 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,611 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,611 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11
[INFO] 2023-11-08 14:52:09,611 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,612 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,612 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8)
[INFO] 2023-11-08 14:52:09,612 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 47 for 8 partitions
[DEBUG] 2023-11-08 14:52:09,612 [qtp1527668063-194] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:09,613 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,613 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,613 [qtp1527668063-194] io.confluent.mds.request.logger log - 57 * Server has received a request on thread qtp1527668063-194null57 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull57 > User Principal: connect
57 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null57 > Cache-Control: no-cachenull57 > Connection: keep-alivenull57 > Content-Type: application/jsonnull57 > Host: kafka.confluent.svc.cluster.local:8090null57 > Pragma: no-cachenull57 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:09,613 [qtp1527668063-183] io.confluent.mds.request.logger log - 56 * Server responded with a response on thread qtp1527668063-183null56 < 200null56 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:09,613 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 88ms correlationId 47 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,614 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:52:09,614 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 12
[INFO] 2023-11-08 14:52:09,615 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 644 "-" "Java/11.0.14.1" 13
[INFO] 2023-11-08 14:52:09,615 [qtp1527668063-194] io.confluent.mds.request.logger log - 57 * Server responded with a response on thread qtp1527668063-194null57 < 200null57 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:09,615 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 48
[INFO] 2023-11-08 14:52:09,617 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:09,617 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:09,617 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:09,618 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 49 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,624 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2)
[INFO] 2023-11-08 14:52:09,624 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 49 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,631 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,632 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,633 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11
[INFO] 2023-11-08 14:52:09,633 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,634 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,639 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,640 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,640 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8
[INFO] 2023-11-08 14:52:09,640 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,640 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,647 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,648 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,648 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5
[INFO] 2023-11-08 14:52:09,648 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,648 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,661 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,662 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,662 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2
[INFO] 2023-11-08 14:52:09,662 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,662 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,672 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,673 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,673 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1
[INFO] 2023-11-08 14:52:09,673 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,673 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,688 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,689 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,689 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0
[INFO] 2023-11-08 14:52:09,689 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,690 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,700 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,701 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,701 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10
[INFO] 2023-11-08 14:52:09,701 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,701 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,715 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,716 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,716 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9
[INFO] 2023-11-08 14:52:09,716 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,716 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,723 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,724 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,724 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7
[INFO] 2023-11-08 14:52:09,724 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,725 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,732 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,733 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,733 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6
[INFO] 2023-11-08 14:52:09,734 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,734 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,740 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,741 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,741 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4
[INFO] 2023-11-08 14:52:09,742 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,742 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,748 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,749 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,750 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3
[INFO] 2023-11-08 14:52:09,750 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,750 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,751 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0)
[INFO] 2023-11-08 14:52:09,751 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 49 for 8 partitions
[INFO] 2023-11-08 14:52:09,752 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,752 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,753 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 135ms correlationId 49 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,756 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 50
[INFO] 2023-11-08 14:52:09,759 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 51 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,766 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3)
[INFO] 2023-11-08 14:52:09,767 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 51 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,775 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,775 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,778 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0
[INFO] 2023-11-08 14:52:09,778 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,779 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,789 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,790 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,790 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6
[INFO] 2023-11-08 14:52:09,790 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,790 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,794 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,795 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,795 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9
[INFO] 2023-11-08 14:52:09,795 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,795 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,799 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,799 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,799 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3
[INFO] 2023-11-08 14:52:09,799 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,800 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,802 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,803 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,803 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1
[INFO] 2023-11-08 14:52:09,803 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,803 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,806 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,807 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,807 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10
[INFO] 2023-11-08 14:52:09,807 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,807 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,811 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,811 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,812 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11
[INFO] 2023-11-08 14:52:09,812 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,812 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,816 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,817 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,817 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7
[INFO] 2023-11-08 14:52:09,817 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,817 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,820 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,821 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,821 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8
[INFO] 2023-11-08 14:52:09,821 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,821 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,824 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,824 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,824 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2
[INFO] 2023-11-08 14:52:09,824 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,824 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,827 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,828 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,828 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4
[INFO] 2023-11-08 14:52:09,828 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,828 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,833 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,834 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:09,834 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5
[INFO] 2023-11-08 14:52:09,834 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,834 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,834 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11, _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10)
[INFO] 2023-11-08 14:52:09,835 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 51 for 8 partitions
[INFO] 2023-11-08 14:52:09,836 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,836 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,837 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 78ms correlationId 51 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,844 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 52
[INFO] 2023-11-08 14:52:09,848 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 53 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,861 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7)
[INFO] 2023-11-08 14:52:09,861 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 53 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:09,867 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,885 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,887 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10
[INFO] 2023-11-08 14:52:09,887 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,887 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,888 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,888 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,888 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,888 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,888 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,888 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,889 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,890 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,891 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,898 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,899 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,899 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1
[INFO] 2023-11-08 14:52:09,899 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,899 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,905 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,905 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,905 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,905 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,906 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,906 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,908 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,909 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,909 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4
[INFO] 2023-11-08 14:52:09,909 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,909 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,918 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,919 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,919 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,919 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,919 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,919 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,919 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,919 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,919 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,919 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,920 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,921 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,921 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-metrics-trigger-measurement-rekey-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,921 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,921 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTERTHIS-0000000105-store-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,921 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,921 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,921 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,921 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,921 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:09,921 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:09,925 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,926 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,926 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7
[INFO] 2023-11-08 14:52:09,926 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,927 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:09,927 [qtp1527668063-194] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 194]
[TRACE] 2023-11-08 14:52:09,932 [qtp1527668063-194] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 194]
[DEBUG] 2023-11-08 14:52:09,940 [qtp1527668063-194] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:09,941 [qtp1527668063-194] io.confluent.mds.request.logger log - 58 * Server has received a request on thread qtp1527668063-194null58 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull58 > User Principal: connect
58 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null58 > Cache-Control: no-cachenull58 > Connection: keep-alivenull58 > Content-Type: application/jsonnull58 > Host: kafka.confluent.svc.cluster.local:8090null58 > Pragma: no-cachenull58 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:09,943 [qtp1527668063-194] io.confluent.mds.request.logger log - 58 * Server responded with a response on thread qtp1527668063-194null58 < 200null58 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:09,949 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,949 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,949 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11
[INFO] 2023-11-08 14:52:09,949 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,950 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,950 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 20
[INFO] 2023-11-08 14:52:09,951 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 23
[INFO] 2023-11-08 14:52:09,951 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:09 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 24
[INFO] 2023-11-08 14:52:09,956 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,957 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,957 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0
[INFO] 2023-11-08 14:52:09,957 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,957 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,963 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,964 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,964 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3
[INFO] 2023-11-08 14:52:09,964 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,964 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,968 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,969 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,969 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2
[INFO] 2023-11-08 14:52:09,969 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,969 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,973 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,973 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,974 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5
[INFO] 2023-11-08 14:52:09,974 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,974 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,983 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,983 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,983 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6
[INFO] 2023-11-08 14:52:09,984 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,984 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,989 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,990 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,990 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9
[INFO] 2023-11-08 14:52:09,990 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,990 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,994 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:09,994 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:09,994 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8
[INFO] 2023-11-08 14:52:09,994 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:09,995 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:09,995 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9)
[INFO] 2023-11-08 14:52:09,995 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 53 for 8 partitions
[INFO] 2023-11-08 14:52:09,996 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,996 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:09,997 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 149ms correlationId 53 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:09,999 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 54
[INFO] 2023-11-08 14:52:10,001 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 55 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:10,188 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5)
[INFO] 2023-11-08 14:52:10,188 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 55 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[TRACE] 2023-11-08 14:52:10,195 [qtp1527668063-248] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 248]
[INFO] 2023-11-08 14:52:10,196 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,196 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,197 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2
[INFO] 2023-11-08 14:52:10,197 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,197 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[TRACE] 2023-11-08 14:52:10,198 [qtp1527668063-248] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user connect with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=connect,dc=test,dc=com [Thread 248]
[DEBUG] 2023-11-08 14:52:10,200 [qtp1527668063-248] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for connect
[INFO] 2023-11-08 14:52:10,202 [qtp1527668063-248] io.confluent.mds.request.logger log - 59 * Server has received a request on thread qtp1527668063-248null59 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull59 > User Principal: connect
59 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null59 > Cache-Control: no-cachenull59 > Connection: keep-alivenull59 > Content-Type: application/jsonnull59 > Host: kafka.confluent.svc.cluster.local:8090null59 > Pragma: no-cachenull59 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:10,204 [qtp1527668063-248] io.confluent.mds.request.logger log - 59 * Server responded with a response on thread qtp1527668063-248null59 < 200null59 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:52:10,205 [qtp1527668063-248] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:10 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:10,205 [qtp1527668063-248] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:10 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:52:10,206 [qtp1527668063-248] io.confluent.rest-utils.requests write - 10.40.0.14 - connect [08/Nov/2023:14:52:10 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 11
[INFO] 2023-11-08 14:52:10,207 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,208 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,208 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3
[INFO] 2023-11-08 14:52:10,208 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,208 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,212 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,212 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,213 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6
[INFO] 2023-11-08 14:52:10,213 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,213 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,216 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,216 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,217 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5
[INFO] 2023-11-08 14:52:10,217 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,217 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,221 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,222 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,222 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0
[INFO] 2023-11-08 14:52:10,222 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,222 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,225 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,226 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,226 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11
[INFO] 2023-11-08 14:52:10,226 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,226 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,230 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,231 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,231 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8
[INFO] 2023-11-08 14:52:10,231 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,231 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,239 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,240 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,240 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9
[INFO] 2023-11-08 14:52:10,240 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,240 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,246 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,246 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,247 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3
[INFO] 2023-11-08 14:52:10,247 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,247 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,256 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,257 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,257 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5
[INFO] 2023-11-08 14:52:10,257 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,257 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,264 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,265 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,265 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1
[INFO] 2023-11-08 14:52:10,265 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,265 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,270 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,271 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,271 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1
[INFO] 2023-11-08 14:52:10,271 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,272 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,278 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,279 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,279 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10
[INFO] 2023-11-08 14:52:10,279 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,279 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,287 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,288 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,296 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8
[INFO] 2023-11-08 14:52:10,297 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,297 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,303 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,304 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,305 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6
[INFO] 2023-11-08 14:52:10,305 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,305 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,310 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,311 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,311 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10
[INFO] 2023-11-08 14:52:10,311 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,311 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,315 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,315 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,316 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4
[INFO] 2023-11-08 14:52:10,316 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,316 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,324 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,324 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,324 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4
[INFO] 2023-11-08 14:52:10,325 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,325 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,328 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,329 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,329 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2
[INFO] 2023-11-08 14:52:10,329 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,329 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,332 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,332 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,332 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0
[INFO] 2023-11-08 14:52:10,332 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,332 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,336 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,337 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,337 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11
[INFO] 2023-11-08 14:52:10,337 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,337 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,340 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,341 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:10,341 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7
[INFO] 2023-11-08 14:52:10,341 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,341 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,344 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,346 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,346 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7
[INFO] 2023-11-08 14:52:10,346 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,346 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,349 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,350 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,350 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9
[INFO] 2023-11-08 14:52:10,350 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,350 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,350 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11, _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10, _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9)
[INFO] 2023-11-08 14:52:10,350 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 55 for 16 partitions
[INFO] 2023-11-08 14:52:10,352 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,352 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,353 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 352ms correlationId 55 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:10,356 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 56
[INFO] 2023-11-08 14:52:10,360 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 57 from controller 1 for 36 partitions
[INFO] 2023-11-08 14:52:10,385 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9)
[INFO] 2023-11-08 14:52:10,386 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 57 from controller 1 epoch 1 as part of the become-leader transition for 12 partitions
[INFO] 2023-11-08 14:52:10,396 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,397 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,400 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,400 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,400 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,400 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,400 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,400 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,401 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,402 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9
[INFO] 2023-11-08 14:52:10,403 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,403 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,405 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,406 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,406 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,406 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,406 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,414 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,415 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,415 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8
[INFO] 2023-11-08 14:52:10,415 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,415 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,423 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,423 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,424 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6
[INFO] 2023-11-08 14:52:10,424 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,424 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,429 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,429 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,429 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,429 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,429 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,429 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,429 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,429 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,430 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,430 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,430 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,430 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,430 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,430 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,430 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,430 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,430 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,430 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,431 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,431 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,431 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,431 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,431 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,431 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,431 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,431 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-AlertHistoryStore-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,431 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9
[INFO] 2023-11-08 14:52:10,432 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,432 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,439 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,440 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,440 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3
[INFO] 2023-11-08 14:52:10,440 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,441 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,444 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,445 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,445 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5
[INFO] 2023-11-08 14:52:10,445 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,445 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,450 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,450 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,451 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0
[INFO] 2023-11-08 14:52:10,451 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,451 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,455 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,455 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,456 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11
[INFO] 2023-11-08 14:52:10,456 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,456 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,459 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,460 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,460 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3
[INFO] 2023-11-08 14:52:10,460 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,460 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,464 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,464 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,465 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0
[INFO] 2023-11-08 14:52:10,465 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,465 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,468 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,468 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,469 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2
[INFO] 2023-11-08 14:52:10,469 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,469 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,472 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,473 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,473 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6
[INFO] 2023-11-08 14:52:10,473 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,473 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,477 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,477 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,477 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7
[INFO] 2023-11-08 14:52:10,478 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,478 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,481 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,481 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,481 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2
[INFO] 2023-11-08 14:52:10,481 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,482 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,485 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,485 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,485 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11
[INFO] 2023-11-08 14:52:10,485 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,486 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,489 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,489 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,489 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1
[INFO] 2023-11-08 14:52:10,489 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,489 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,492 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,493 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,493 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1
[INFO] 2023-11-08 14:52:10,493 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,493 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,496 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,496 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,496 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8
[INFO] 2023-11-08 14:52:10,496 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,497 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,500 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,500 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,501 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5
[INFO] 2023-11-08 14:52:10,501 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,501 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,504 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,504 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,505 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0
[INFO] 2023-11-08 14:52:10,505 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,505 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,508 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,508 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,508 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4
[INFO] 2023-11-08 14:52:10,508 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,508 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,511 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,512 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,512 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4
[INFO] 2023-11-08 14:52:10,512 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,512 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,515 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,516 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,516 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10
[INFO] 2023-11-08 14:52:10,516 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,516 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,519 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,519 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,519 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4
[INFO] 2023-11-08 14:52:10,520 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,520 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,526 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,527 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,527 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8
[INFO] 2023-11-08 14:52:10,527 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,527 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,530 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,531 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,531 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3
[INFO] 2023-11-08 14:52:10,531 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,531 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,534 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,535 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,535 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1
[INFO] 2023-11-08 14:52:10,535 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,535 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,538 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,538 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,538 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11
[INFO] 2023-11-08 14:52:10,538 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,538 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,543 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,544 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,544 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7
[INFO] 2023-11-08 14:52:10,544 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,544 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,548 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,549 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,549 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5
[INFO] 2023-11-08 14:52:10,549 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,549 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,552 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,553 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,553 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9
[INFO] 2023-11-08 14:52:10,553 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,553 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,557 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,558 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,558 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10
[INFO] 2023-11-08 14:52:10,558 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,558 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,562 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,563 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,563 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7
[INFO] 2023-11-08 14:52:10,563 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,563 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,567 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,568 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,568 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10
[INFO] 2023-11-08 14:52:10,568 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,568 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,572 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,573 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,573 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2
[INFO] 2023-11-08 14:52:10,573 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,573 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,576 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,577 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,577 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6
[INFO] 2023-11-08 14:52:10,577 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,578 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,578 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2, _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0)
[INFO] 2023-11-08 14:52:10,578 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 57 for 24 partitions
[INFO] 2023-11-08 14:52:10,580 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,580 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,581 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 221ms correlationId 57 from controller 1 for 36 partitions
[INFO] 2023-11-08 14:52:10,584 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Add 36 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 58
[INFO] 2023-11-08 14:52:10,592 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 59 from controller 1 for 36 partitions
[INFO] 2023-11-08 14:52:10,611 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9)
[INFO] 2023-11-08 14:52:10,611 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 59 from controller 1 epoch 1 as part of the become-leader transition for 12 partitions
[INFO] 2023-11-08 14:52:10,614 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,615 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,616 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9
[INFO] 2023-11-08 14:52:10,616 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,616 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,619 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,619 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,620 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2
[INFO] 2023-11-08 14:52:10,620 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,620 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,622 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,623 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,623 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3
[INFO] 2023-11-08 14:52:10,623 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,623 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,626 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,627 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,627 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0
[INFO] 2023-11-08 14:52:10,627 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,627 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,631 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,631 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,631 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4
[INFO] 2023-11-08 14:52:10,632 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,632 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[WARN] 2023-11-08 14:52:10,634 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-2. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:10,634 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[WARN] 2023-11-08 14:52:10,635 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-11. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:10,635 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-5. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:10,635 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MetricsAggregateStore-repartition-8. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:10,635 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,635 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,635 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,635 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8
[INFO] 2023-11-08 14:52:10,635 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,635 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,636 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,636 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,637 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,637 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,637 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,637 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,637 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,637 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,637 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,639 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,639 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,639 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6
[INFO] 2023-11-08 14:52:10,639 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,639 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,643 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,644 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,644 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10
[INFO] 2023-11-08 14:52:10,644 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,644 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,648 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,648 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,649 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5
[INFO] 2023-11-08 14:52:10,649 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,649 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,652 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,653 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,653 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1
[INFO] 2023-11-08 14:52:10,653 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,653 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,656 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,657 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,657 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7
[INFO] 2023-11-08 14:52:10,657 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,657 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,660 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,660 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,661 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11
[INFO] 2023-11-08 14:52:10,661 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,661 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,664 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,664 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,664 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9
[INFO] 2023-11-08 14:52:10,665 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,665 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,668 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,668 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,668 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5
[INFO] 2023-11-08 14:52:10,668 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,669 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,672 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,672 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,673 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6
[INFO] 2023-11-08 14:52:10,673 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,673 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,676 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,676 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,676 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2
[INFO] 2023-11-08 14:52:10,676 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,676 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,682 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,683 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,683 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10
[INFO] 2023-11-08 14:52:10,683 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,683 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,686 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,687 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,687 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6
[INFO] 2023-11-08 14:52:10,687 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,687 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,690 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,690 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,690 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1
[INFO] 2023-11-08 14:52:10,691 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,691 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,694 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,694 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,694 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2
[INFO] 2023-11-08 14:52:10,694 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,694 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,697 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,698 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,698 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9
[INFO] 2023-11-08 14:52:10,698 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,698 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,701 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,702 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,702 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5
[INFO] 2023-11-08 14:52:10,702 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,702 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,707 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,708 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,708 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10
[INFO] 2023-11-08 14:52:10,708 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,708 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,712 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,712 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,712 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11
[INFO] 2023-11-08 14:52:10,712 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,713 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,716 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,717 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,717 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7
[INFO] 2023-11-08 14:52:10,717 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,717 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,724 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,724 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,724 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0
[INFO] 2023-11-08 14:52:10,725 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,725 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,728 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,728 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,728 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0
[INFO] 2023-11-08 14:52:10,729 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,729 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,731 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,732 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,732 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4
[INFO] 2023-11-08 14:52:10,732 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,732 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,736 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,736 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,736 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8
[INFO] 2023-11-08 14:52:10,736 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,736 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,743 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,744 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,744 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11
[INFO] 2023-11-08 14:52:10,744 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,744 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,749 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,750 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,750 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8
[INFO] 2023-11-08 14:52:10,750 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,750 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,755 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,755 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,756 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4
[INFO] 2023-11-08 14:52:10,756 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,756 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,768 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,769 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,769 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3
[INFO] 2023-11-08 14:52:10,769 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,769 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,773 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,774 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,774 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1
[INFO] 2023-11-08 14:52:10,774 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,774 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,778 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,779 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,779 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3
[INFO] 2023-11-08 14:52:10,779 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,780 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,783 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,784 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,784 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7
[INFO] 2023-11-08 14:52:10,784 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,784 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,785 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7)
[INFO] 2023-11-08 14:52:10,785 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 59 for 24 partitions
[INFO] 2023-11-08 14:52:10,789 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,789 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,790 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 199ms correlationId 59 from controller 1 for 36 partitions
[INFO] 2023-11-08 14:52:10,792 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Add 36 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 60
[INFO] 2023-11-08 14:52:10,794 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 61 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:10,798 [data-plane-kafka-request-handler-2] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Dynamic member with unknown member id joins group confluent.connect in PreparingRebalance state. Created a new member id connect-1-e1593b4b-d301-4048-8dcd-aaee32fc2401 and request the member to rejoin with this id.
[INFO] 2023-11-08 14:52:10,817 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3)
[INFO] 2023-11-08 14:52:10,818 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 61 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[INFO] 2023-11-08 14:52:10,821 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,822 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,823 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9
[INFO] 2023-11-08 14:52:10,823 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,823 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,826 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,827 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,827 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6
[INFO] 2023-11-08 14:52:10,827 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,827 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,831 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,832 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,832 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10
[INFO] 2023-11-08 14:52:10,832 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,832 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,841 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,841 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,842 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3
[INFO] 2023-11-08 14:52:10,842 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,842 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,850 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,850 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,850 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0
[INFO] 2023-11-08 14:52:10,851 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,851 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,855 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,856 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,856 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7
[INFO] 2023-11-08 14:52:10,856 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,856 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,861 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,861 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,862 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1
[INFO] 2023-11-08 14:52:10,862 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,862 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,865 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,866 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,866 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4
[INFO] 2023-11-08 14:52:10,866 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,866 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,870 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,871 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,871 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10
[INFO] 2023-11-08 14:52:10,871 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,871 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,875 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,876 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,876 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8
[INFO] 2023-11-08 14:52:10,876 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,876 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,880 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,880 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,880 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4
[INFO] 2023-11-08 14:52:10,881 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,881 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,884 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,884 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,885 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2
[INFO] 2023-11-08 14:52:10,885 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,885 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,888 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,888 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,888 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6
[INFO] 2023-11-08 14:52:10,888 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,888 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,892 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,892 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,892 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8
[INFO] 2023-11-08 14:52:10,892 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,892 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,896 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,896 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,896 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2
[INFO] 2023-11-08 14:52:10,896 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,897 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,903 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,904 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,904 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11
[INFO] 2023-11-08 14:52:10,904 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,905 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,909 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,910 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,910 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,910 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0
[INFO] 2023-11-08 14:52:10,910 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,910 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,910 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,911 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,911 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,911 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,911 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,911 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,911 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,911 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,911 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,911 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,911 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,912 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,913 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,914 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,914 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,914 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-aggregate-rekey-store-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,914 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,914 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,914 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:10,914 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:10,917 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,918 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,918 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9
[INFO] 2023-11-08 14:52:10,918 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,918 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,922 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,923 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,923 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7
[INFO] 2023-11-08 14:52:10,923 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,923 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,926 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,927 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,927 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11
[INFO] 2023-11-08 14:52:10,927 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,927 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,930 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,931 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,931 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5
[INFO] 2023-11-08 14:52:10,931 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,931 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,934 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,935 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,935 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5
[INFO] 2023-11-08 14:52:10,935 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,935 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,938 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,939 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,939 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1
[INFO] 2023-11-08 14:52:10,939 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,939 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,942 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,943 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,943 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3
[INFO] 2023-11-08 14:52:10,943 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,943 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,943 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9, _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11)
[INFO] 2023-11-08 14:52:10,943 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 61 for 16 partitions
[INFO] 2023-11-08 14:52:10,945 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,945 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:10,946 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 152ms correlationId 61 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:10,948 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 62
[INFO] 2023-11-08 14:52:10,950 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 63 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:10,964 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9)
[INFO] 2023-11-08 14:52:10,964 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 63 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[INFO] 2023-11-08 14:52:10,968 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,968 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,969 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11
[INFO] 2023-11-08 14:52:10,969 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,969 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,973 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,974 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,974 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6
[INFO] 2023-11-08 14:52:10,974 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,974 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,978 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,978 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,978 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9
[INFO] 2023-11-08 14:52:10,979 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,979 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,982 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,983 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,983 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8
[INFO] 2023-11-08 14:52:10,983 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,983 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,987 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,987 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,987 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2
[INFO] 2023-11-08 14:52:10,988 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,988 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,991 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,992 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:10,992 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5
[INFO] 2023-11-08 14:52:10,992 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,992 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:10,997 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:10,997 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:10,997 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0
[INFO] 2023-11-08 14:52:10,997 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:10,998 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,003 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,006 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,007 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3
[INFO] 2023-11-08 14:52:11,007 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,007 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,014 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,015 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,016 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4
[INFO] 2023-11-08 14:52:11,016 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,016 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,029 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,030 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,030 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8
[INFO] 2023-11-08 14:52:11,030 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,030 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,039 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,040 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,040 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7
[INFO] 2023-11-08 14:52:11,041 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,041 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,045 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,046 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,047 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10
[INFO] 2023-11-08 14:52:11,047 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,047 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,052 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,053 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,053 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9
[INFO] 2023-11-08 14:52:11,053 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,053 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,057 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,057 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,058 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4
[INFO] 2023-11-08 14:52:11,058 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,058 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,061 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,062 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,062 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2
[INFO] 2023-11-08 14:52:11,062 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,062 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,067 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,068 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,068 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0
[INFO] 2023-11-08 14:52:11,068 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,068 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,071 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,071 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,072 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5
[INFO] 2023-11-08 14:52:11,072 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,072 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,078 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,079 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,079 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10
[INFO] 2023-11-08 14:52:11,079 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,079 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,085 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,086 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,086 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7
[INFO] 2023-11-08 14:52:11,086 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,086 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,090 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,091 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,091 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6
[INFO] 2023-11-08 14:52:11,091 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,091 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,094 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,095 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,095 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11
[INFO] 2023-11-08 14:52:11,095 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,095 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,099 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,099 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,099 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3
[INFO] 2023-11-08 14:52:11,099 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,099 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,106 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,107 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,107 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1
[INFO] 2023-11-08 14:52:11,107 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,107 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,110 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,111 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,111 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1
[INFO] 2023-11-08 14:52:11,111 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,111 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,111 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7)
[INFO] 2023-11-08 14:52:11,111 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 63 for 16 partitions
[INFO] 2023-11-08 14:52:11,113 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,113 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,114 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 164ms correlationId 63 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:11,120 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 64
[INFO] 2023-11-08 14:52:11,126 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 65 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:11,136 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,136 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,137 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,138 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,139 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11)
[INFO] 2023-11-08 14:52:11,139 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 65 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[WARN] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-8. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-5. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-11. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-repartition-2. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-5. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-11. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-2. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-changelog-8. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,142 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,143 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregatedTopicPartitionTableWindows-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,144 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,145 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,146 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,147 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2
[INFO] 2023-11-08 14:52:11,147 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,147 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,152 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,152 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,152 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11
[INFO] 2023-11-08 14:52:11,152 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,153 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,156 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,157 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,157 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8
[INFO] 2023-11-08 14:52:11,157 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,157 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,161 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,162 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,162 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8
[INFO] 2023-11-08 14:52:11,162 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,162 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,166 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,166 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,166 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11
[INFO] 2023-11-08 14:52:11,166 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,167 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,170 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,171 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,171 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5
[INFO] 2023-11-08 14:52:11,171 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,171 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,174 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,175 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,175 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2
[INFO] 2023-11-08 14:52:11,175 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,175 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,179 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,180 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,180 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5
[INFO] 2023-11-08 14:52:11,180 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,180 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,184 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,184 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,184 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1
[INFO] 2023-11-08 14:52:11,184 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,184 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,188 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,188 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,188 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3
[INFO] 2023-11-08 14:52:11,188 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,189 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,192 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,192 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,192 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9
[INFO] 2023-11-08 14:52:11,193 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,193 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,196 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,196 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,197 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9
[INFO] 2023-11-08 14:52:11,197 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,197 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,200 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,200 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,200 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7
[INFO] 2023-11-08 14:52:11,201 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,201 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,204 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,204 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,204 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3
[INFO] 2023-11-08 14:52:11,204 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,204 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,208 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,208 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,209 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7
[INFO] 2023-11-08 14:52:11,209 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,209 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,212 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,213 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,213 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1
[INFO] 2023-11-08 14:52:11,213 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,213 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,219 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,220 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,220 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0
[INFO] 2023-11-08 14:52:11,220 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,220 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,225 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,226 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,226 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10
[INFO] 2023-11-08 14:52:11,226 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,226 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,230 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,230 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,230 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6
[INFO] 2023-11-08 14:52:11,230 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,231 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,236 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,236 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,236 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4
[INFO] 2023-11-08 14:52:11,236 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,237 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,240 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,241 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,241 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10
[INFO] 2023-11-08 14:52:11,241 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,241 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,250 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,251 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,251 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4
[INFO] 2023-11-08 14:52:11,251 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,251 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,254 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,255 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,255 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0
[INFO] 2023-11-08 14:52:11,255 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,255 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,260 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,260 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,260 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6
[INFO] 2023-11-08 14:52:11,260 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,261 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,261 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3, _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1)
[INFO] 2023-11-08 14:52:11,261 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 65 for 16 partitions
[INFO] 2023-11-08 14:52:11,261 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,262 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,262 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 136ms correlationId 65 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:11,405 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 66
[INFO] 2023-11-08 14:52:11,410 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 67 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:11,422 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6)
[INFO] 2023-11-08 14:52:11,422 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 67 from controller 1 epoch 1 as part of the become-leader transition for 8 partitions
[INFO] 2023-11-08 14:52:11,427 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,427 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,428 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1
[INFO] 2023-11-08 14:52:11,428 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,429 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,435 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,436 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,436 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0
[INFO] 2023-11-08 14:52:11,436 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,436 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,439 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,440 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,440 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6
[INFO] 2023-11-08 14:52:11,440 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,440 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,444 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,444 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,444 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10
[INFO] 2023-11-08 14:52:11,444 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,444 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,448 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,449 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,449 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7
[INFO] 2023-11-08 14:52:11,449 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,449 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,465 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,466 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,466 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9
[INFO] 2023-11-08 14:52:11,466 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,466 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,469 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,470 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,470 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3
[INFO] 2023-11-08 14:52:11,470 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,470 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,473 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,473 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,474 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4
[INFO] 2023-11-08 14:52:11,474 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,474 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,476 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,477 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,477 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2
[INFO] 2023-11-08 14:52:11,477 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,477 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,480 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,480 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,480 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0
[INFO] 2023-11-08 14:52:11,480 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,480 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,483 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,483 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,484 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11
[INFO] 2023-11-08 14:52:11,484 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,484 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,486 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,486 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,487 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11
[INFO] 2023-11-08 14:52:11,487 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,487 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,489 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,489 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,489 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8
[INFO] 2023-11-08 14:52:11,489 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,489 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,491 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,492 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,492 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8
[INFO] 2023-11-08 14:52:11,492 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,492 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,494 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,494 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,495 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2
[INFO] 2023-11-08 14:52:11,495 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,495 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,497 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,497 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,497 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6
[INFO] 2023-11-08 14:52:11,497 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,497 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,500 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,500 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,500 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4
[INFO] 2023-11-08 14:52:11,500 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,500 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,503 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,503 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,503 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1
[INFO] 2023-11-08 14:52:11,504 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,504 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,506 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,506 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,507 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10
[INFO] 2023-11-08 14:52:11,507 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,507 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,509 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,510 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,510 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9
[INFO] 2023-11-08 14:52:11,510 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,510 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,512 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,512 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,513 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7
[INFO] 2023-11-08 14:52:11,513 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,513 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,515 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,516 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,516 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5
[INFO] 2023-11-08 14:52:11,516 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,516 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,518 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,518 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,518 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3
[INFO] 2023-11-08 14:52:11,518 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,518 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,520 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,521 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,521 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5
[INFO] 2023-11-08 14:52:11,521 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,521 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,521 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11, _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10)
[INFO] 2023-11-08 14:52:11,521 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 67 for 16 partitions
[INFO] 2023-11-08 14:52:11,522 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,522 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,523 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 113ms correlationId 67 from controller 1 for 24 partitions
[INFO] 2023-11-08 14:52:11,524 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Add 24 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 68
[INFO] 2023-11-08 14:52:11,526 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 69 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,533 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10)
[INFO] 2023-11-08 14:52:11,533 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 69 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,537 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,537 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,538 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7
[INFO] 2023-11-08 14:52:11,538 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,539 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,542 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,543 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,543 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1
[INFO] 2023-11-08 14:52:11,543 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,543 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,546 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,547 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,547 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4
[INFO] 2023-11-08 14:52:11,547 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,548 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,551 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,552 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,552 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10
[INFO] 2023-11-08 14:52:11,552 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,552 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,557 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,558 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,558 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5
[INFO] 2023-11-08 14:52:11,558 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,558 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,562 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,562 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,563 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6
[INFO] 2023-11-08 14:52:11,563 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,563 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,566 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,566 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,566 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8
[INFO] 2023-11-08 14:52:11,567 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,567 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,570 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,570 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,570 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2
[INFO] 2023-11-08 14:52:11,571 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,571 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,573 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,574 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,574 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3
[INFO] 2023-11-08 14:52:11,574 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,574 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,577 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,577 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,577 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0
[INFO] 2023-11-08 14:52:11,577 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,577 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,580 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,580 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,581 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9
[INFO] 2023-11-08 14:52:11,581 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,581 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,583 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,583 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=67108864}
[INFO] 2023-11-08 14:52:11,584 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11
[INFO] 2023-11-08 14:52:11,584 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,584 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,584 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8, _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6)
[INFO] 2023-11-08 14:52:11,584 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 69 for 8 partitions
[INFO] 2023-11-08 14:52:11,585 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,585 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,586 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 60ms correlationId 69 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,588 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 70
[INFO] 2023-11-08 14:52:11,589 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 71 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,594 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10)
[INFO] 2023-11-08 14:52:11,595 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 71 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,597 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,598 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,599 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7
[INFO] 2023-11-08 14:52:11,599 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,599 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,603 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,604 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,604 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4
[INFO] 2023-11-08 14:52:11,604 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,604 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,607 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,607 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,607 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1
[INFO] 2023-11-08 14:52:11,607 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,607 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,610 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,611 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,611 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10
[INFO] 2023-11-08 14:52:11,611 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,611 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,614 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,614 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,614 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8
[INFO] 2023-11-08 14:52:11,614 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,614 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,621 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,622 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,622 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5
[INFO] 2023-11-08 14:52:11,622 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,622 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,625 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,626 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,626 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6
[INFO] 2023-11-08 14:52:11,626 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,626 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,630 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,630 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,630 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3
[INFO] 2023-11-08 14:52:11,631 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,631 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,633 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,634 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,634 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2
[INFO] 2023-11-08 14:52:11,634 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,634 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,636 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,637 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,637 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0
[INFO] 2023-11-08 14:52:11,637 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,637 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,639 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,640 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,640 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11
[INFO] 2023-11-08 14:52:11,640 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,640 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,642 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,643 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,643 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9
[INFO] 2023-11-08 14:52:11,643 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,643 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,644 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,645 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,646 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,647 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,647 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,647 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,647 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,647 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,647 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,647 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,647 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,647 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,648 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9, _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8)
[INFO] 2023-11-08 14:52:11,648 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 71 for 8 partitions
[INFO] 2023-11-08 14:52:11,649 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,649 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,650 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 61ms correlationId 71 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,651 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 72
[INFO] 2023-11-08 14:52:11,653 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 73 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,662 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,662 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,663 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,664 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,664 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,664 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,664 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,664 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,664 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,664 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,664 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringVerifierStore-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-aggregate-topic-partition-store-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:11,665 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-group-aggregate-store-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:11,666 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6)
[INFO] 2023-11-08 14:52:11,666 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 73 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,670 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,671 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,672 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9
[INFO] 2023-11-08 14:52:11,672 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,672 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,676 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,677 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,677 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0
[INFO] 2023-11-08 14:52:11,677 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,677 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[WARN] 2023-11-08 14:52:11,681 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-3. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,681 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-0. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,681 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-9. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[WARN] 2023-11-08 14:52:11,681 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread warn - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Received UNKNOWN_TOPIC_OR_PARTITION from the leader for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-6. This error may be returned transiently when the partition is being created or deleted, but it is not expected to persist.
[INFO] 2023-11-08 14:52:11,684 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,685 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,685 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3
[INFO] 2023-11-08 14:52:11,685 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,686 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,689 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,690 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,690 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6
[INFO] 2023-11-08 14:52:11,690 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,690 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,694 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,695 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,695 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8
[INFO] 2023-11-08 14:52:11,695 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,695 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,698 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,699 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,699 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10
[INFO] 2023-11-08 14:52:11,699 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,699 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,703 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,703 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,704 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11
[INFO] 2023-11-08 14:52:11,704 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,704 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,706 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,707 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,707 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1
[INFO] 2023-11-08 14:52:11,707 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,707 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,711 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,712 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,712 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2
[INFO] 2023-11-08 14:52:11,712 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,712 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,715 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,716 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,716 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4
[INFO] 2023-11-08 14:52:11,716 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,716 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,719 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,720 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,720 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5
[INFO] 2023-11-08 14:52:11,720 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,720 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,723 [data-plane-kafka-request-handler-4] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,724 [data-plane-kafka-request-handler-4] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,724 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7
[INFO] 2023-11-08 14:52:11,724 [data-plane-kafka-request-handler-4] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,724 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,724 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11)
[INFO] 2023-11-08 14:52:11,724 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 73 for 8 partitions
[INFO] 2023-11-08 14:52:11,725 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,725 [data-plane-kafka-request-handler-4] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,726 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 73ms correlationId 73 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,728 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 74
[INFO] 2023-11-08 14:52:11,730 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 75 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,738 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4)
[INFO] 2023-11-08 14:52:11,738 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 75 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,741 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,742 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,743 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7
[INFO] 2023-11-08 14:52:11,743 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,743 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,746 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,747 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,747 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10
[INFO] 2023-11-08 14:52:11,747 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,747 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,750 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,750 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,750 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1
[INFO] 2023-11-08 14:52:11,750 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,750 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,753 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,754 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,754 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4
[INFO] 2023-11-08 14:52:11,754 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,754 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,757 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,757 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,757 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6
[INFO] 2023-11-08 14:52:11,757 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,758 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,761 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,761 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,761 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9
[INFO] 2023-11-08 14:52:11,761 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,761 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,764 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,765 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,765 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8
[INFO] 2023-11-08 14:52:11,765 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,765 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,768 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,769 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,769 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11
[INFO] 2023-11-08 14:52:11,769 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,769 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,772 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,772 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,772 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0
[INFO] 2023-11-08 14:52:11,772 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,772 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,777 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,778 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,778 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3
[INFO] 2023-11-08 14:52:11,778 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,778 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,782 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,782 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,782 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2
[INFO] 2023-11-08 14:52:11,782 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,783 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,791 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,792 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,792 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5
[INFO] 2023-11-08 14:52:11,792 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,792 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,793 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0)
[INFO] 2023-11-08 14:52:11,793 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 75 for 8 partitions
[INFO] 2023-11-08 14:52:11,794 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,794 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,795 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 65ms correlationId 75 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,798 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 76
[INFO] 2023-11-08 14:52:11,802 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 77 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,809 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4)
[INFO] 2023-11-08 14:52:11,809 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 77 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,813 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,814 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,815 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7
[INFO] 2023-11-08 14:52:11,815 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,815 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,818 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,819 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,819 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10
[INFO] 2023-11-08 14:52:11,819 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,820 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,822 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,823 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,823 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1
[INFO] 2023-11-08 14:52:11,823 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,823 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,826 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,827 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,827 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4
[INFO] 2023-11-08 14:52:11,827 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,827 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,830 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,830 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,831 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6
[INFO] 2023-11-08 14:52:11,831 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,831 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,834 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,834 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,834 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5
[INFO] 2023-11-08 14:52:11,834 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,834 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,837 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,838 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,838 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8
[INFO] 2023-11-08 14:52:11,838 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,838 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,841 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,841 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,841 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9
[INFO] 2023-11-08 14:52:11,842 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,842 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,844 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,845 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,845 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11
[INFO] 2023-11-08 14:52:11,845 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,845 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,849 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,849 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,849 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0
[INFO] 2023-11-08 14:52:11,849 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,849 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,852 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,853 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,853 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2
[INFO] 2023-11-08 14:52:11,853 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,853 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,856 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,857 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,857 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3
[INFO] 2023-11-08 14:52:11,857 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,857 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,857 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3, _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0)
[INFO] 2023-11-08 14:52:11,858 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 77 for 8 partitions
[INFO] 2023-11-08 14:52:11,858 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,858 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,859 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 57ms correlationId 77 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,864 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 78
[INFO] 2023-11-08 14:52:11,887 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 79 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,892 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3)
[INFO] 2023-11-08 14:52:11,892 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 79 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,896 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,896 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,897 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6
[INFO] 2023-11-08 14:52:11,897 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,897 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,901 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,901 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,902 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9
[INFO] 2023-11-08 14:52:11,902 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,902 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,905 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,906 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,906 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0
[INFO] 2023-11-08 14:52:11,906 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,906 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,910 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,911 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,911 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3
[INFO] 2023-11-08 14:52:11,911 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,911 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,914 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,915 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,915 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5
[INFO] 2023-11-08 14:52:11,915 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,915 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,919 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,920 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,920 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8
[INFO] 2023-11-08 14:52:11,920 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,920 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,923 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,924 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,924 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7
[INFO] 2023-11-08 14:52:11,924 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,924 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,927 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,928 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,928 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10
[INFO] 2023-11-08 14:52:11,928 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,928 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,930 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,931 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,931 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11
[INFO] 2023-11-08 14:52:11,931 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,931 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,934 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,934 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,934 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2
[INFO] 2023-11-08 14:52:11,934 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,934 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,937 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,938 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,938 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1
[INFO] 2023-11-08 14:52:11,938 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,938 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,941 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,941 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:11,941 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4
[INFO] 2023-11-08 14:52:11,942 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,942 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,942 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2, _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1)
[INFO] 2023-11-08 14:52:11,942 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 79 for 8 partitions
[INFO] 2023-11-08 14:52:11,943 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,943 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:11,944 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 57ms correlationId 79 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,946 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 80
[INFO] 2023-11-08 14:52:11,987 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 81 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:11,992 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7)
[INFO] 2023-11-08 14:52:11,992 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 81 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:11,995 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:11,996 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:11,996 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1
[INFO] 2023-11-08 14:52:11,996 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:11,996 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:11,999 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,000 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,000 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4
[INFO] 2023-11-08 14:52:12,000 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,000 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,003 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,003 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,003 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10
[INFO] 2023-11-08 14:52:12,003 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,003 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,006 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,006 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,007 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7
[INFO] 2023-11-08 14:52:12,007 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,007 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,010 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,011 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,011 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2
[INFO] 2023-11-08 14:52:12,011 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,011 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,014 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,014 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,014 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3
[INFO] 2023-11-08 14:52:12,014 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,014 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,017 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,018 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,018 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0
[INFO] 2023-11-08 14:52:12,018 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,018 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,020 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,021 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,021 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9
[INFO] 2023-11-08 14:52:12,021 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,021 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,024 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,024 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,025 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11
[INFO] 2023-11-08 14:52:12,025 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,025 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,027 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,028 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,028 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5
[INFO] 2023-11-08 14:52:12,028 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,028 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,030 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,031 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,031 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6
[INFO] 2023-11-08 14:52:12,031 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,031 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,033 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,034 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,034 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8
[INFO] 2023-11-08 14:52:12,034 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,034 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,034 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6, _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11)
[INFO] 2023-11-08 14:52:12,034 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 81 for 8 partitions
[INFO] 2023-11-08 14:52:12,035 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,035 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,036 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 49ms correlationId 81 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,038 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 82
[INFO] 2023-11-08 14:52:12,067 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 83 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,074 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8)
[INFO] 2023-11-08 14:52:12,074 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 83 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,078 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,079 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,080 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5
[INFO] 2023-11-08 14:52:12,080 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,080 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,084 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,084 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,084 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2
[INFO] 2023-11-08 14:52:12,085 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,085 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,088 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,088 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,088 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11
[INFO] 2023-11-08 14:52:12,088 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,089 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,092 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,092 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,092 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8
[INFO] 2023-11-08 14:52:12,092 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,092 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,114 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,115 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,115 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4
[INFO] 2023-11-08 14:52:12,115 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,115 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,118 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,118 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,118 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3
[INFO] 2023-11-08 14:52:12,118 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,118 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,121 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,121 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,122 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0
[INFO] 2023-11-08 14:52:12,122 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,122 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,124 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,125 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,125 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1
[INFO] 2023-11-08 14:52:12,125 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,125 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,127 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,128 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,128 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10
[INFO] 2023-11-08 14:52:12,128 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,128 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,130 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,131 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,131 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9
[INFO] 2023-11-08 14:52:12,131 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,131 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,134 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,134 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,134 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6
[INFO] 2023-11-08 14:52:12,134 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,134 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,136 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,137 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,137 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7
[INFO] 2023-11-08 14:52:12,137 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,137 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,137 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10, _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9)
[INFO] 2023-11-08 14:52:12,137 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 83 for 8 partitions
[INFO] 2023-11-08 14:52:12,138 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,138 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,139 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 72ms correlationId 83 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,141 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 84
[INFO] 2023-11-08 14:52:12,146 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 85 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,153 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11)
[INFO] 2023-11-08 14:52:12,154 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,154 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 85 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,154 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,154 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,154 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,154 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,154 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,154 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,155 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,156 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,157 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,157 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,158 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,159 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2
[INFO] 2023-11-08 14:52:12,159 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,159 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,162 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,162 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,162 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5
[INFO] 2023-11-08 14:52:12,163 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,163 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,166 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,167 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,167 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8
[INFO] 2023-11-08 14:52:12,167 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,167 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,170 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,170 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,171 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11
[INFO] 2023-11-08 14:52:12,171 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,171 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,174 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,174 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,174 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0
[INFO] 2023-11-08 14:52:12,174 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,175 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,177 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,177 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,178 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1
[INFO] 2023-11-08 14:52:12,178 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,178 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,180 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,181 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,181 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3
[INFO] 2023-11-08 14:52:12,181 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,181 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,183 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,184 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,184 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4
[INFO] 2023-11-08 14:52:12,184 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,184 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,185 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-monitoring-trigger-event-rekey-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringMessageAggregatorWindows-ONE_MINUTE-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,186 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,186 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,187 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringStream-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,187 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,187 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-KSTREAM-OUTEROTHER-0000000106-store-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,187 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,187 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6
[INFO] 2023-11-08 14:52:12,187 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,187 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,190 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,190 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,190 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7
[INFO] 2023-11-08 14:52:12,190 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,191 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,193 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,194 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,194 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9
[INFO] 2023-11-08 14:52:12,194 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,194 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,197 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,198 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,198 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10
[INFO] 2023-11-08 14:52:12,198 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,198 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,198 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7)
[INFO] 2023-11-08 14:52:12,198 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 85 for 8 partitions
[INFO] 2023-11-08 14:52:12,199 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,199 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,200 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 54ms correlationId 85 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,202 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 86
[INFO] 2023-11-08 14:52:12,232 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 87 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,238 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8)
[INFO] 2023-11-08 14:52:12,238 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 87 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,242 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,242 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,243 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5
[INFO] 2023-11-08 14:52:12,243 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,244 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,247 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,247 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,247 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2
[INFO] 2023-11-08 14:52:12,247 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,248 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,250 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,251 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,251 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11
[INFO] 2023-11-08 14:52:12,251 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,251 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,254 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,255 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,255 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8
[INFO] 2023-11-08 14:52:12,255 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,255 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,258 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,258 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,258 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4
[INFO] 2023-11-08 14:52:12,258 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,259 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,262 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,262 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,262 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3
[INFO] 2023-11-08 14:52:12,262 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,262 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,263 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,264 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,265 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,265 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,266 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6
[INFO] 2023-11-08 14:52:12,266 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,266 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,268 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,269 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,269 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0
[INFO] 2023-11-08 14:52:12,269 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,269 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,271 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,272 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,272 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1
[INFO] 2023-11-08 14:52:12,272 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,272 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,274 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,275 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,275 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7
[INFO] 2023-11-08 14:52:12,275 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,275 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,277 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,278 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,278 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10
[INFO] 2023-11-08 14:52:12,278 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,278 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,280 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,281 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=60566400000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=60566400000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,281 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9
[INFO] 2023-11-08 14:52:12,281 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,281 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,281 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10, _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9)
[INFO] 2023-11-08 14:52:12,281 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 87 for 8 partitions
[INFO] 2023-11-08 14:52:12,281 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,282 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,283 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 51ms correlationId 87 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,284 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 88
[INFO] 2023-11-08 14:52:12,321 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 89 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,327 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-monitoring-8, _confluent-monitoring-5, _confluent-monitoring-2, _confluent-monitoring-11)
[INFO] 2023-11-08 14:52:12,327 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 89 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,331 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,331 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-8 in /mnt/data/data0/logs/_confluent-monitoring-8 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,332 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-8 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-8
[INFO] 2023-11-08 14:52:12,332 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-8 broker=0] Log loaded for partition _confluent-monitoring-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,332 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-monitoring-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,335 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,335 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-5 in /mnt/data/data0/logs/_confluent-monitoring-5 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,335 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-5 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-5
[INFO] 2023-11-08 14:52:12,336 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-5 broker=0] Log loaded for partition _confluent-monitoring-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,336 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-monitoring-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,338 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,339 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-2 in /mnt/data/data0/logs/_confluent-monitoring-2 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,339 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-2 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-2
[INFO] 2023-11-08 14:52:12,339 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-2 broker=0] Log loaded for partition _confluent-monitoring-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,339 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-monitoring-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,341 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,342 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-11 in /mnt/data/data0/logs/_confluent-monitoring-11 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,342 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-11 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-11
[INFO] 2023-11-08 14:52:12,342 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-11 broker=0] Log loaded for partition _confluent-monitoring-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,342 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Leader _confluent-monitoring-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,345 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,345 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-7 in /mnt/data/data0/logs/_confluent-monitoring-7 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,346 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-7 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-7
[INFO] 2023-11-08 14:52:12,346 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-7 broker=0] Log loaded for partition _confluent-monitoring-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,346 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-monitoring-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,349 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,349 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-9 in /mnt/data/data0/logs/_confluent-monitoring-9 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,349 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-9 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-9
[INFO] 2023-11-08 14:52:12,349 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-9 broker=0] Log loaded for partition _confluent-monitoring-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,350 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-monitoring-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,352 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,353 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-10 in /mnt/data/data0/logs/_confluent-monitoring-10 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,353 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-10 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-10
[INFO] 2023-11-08 14:52:12,353 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-10 broker=0] Log loaded for partition _confluent-monitoring-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,353 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-monitoring-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,355 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,355 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-3 in /mnt/data/data0/logs/_confluent-monitoring-3 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,356 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-3 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-3
[INFO] 2023-11-08 14:52:12,356 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-3 broker=0] Log loaded for partition _confluent-monitoring-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,356 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-monitoring-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,358 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,358 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-4 in /mnt/data/data0/logs/_confluent-monitoring-4 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,358 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-4 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-4
[INFO] 2023-11-08 14:52:12,358 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-4 broker=0] Log loaded for partition _confluent-monitoring-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,358 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-monitoring-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,361 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,361 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-6 in /mnt/data/data0/logs/_confluent-monitoring-6 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,361 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-6 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-6
[INFO] 2023-11-08 14:52:12,361 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-6 broker=0] Log loaded for partition _confluent-monitoring-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,362 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-monitoring-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,364 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,364 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-0 in /mnt/data/data0/logs/_confluent-monitoring-0 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,364 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-0 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-0
[INFO] 2023-11-08 14:52:12,365 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-0 broker=0] Log loaded for partition _confluent-monitoring-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,365 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-monitoring-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,367 [data-plane-kafka-request-handler-7] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-monitoring-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,367 [data-plane-kafka-request-handler-7] kafka.log.LogManager info - Created log for partition _confluent-monitoring-1 in /mnt/data/data0/logs/_confluent-monitoring-1 with properties {cleanup.policy=delete, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=259200000}
[INFO] 2023-11-08 14:52:12,367 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-1 broker=0] No checkpointed highwatermark is found for partition _confluent-monitoring-1
[INFO] 2023-11-08 14:52:12,367 [data-plane-kafka-request-handler-7] kafka.cluster.Partition info - [Partition _confluent-monitoring-1 broker=0] Log loaded for partition _confluent-monitoring-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,367 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Follower _confluent-monitoring-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,368 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-monitoring-10, _confluent-monitoring-9, _confluent-monitoring-3, _confluent-monitoring-1, _confluent-monitoring-0, _confluent-monitoring-7, _confluent-monitoring-6, _confluent-monitoring-4)
[INFO] 2023-11-08 14:52:12,368 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 89 for 8 partitions
[INFO] 2023-11-08 14:52:12,368 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-monitoring-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,368 [data-plane-kafka-request-handler-7] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-monitoring-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-monitoring-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,369 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 48ms correlationId 89 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,371 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 90
[DEBUG] 2023-11-08 14:52:12,372 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:12,373 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:12,412 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 91 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,417 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11)
[INFO] 2023-11-08 14:52:12,417 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 91 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,420 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,420 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,421 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8
[INFO] 2023-11-08 14:52:12,421 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,421 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,423 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,424 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,424 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2
[INFO] 2023-11-08 14:52:12,424 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,424 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,426 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,426 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,426 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5
[INFO] 2023-11-08 14:52:12,426 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,426 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,428 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,428 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,429 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11
[INFO] 2023-11-08 14:52:12,429 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,429 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,431 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,432 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,432 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6
[INFO] 2023-11-08 14:52:12,432 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,432 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,434 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,435 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,435 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7
[INFO] 2023-11-08 14:52:12,435 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,435 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,437 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,437 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,438 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9
[INFO] 2023-11-08 14:52:12,438 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,438 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,439 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,440 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,440 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3
[INFO] 2023-11-08 14:52:12,440 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,440 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,442 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,442 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,442 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4
[INFO] 2023-11-08 14:52:12,442 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,442 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,444 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,445 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,445 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0
[INFO] 2023-11-08 14:52:12,445 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,445 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,447 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,447 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,447 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1
[INFO] 2023-11-08 14:52:12,447 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,448 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,449 [data-plane-kafka-request-handler-1] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,450 [data-plane-kafka-request-handler-1] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 with properties {cleanup.policy=compact,delete, delete.retention.ms=432000000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=432000000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,450 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10
[INFO] 2023-11-08 14:52:12,450 [data-plane-kafka-request-handler-1] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,450 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,450 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7, _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6)
[INFO] 2023-11-08 14:52:12,450 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 91 for 8 partitions
[INFO] 2023-11-08 14:52:12,451 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,451 [data-plane-kafka-request-handler-1] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,452 [data-plane-kafka-request-handler-1] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 40ms correlationId 91 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,502 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 92
[INFO] 2023-11-08 14:52:12,588 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 93 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,593 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1)
[INFO] 2023-11-08 14:52:12,594 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 93 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,597 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,597 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,598 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4
[INFO] 2023-11-08 14:52:12,598 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,598 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,601 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,601 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,601 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7
[INFO] 2023-11-08 14:52:12,601 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,601 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,603 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,604 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,604 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10
[INFO] 2023-11-08 14:52:12,604 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,604 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,606 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,606 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,606 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1
[INFO] 2023-11-08 14:52:12,606 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,606 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,609 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,609 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,610 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5
[INFO] 2023-11-08 14:52:12,610 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,610 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,612 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,612 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,613 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6
[INFO] 2023-11-08 14:52:12,613 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,613 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,615 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,615 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,615 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8
[INFO] 2023-11-08 14:52:12,615 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,616 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,618 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,618 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,618 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9
[INFO] 2023-11-08 14:52:12,618 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,618 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,621 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,622 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,622 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11
[INFO] 2023-11-08 14:52:12,622 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,622 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,624 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,625 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,625 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0
[INFO] 2023-11-08 14:52:12,625 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,625 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,628 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,628 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,629 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2
[INFO] 2023-11-08 14:52:12,629 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,629 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,631 [data-plane-kafka-request-handler-3] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,632 [data-plane-kafka-request-handler-3] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,632 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3
[INFO] 2023-11-08 14:52:12,632 [data-plane-kafka-request-handler-3] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,632 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,632 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2, _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0)
[INFO] 2023-11-08 14:52:12,632 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 93 for 8 partitions
[INFO] 2023-11-08 14:52:12,633 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,633 [data-plane-kafka-request-handler-3] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,634 [data-plane-kafka-request-handler-3] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 46ms correlationId 93 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,639 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 94
[INFO] 2023-11-08 14:52:12,669 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 95 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,675 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0)
[INFO] 2023-11-08 14:52:12,676 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 95 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,679 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,679 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,680 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9
[INFO] 2023-11-08 14:52:12,680 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,681 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,684 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,763 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,763 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6
[INFO] 2023-11-08 14:52:12,763 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,763 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,765 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-monitoring-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,766 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-monitoring-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-monitoring-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-monitoring-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,767 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-monitoring-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,768 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,769 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-THREE_HOURS-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,770 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:12,770 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-monitoring-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-monitoring-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-MonitoringTriggerStore-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-monitoring-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-monitoring-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:12,771 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-Group-ONE_MINUTE-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:12,772 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,772 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3
[INFO] 2023-11-08 14:52:12,772 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,772 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,775 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,776 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,776 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0
[INFO] 2023-11-08 14:52:12,776 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,776 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,780 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_partition_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 3600000)
[INFO] 2023-11-08 14:52:12,780 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,780 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,781 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10
[INFO] 2023-11-08 14:52:12,781 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,781 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,781 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,783 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,783 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,784 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,784 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,786 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,786 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,786 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,786 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,786 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,786 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11
[INFO] 2023-11-08 14:52:12,786 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,786 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,786 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,787 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,787 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,787 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,787 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,787 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,787 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,791 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,791 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,792 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8
[INFO] 2023-11-08 14:52:12,792 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,792 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,794 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_broker_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 12000000)
[INFO] 2023-11-08 14:52:12,797 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,797 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,797 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,797 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,797 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,798 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,798 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,798 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,798 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,798 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,798 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,798 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,798 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,798 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,798 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,798 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,799 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,799 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,799 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,799 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,799 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,799 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,799 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,799 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,799 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7
[INFO] 2023-11-08 14:52:12,799 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,799 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,800 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,800 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,800 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,800 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,800 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,800 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,800 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,800 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,800 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,800 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:12,803 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,803 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,803 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4
[INFO] 2023-11-08 14:52:12,803 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,804 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,806 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,807 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,807 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5
[INFO] 2023-11-08 14:52:12,807 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,807 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,810 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,811 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,811 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2
[INFO] 2023-11-08 14:52:12,811 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,811 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,814 [data-plane-kafka-request-handler-6] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,815 [data-plane-kafka-request-handler-6] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,815 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1
[INFO] 2023-11-08 14:52:12,815 [data-plane-kafka-request-handler-6] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,815 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,815 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2, _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1)
[INFO] 2023-11-08 14:52:12,816 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 95 for 8 partitions
[INFO] 2023-11-08 14:52:12,816 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,816 [data-plane-kafka-request-handler-6] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,817 [data-plane-kafka-request-handler-6] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 148ms correlationId 95 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,819 [data-plane-kafka-request-handler-4] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 96
[INFO] 2023-11-08 14:52:12,821 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 97 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,827 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1)
[INFO] 2023-11-08 14:52:12,827 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 97 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,831 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,832 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,833 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7
[INFO] 2023-11-08 14:52:12,833 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,833 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-7 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,836 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,836 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,836 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10
[INFO] 2023-11-08 14:52:12,836 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,836 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-10 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,839 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,839 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,839 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4
[INFO] 2023-11-08 14:52:12,839 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,839 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-4 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,842 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,843 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,843 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1
[INFO] 2023-11-08 14:52:12,843 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,843 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-1 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,846 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,846 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,847 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8
[INFO] 2023-11-08 14:52:12,847 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,847 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,850 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,851 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,851 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6
[INFO] 2023-11-08 14:52:12,851 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,851 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,854 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,854 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,854 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5
[INFO] 2023-11-08 14:52:12,854 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,854 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,857 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,858 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,858 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11
[INFO] 2023-11-08 14:52:12,858 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,858 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,861 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,862 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,862 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9
[INFO] 2023-11-08 14:52:12,862 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,862 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,865 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,865 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,865 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0
[INFO] 2023-11-08 14:52:12,865 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,865 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,868 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,868 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,868 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3
[INFO] 2023-11-08 14:52:12,868 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,869 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,872 [data-plane-kafka-request-handler-5] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,872 [data-plane-kafka-request-handler-5] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 with properties {cleanup.policy=compact, delete.retention.ms=691200000, message.timestamp.difference.max.ms=9223372036854775807, message.timestamp.type="CreateTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=691200000, segment.bytes=134217728}
[INFO] 2023-11-08 14:52:12,873 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2
[INFO] 2023-11-08 14:52:12,873 [data-plane-kafka-request-handler-5] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,873 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,873 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5, _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0)
[INFO] 2023-11-08 14:52:12,873 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 97 for 8 partitions
[INFO] 2023-11-08 14:52:12,874 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,874 [data-plane-kafka-request-handler-5] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,875 [data-plane-kafka-request-handler-5] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 54ms correlationId 97 from controller 1 for 12 partitions
[INFO] 2023-11-08 14:52:12,877 [data-plane-kafka-request-handler-2] state.change.logger info - [Broker id=0] Add 12 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 98
[INFO] 2023-11-08 14:52:12,879 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Handling LeaderAndIsr request correlationId 99 from controller 1 for 13 partitions
[INFO] 2023-11-08 14:52:12,885 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions Set(_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0)
[INFO] 2023-11-08 14:52:12,886 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of LeaderAndIsr request correlationId 99 from controller 1 epoch 1 as part of the become-leader transition for 4 partitions
[INFO] 2023-11-08 14:52:12,889 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,889 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,890 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9
[INFO] 2023-11-08 14:52:12,890 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,890 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-9 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,894 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,894 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,894 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3
[INFO] 2023-11-08 14:52:12,894 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,895 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-3 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,2,1 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,897 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,898 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,898 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6
[INFO] 2023-11-08 14:52:12,898 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,898 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-6 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,901 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,901 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,901 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0
[INFO] 2023-11-08 14:52:12,901 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,901 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Leader _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-0 starts at leader epoch 0 from offset 0 with high watermark 0 ISR 0,1,2 observers  addingReplicas  removingReplicas . Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,904 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,904 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,904 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11
[INFO] 2023-11-08 14:52:12,904 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,904 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,907 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent_balancer_api_state-0, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,907 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent_balancer_api_state-0 in /mnt/data/data0/logs/_confluent_balancer_api_state-0 with properties {cleanup.policy=compact, retention.ms=-1}
[INFO] 2023-11-08 14:52:12,907 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent_balancer_api_state-0 broker=0] No checkpointed highwatermark is found for partition _confluent_balancer_api_state-0
[INFO] 2023-11-08 14:52:12,907 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent_balancer_api_state-0 broker=0] Log loaded for partition _confluent_balancer_api_state-0 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,907 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent_balancer_api_state-0 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,910 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,910 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,910 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8
[INFO] 2023-11-08 14:52:12,910 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,910 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,913 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,913 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,913 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7
[INFO] 2023-11-08 14:52:12,913 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,913 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,915 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,916 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,916 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10
[INFO] 2023-11-08 14:52:12,916 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,916 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,918 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,919 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,919 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4
[INFO] 2023-11-08 14:52:12,919 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,919 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,921 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,921 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,921 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5
[INFO] 2023-11-08 14:52:12,921 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,921 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,923 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,923 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,924 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2
[INFO] 2023-11-08 14:52:12,924 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,924 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,925 [data-plane-kafka-request-handler-0] kafka.log.MergedLog$ rebuildProducerState - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1, dir=/mnt/data/data0/logs] Loading producer state till offset 0 with message format version 2
[INFO] 2023-11-08 14:52:12,926 [data-plane-kafka-request-handler-0] kafka.log.LogManager info - Created log for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 in /mnt/data/data0/logs/_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 with properties {cleanup.policy=delete, message.timestamp.type="LogAppendTime", min.insync.replicas=2, retention.bytes=-1, retention.ms=604800000}
[INFO] 2023-11-08 14:52:12,926 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 broker=0] No checkpointed highwatermark is found for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1
[INFO] 2023-11-08 14:52:12,926 [data-plane-kafka-request-handler-0] kafka.cluster.Partition info - [Partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 broker=0] Log loaded for partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 with initial high watermark 0
[INFO] 2023-11-08 14:52:12,926 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Follower _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 starts at leader epoch 0 from offset 0 with high watermark 0. Previous leader epoch was -1.
[INFO] 2023-11-08 14:52:12,926 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Removed fetcher for partitions HashSet(_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11, _confluent_balancer_api_state-0, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2, _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1)
[INFO] 2023-11-08 14:52:12,926 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Stopped fetchers as part of become-follower request from controller 1 epoch 1 with correlation id 99 for 9 partitions
[INFO] 2023-11-08 14:52:12,927 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-2-Default to broker 2 for partitions HashMap(_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent_balancer_api_state-0 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 -> InitialFetchState(None,BrokerEndPoint(id=2, host=kafka-2.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,927 [data-plane-kafka-request-handler-0] kafka.server.ReplicaFetcherManager info - [ReplicaFetcherManager on broker 0] Added fetcher ReplicaFetcherThread-0-1-Default to broker 1 for partitions HashMap(_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0), _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 -> InitialFetchState(None,BrokerEndPoint(id=1, host=kafka-1.kafka.confluent.svc.cluster.local:9072),0,0))
[INFO] 2023-11-08 14:52:12,928 [data-plane-kafka-request-handler-0] state.change.logger info - [Broker id=0] Finished LeaderAndIsr request in 49ms correlationId 99 from controller 1 for 13 partitions
[INFO] 2023-11-08 14:52:12,929 [data-plane-kafka-request-handler-7] state.change.logger info - [Broker id=0] Add 13 partitions and deleted 0 partitions from metadata cache in response to UpdateMetadata request sent by controller 1 epoch 1 with correlation id 100
[INFO] 2023-11-08 14:52:13,091 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-4, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-7, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-10, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,092 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,093 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,093 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,093 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,093 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,093 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,093 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,093 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,093 [ReplicaFetcherThread-0-1-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,093 [ReplicaFetcherThread-0-1-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-1, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,254 [data-plane-kafka-request-handler-6] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Dynamic member with unknown member id joins group _confluent-controlcenter-7-1-0-0-command in Empty state. Created a new member id _confluent-controlcenter-7-1-0-0-command-a742617c-278b-49d9-980a-b9af72e310bc-StreamThread-1-consumer-2e55ff5a-70d6-4489-9390-8dd664448aea and request the member to rejoin with this id.
[INFO] 2023-11-08 14:52:13,257 [data-plane-kafka-request-handler-4] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Preparing to rebalance group _confluent-controlcenter-7-1-0-0-command in state PreparingRebalance with old generation 0 (__consumer_offsets-20) (reason: Adding new member _confluent-controlcenter-7-1-0-0-command-a742617c-278b-49d9-980a-b9af72e310bc-StreamThread-1-consumer-2e55ff5a-70d6-4489-9390-8dd664448aea with group instance id None)
[INFO] 2023-11-08 14:52:13,272 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-5, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-8, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-11, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-6, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent_balancer_api_state-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent_balancer_api_state-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-9, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-3, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,273 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-actual-group-consumption-rekey-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,274 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,274 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerEventsStore-changelog-0, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:13,274 [ReplicaFetcherThread-0-2-Default] kafka.server.ReplicaFetcherThread info - [ReplicaFetcher replicaId=0, leaderId=2, fetcherId=0] Truncating partition _confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2 with TruncationState(offset=0, completed=true) due to local high watermark 0
[INFO] 2023-11-08 14:52:13,274 [ReplicaFetcherThread-0-2-Default] kafka.log.MergedLog info - [MergedLog partition=_confluent-controlcenter-7-1-0-0-TriggerActionsStore-repartition-2, dir=/mnt/data/data0/logs] Truncating to 0 has no effect as the largest offset in the log is -1
[INFO] 2023-11-08 14:52:14,095 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:14,098 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_api_state with config: HashMap(cleanup.policy -> compact, retention.ms -> -1)
[INFO] 2023-11-08 14:52:14,099 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,178 [executor-Rebalance] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Stabilized group confluent.connect generation 1 (__consumer_offsets-8) with 2 members
[INFO] 2023-11-08 14:52:14,197 [data-plane-kafka-request-handler-4] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Assignment received from leader connect-1-fba34a8e-0b61-49b4-a33b-788e9f50280a for group confluent.connect for generation 1. The group has 2 members, 0 of which are static.
[INFO] 2023-11-08 14:52:14,678 [data-plane-kafka-request-handler-4] kafka.server.ZkAdminManager info - [Admin Manager on Broker 0]: User:operator is updating topic _confluent_balancer_partition_samples with new configuration : cleanup.policy -> delete,retention.ms -> 3600000
[INFO] 2023-11-08 14:52:14,694 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:14,696 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_partition_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 3600000)
[INFO] 2023-11-08 14:52:14,696 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,696 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,696 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,696 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,696 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,697 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,704 [data-plane-kafka-network-thread-0-ListenerName(REPLICATION)-SASL_SSL-9] kafka.request.logger updateRequestMetrics - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":44,"requestApiVersion":1,"correlationId":5,"clientId":"kafka-cruise-control","requestApiKeyName":"INCREMENTAL_ALTER_CONFIGS"},"request":{"resources":[{"resourceType":2,"resourceName":"_confluent_balancer_partition_samples","configs":[{"name":"cleanup.policy","configOperation":0,"value":"delete"},{"name":"retention.ms","configOperation":0,"value":"3600000"}]}],"validateOnly":false},"response":{"throttleTimeMs":0,"responses":[{"errorCode":0,"errorMessage":null,"resourceType":2,"resourceName":"_confluent_balancer_partition_samples"}]},"connection":"10.40.0.17:9072-10.40.1.7:36612-15","totalTimeMs":50.505,"requestQueueTimeMs":1.595,"localTimeMs":45.946,"remoteTimeMs":2.8,"throttleTimeMs":0,"responseQueueTimeMs":0.092,"sendTimeMs":0.071,"securityProtocol":"SASL_SSL","principal":{"class":"KafkaPrincipal","type":"User","name":"operator","tokenAuthenticated":false},"listener":"REPLICATION","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.1.0-ce"},"isDisconnectedClient":false}
[INFO] 2023-11-08 14:52:14,751 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:14,754 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_broker_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 12000000)
[INFO] 2023-11-08 14:52:14,754 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,754 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,754 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,755 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,756 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,811 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:14,813 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_partition_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 3600000)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,814 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,815 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,850 [data-plane-kafka-request-handler-2] kafka.server.ZkAdminManager info - [Admin Manager on Broker 0]: User:operator is updating topic _confluent_balancer_broker_samples with new configuration : cleanup.policy -> delete,retention.ms -> 12000000
[INFO] 2023-11-08 14:52:14,861 [/config/changes-event-process-thread] kafka.common.ZkNodeChangeNotificationListener info - Processing notification(s) to /config/changes
[INFO] 2023-11-08 14:52:14,862 [data-plane-kafka-network-thread-0-ListenerName(REPLICATION)-SASL_SSL-10] kafka.request.logger updateRequestMetrics - Completed request:{"isForwarded":false,"requestHeader":{"requestApiKey":44,"requestApiVersion":1,"correlationId":5,"clientId":"kafka-cruise-control","requestApiKeyName":"INCREMENTAL_ALTER_CONFIGS"},"request":{"resources":[{"resourceType":2,"resourceName":"_confluent_balancer_broker_samples","configs":[{"name":"cleanup.policy","configOperation":0,"value":"delete"},{"name":"retention.ms","configOperation":0,"value":"12000000"}]}],"validateOnly":false},"response":{"throttleTimeMs":0,"responses":[{"errorCode":0,"errorMessage":null,"resourceType":2,"resourceName":"_confluent_balancer_broker_samples"}]},"connection":"10.40.0.17:9072-10.40.1.7:36626-15","totalTimeMs":18.388,"requestQueueTimeMs":0.095,"localTimeMs":18.03,"remoteTimeMs":0.122,"throttleTimeMs":0,"responseQueueTimeMs":0.068,"sendTimeMs":0.071,"securityProtocol":"SASL_SSL","principal":{"class":"KafkaPrincipal","type":"User","name":"operator","tokenAuthenticated":false},"listener":"REPLICATION","clientInformation":{"softwareName":"apache-kafka-java","softwareVersion":"7.1.0-ce"},"isDisconnectedClient":false}
[INFO] 2023-11-08 14:52:14,863 [/config/changes-event-process-thread] kafka.server.DynamicConfigManager info - Processing override for entityPath: topics/_confluent_balancer_broker_samples with config: HashMap(cleanup.policy -> delete, retention.ms -> 12000000)
[INFO] 2023-11-08 14:52:14,864 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,864 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,864 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,864 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,864 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,864 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,864 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,864 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,865 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[INFO] 2023-11-08 14:52:14,866 [/config/changes-event-process-thread] kafka.tier.state.FileTierPartitionState setTieringDisabled - Setting tieringEnabled to false (earlier value: false)
[DEBUG] 2023-11-08 14:52:15,372 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:15,375 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:15,953 [ldap-group-manager] io.confluent.security.auth.provider.ldap.LdapGroupManager search - Searching groups with base dc=test,dc=com filter (objectClass=group): 
[DEBUG] 2023-11-08 14:52:15,956 [ldap-group-manager] io.confluent.security.auth.provider.ldap.LdapGroupManager searchAndProcessResults - Search completed, group cache is {}
[DEBUG] 2023-11-08 14:52:15,956 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-0]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,957 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-1]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,957 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-2]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,957 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-3]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,957 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-4]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,957 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-5]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,962 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-0]Status record of generation 1 for partition _confluent-metadata-auth-0 written at offset 3
[DEBUG] 2023-11-08 14:52:15,963 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,963 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:3 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,963 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-0]Received new generation id 1 for partition writer _confluent-metadata-auth-0 at offset 3 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:52:15,963 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Status record of generation 1 for partition _confluent-metadata-auth-1 written at offset 4
[DEBUG] 2023-11-08 14:52:15,964 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-4]Status record of generation 1 for partition _confluent-metadata-auth-4 written at offset 5
[DEBUG] 2023-11-08 14:52:15,964 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-2]Status record of generation 1 for partition _confluent-metadata-auth-2 written at offset 4
[DEBUG] 2023-11-08 14:52:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:4 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-1]Received new generation id 1 for partition writer _confluent-metadata-auth-1 at offset 4 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:52:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:4 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-2]Received new generation id 1 for partition writer _confluent-metadata-auth-2 at offset 4 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:52:15,965 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:5 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-4]Received new generation id 1 for partition writer _confluent-metadata-auth-4 at offset 5 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:52:15,967 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-3]Status record of generation 1 for partition _confluent-metadata-auth-3 written at offset 6
[DEBUG] 2023-11-08 14:52:15,967 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:6 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-3]Received new generation id 1 for partition writer _confluent-metadata-auth-3 at offset 6 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:52:15,969 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Status record of generation 1 for partition _confluent-metadata-auth-5 written at offset 7
[DEBUG] 2023-11-08 14:52:15,969 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,969 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:7 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:52:15,969 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-5]Received new generation id 1 for partition writer _confluent-metadata-auth-5 at offset 7 status INITIALIZED generation 1
[INFO] 2023-11-08 14:52:16,264 [executor-Rebalance] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Stabilized group _confluent-controlcenter-7-1-0-0-command generation 1 (__consumer_offsets-20) with 1 members
[INFO] 2023-11-08 14:52:16,300 [data-plane-kafka-request-handler-4] kafka.coordinator.group.GroupCoordinator info - [GroupCoordinator 0]: Assignment received from leader _confluent-controlcenter-7-1-0-0-command-a742617c-278b-49d9-980a-b9af72e310bc-StreamThread-1-consumer-2e55ff5a-70d6-4489-9390-8dd664448aea for group _confluent-controlcenter-7-1-0-0-command for generation 1. The group has 1 members, 0 of which are static.
[DEBUG] 2023-11-08 14:52:18,372 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:18,376 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:18,980 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:52:18,980 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:52:18,981 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:52:18,995 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:18,996 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:52:18,996 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:52:18,996 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:52:18,996 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455138996
[WARN] 2023-11-08 14:52:18,999 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:52:19,001 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455139001
[INFO] 2023-11-08 14:52:19,011 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:52:19,025 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:52:19,026 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:52:19,026 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:52:19,026 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:52:19,026 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:52:19,026 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:52:19,026 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:52:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:52:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:52:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:52:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:52:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:52:19,027 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:52:21,372 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:21,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:24,372 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:24,373 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:27,372 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:27,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:27,670 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:52:27 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 13
[INFO] 2023-11-08 14:52:27,670 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:52:27 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 13
[INFO] 2023-11-08 14:52:27,670 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:52:27 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 13
[DEBUG] 2023-11-08 14:52:30,372 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:30,373 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:33,373 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:33,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:36,373 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:36,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:39,373 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:39,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:42,373 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:42,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:45,112 [qtp1527668063-254] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 254]
[TRACE] 2023-11-08 14:52:45,114 [qtp1527668063-254] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 254]
[DEBUG] 2023-11-08 14:52:45,128 [qtp1527668063-254] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:52:45,129 [qtp1527668063-254] io.confluent.mds.request.logger log - 60 * Server has received a request on thread qtp1527668063-254null60 > GET https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull60 > User Principal: c3
60 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null60 > Cache-Control: no-cachenull60 > Connection: keep-alivenull60 > Content-Type: application/jsonnull60 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null60 > Pragma: no-cachenull60 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:52:45,133 [qtp1527668063-254] io.confluent.mds.request.logger log - 60 * Server responded with a response on thread qtp1527668063-254null60 < 200null60 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:52:45,134 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:52:45 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 23
[INFO] 2023-11-08 14:52:45,134 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:52:45 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 23
[INFO] 2023-11-08 14:52:45,134 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:52:45 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 23
[DEBUG] 2023-11-08 14:52:45,373 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:45,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:47,357 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:52:47 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[INFO] 2023-11-08 14:52:47,357 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:52:47 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[INFO] 2023-11-08 14:52:47,357 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:52:47 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[DEBUG] 2023-11-08 14:52:48,373 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:48,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:52:49,128 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:52:49,129 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:52:49,129 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:52:49,142 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,142 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:52:49,143 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:52:49,143 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:52:49,143 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455169142
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,145 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:52:49,146 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:52:49,147 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:52:49,147 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455169146
[INFO] 2023-11-08 14:52:49,154 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:52:49,172 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:52:49,173 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:52:49,173 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:52:49,173 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:52:49,173 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:52:49,173 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:52:49,174 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:52:49,174 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:52:49,174 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:52:49,174 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:52:49,174 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:52:49,174 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:52:49,174 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:52:51,373 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:51,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:54,373 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:54,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:52:57,374 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:52:57,375 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:52:58,218 [qtp1527668063-198] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 198]
[TRACE] 2023-11-08 14:52:58,220 [qtp1527668063-198] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 198]
[DEBUG] 2023-11-08 14:52:58,222 [qtp1527668063-198] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:52:58,223 [qtp1527668063-198] io.confluent.mds.request.logger log - 61 * Server has received a request on thread qtp1527668063-198null61 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull61 > User Principal: c3
61 > authorization: Basic YzM6YzMtc2VjcmV0null61 > host: kafka.confluent.svc.cluster.local:8090null61 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:52:58,223 [qtp1527668063-198] io.confluent.mds.request.logger log - 61 * Server responded with a response on thread qtp1527668063-198null61 < 200null61 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:52:58,225 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:58 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[INFO] 2023-11-08 14:52:58,225 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:58 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[INFO] 2023-11-08 14:52:58,225 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:52:58 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[DEBUG] 2023-11-08 14:53:00,373 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:00,374 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:03,374 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:03,375 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:53:06,313 [qtp1527668063-242] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 242]
[TRACE] 2023-11-08 14:53:06,316 [qtp1527668063-242] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 242]
[DEBUG] 2023-11-08 14:53:06,317 [qtp1527668063-242] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:53:06,318 [qtp1527668063-242] io.confluent.mds.request.logger log - 62 * Server has received a request on thread qtp1527668063-242null62 > GET https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull62 > User Principal: c3
62 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null62 > Cache-Control: no-cachenull62 > Connection: keep-alivenull62 > Content-Type: application/jsonnull62 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null62 > Pragma: no-cachenull62 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:53:06,322 [qtp1527668063-242] io.confluent.mds.request.logger log - 62 * Server responded with a response on thread qtp1527668063-242null62 < 200null62 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:53:06,323 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:53:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:53:06,323 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:53:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:53:06,323 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:53:06 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[DEBUG] 2023-11-08 14:53:06,374 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:06,375 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:53:07,028 [qtp1527668063-198] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 198]
[TRACE] 2023-11-08 14:53:07,029 [qtp1527668063-198] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 198]
[DEBUG] 2023-11-08 14:53:07,031 [qtp1527668063-198] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:53:07,032 [qtp1527668063-198] io.confluent.mds.request.logger log - 63 * Server has received a request on thread qtp1527668063-198null63 > GET https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull63 > User Principal: c3
63 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null63 > Cache-Control: no-cachenull63 > Connection: keep-alivenull63 > Content-Type: application/jsonnull63 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null63 > Pragma: no-cachenull63 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:53:07,035 [qtp1527668063-198] io.confluent.mds.request.logger log - 63 * Server responded with a response on thread qtp1527668063-198null63 < 200null63 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:53:07,037 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:53:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:53:07,037 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:53:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[INFO] 2023-11-08 14:53:07,037 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:53:07 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 10
[DEBUG] 2023-11-08 14:53:09,374 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:09,375 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:53:10,256 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:53:10 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[INFO] 2023-11-08 14:53:10,256 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:53:10 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[INFO] 2023-11-08 14:53:10,256 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:53:10 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[DEBUG] 2023-11-08 14:53:12,374 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:12,375 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:15,375 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:15,376 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:53:15,956 [ldap-group-manager] io.confluent.security.auth.provider.ldap.LdapGroupManager search - Searching groups with base dc=test,dc=com filter (objectClass=group): 
[DEBUG] 2023-11-08 14:53:15,958 [ldap-group-manager] io.confluent.security.auth.provider.ldap.LdapGroupManager searchAndProcessResults - Search completed, group cache is {}
[DEBUG] 2023-11-08 14:53:15,958 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-0]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,959 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-1]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,959 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-2]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,959 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-3]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,959 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-4]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,959 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-5]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,962 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-0]Status record of generation 1 for partition _confluent-metadata-auth-0 written at offset 4
[DEBUG] 2023-11-08 14:53:15,963 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,963 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-2]Status record of generation 1 for partition _confluent-metadata-auth-2 written at offset 5
[DEBUG] 2023-11-08 14:53:15,963 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:4 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,963 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-0]Received new generation id 1 for partition writer _confluent-metadata-auth-0 at offset 4 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:53:15,963 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Status record of generation 1 for partition _confluent-metadata-auth-1 written at offset 5
[DEBUG] 2023-11-08 14:53:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:5 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-1]Received new generation id 1 for partition writer _confluent-metadata-auth-1 at offset 5 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:53:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:5 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-2]Received new generation id 1 for partition writer _confluent-metadata-auth-2 at offset 5 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:53:15,965 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-3]Status record of generation 1 for partition _confluent-metadata-auth-3 written at offset 7
[DEBUG] 2023-11-08 14:53:15,965 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:7 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-3]Received new generation id 1 for partition writer _confluent-metadata-auth-3 at offset 7 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:53:15,966 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Status record of generation 1 for partition _confluent-metadata-auth-5 written at offset 8
[DEBUG] 2023-11-08 14:53:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,966 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-4]Status record of generation 1 for partition _confluent-metadata-auth-4 written at offset 6
[DEBUG] 2023-11-08 14:53:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:6 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-4]Received new generation id 1 for partition writer _confluent-metadata-auth-4 at offset 6 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:53:15,967 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:8 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:53:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-5]Received new generation id 1 for partition writer _confluent-metadata-auth-5 at offset 8 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:53:18,375 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:18,376 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:53:18,689 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:53:18,690 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:53:18,690 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:53:18,693 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:53:18,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:53:18,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:53:18,694 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455198694
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,697 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,698 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:53:18,699 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455198699
[INFO] 2023-11-08 14:53:18,707 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:53:18,725 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:53:18,726 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:53:18,726 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:53:18,726 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:53:18,726 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:53:18,726 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:53:18,727 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:53:18,727 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:53:18,727 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:53:18,727 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:53:18,727 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:53:18,727 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:53:18,728 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:53:21,375 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:21,376 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:24,376 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:24,377 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:27,377 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:27,378 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:30,377 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:30,378 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:33,377 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:33,378 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:36,377 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:36,378 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:39,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:39,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:42,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:42,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:53:44,792 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 183]
[TRACE] 2023-11-08 14:53:44,794 [qtp1527668063-183] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 183]
[DEBUG] 2023-11-08 14:53:44,807 [qtp1527668063-183] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:53:44,808 [qtp1527668063-183] io.confluent.mds.request.logger log - 64 * Server has received a request on thread qtp1527668063-183null64 > GET https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull64 > User Principal: c3
64 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null64 > Cache-Control: no-cachenull64 > Connection: keep-alivenull64 > Content-Type: application/jsonnull64 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null64 > Pragma: no-cachenull64 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:53:44,811 [qtp1527668063-183] io.confluent.mds.request.logger log - 64 * Server responded with a response on thread qtp1527668063-183null64 < 200null64 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:53:44,813 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:53:44 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 22
[INFO] 2023-11-08 14:53:44,813 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:53:44 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 22
[INFO] 2023-11-08 14:53:44,813 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:53:44 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 22
[DEBUG] 2023-11-08 14:53:45,378 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:45,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:53:46,341 [pool-18-thread-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - all topics exist
[INFO] 2023-11-08 14:53:46,341 [pool-18-thread-1] io.confluent.security.audit.telemetry.exporter.NonBlockingKafkaExporter ensureOrCheckTopicsWithMetadata - Event logger has metadata for all topics
[DEBUG] 2023-11-08 14:53:48,379 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:48,379 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:53:48,751 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:53:48,751 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:53:48,752 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:53:48,763 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,763 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:53:48,764 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:53:48,764 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:53:48,764 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455228764
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,767 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,768 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,769 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,769 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:53:48,769 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:53:48,769 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:53:48,769 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:53:48,769 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455228769
[INFO] 2023-11-08 14:53:48,780 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:53:48,795 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:53:48,796 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:53:48,796 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:53:48,796 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:53:48,796 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:53:48,796 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:53:48,797 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:53:48,797 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:53:48,797 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:53:48,797 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:53:48,797 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:53:48,797 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:53:48,798 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:53:51,379 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:51,380 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:54,379 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:54,380 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:53:57,379 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:53:57,380 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:00,379 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:00,380 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:03,379 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:03,380 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:06,380 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:06,381 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:09,381 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:09,382 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:12,382 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:12,382 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:15,383 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:15,384 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:54:15,958 [ldap-group-manager] io.confluent.security.auth.provider.ldap.LdapGroupManager search - Searching groups with base dc=test,dc=com filter (objectClass=group): 
[DEBUG] 2023-11-08 14:54:15,960 [ldap-group-manager] io.confluent.security.auth.provider.ldap.LdapGroupManager searchAndProcessResults - Search completed, group cache is {}
[DEBUG] 2023-11-08 14:54:15,960 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-0]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,960 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-1]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,960 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-2]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,961 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-3]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,961 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-4]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,961 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-5]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,963 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-0]Status record of generation 1 for partition _confluent-metadata-auth-0 written at offset 5
[DEBUG] 2023-11-08 14:54:15,963 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,963 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:5 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,963 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-2]Status record of generation 1 for partition _confluent-metadata-auth-2 written at offset 6
[DEBUG] 2023-11-08 14:54:15,964 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-0]Received new generation id 1 for partition writer _confluent-metadata-auth-0 at offset 5 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:54:15,964 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Status record of generation 1 for partition _confluent-metadata-auth-1 written at offset 6
[DEBUG] 2023-11-08 14:54:15,964 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:6 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-1]Received new generation id 1 for partition writer _confluent-metadata-auth-1 at offset 6 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:54:15,965 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:6 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-2]Received new generation id 1 for partition writer _confluent-metadata-auth-2 at offset 6 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:54:15,965 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-3]Status record of generation 1 for partition _confluent-metadata-auth-3 written at offset 8
[DEBUG] 2023-11-08 14:54:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:8 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-3]Received new generation id 1 for partition writer _confluent-metadata-auth-3 at offset 8 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:54:15,966 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Status record of generation 1 for partition _confluent-metadata-auth-5 written at offset 9
[DEBUG] 2023-11-08 14:54:15,966 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-4]Status record of generation 1 for partition _confluent-metadata-auth-4 written at offset 7
[DEBUG] 2023-11-08 14:54:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:9 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-5]Received new generation id 1 for partition writer _confluent-metadata-auth-5 at offset 9 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:54:15,967 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:7 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:54:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-4]Received new generation id 1 for partition writer _confluent-metadata-auth-4 at offset 7 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:54:18,383 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:18,384 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:54:18,822 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:54:18,823 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:54:18,823 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:54:18,838 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,839 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:54:18,839 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:54:18,839 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:54:18,839 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455258839
[WARN] 2023-11-08 14:54:18,841 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,841 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,841 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,841 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,841 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,841 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,841 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,842 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:54:18,843 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455258843
[INFO] 2023-11-08 14:54:18,902 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:54:18,916 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:54:18,916 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:54:18,916 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:54:18,917 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:54:18,917 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:54:18,917 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:54:18,917 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:54:18,917 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:54:18,917 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:54:18,918 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:54:18,918 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:54:18,918 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:54:18,918 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:54:21,383 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:21,384 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:24,383 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:24,384 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:27,384 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:27,385 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:30,385 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:30,385 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:33,386 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:33,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:36,386 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:36,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:39,386 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:39,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:54:42,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:42,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:54:42,543 [qtp1527668063-253] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 253]
[TRACE] 2023-11-08 14:54:42,545 [qtp1527668063-253] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 253]
[DEBUG] 2023-11-08 14:54:42,561 [qtp1527668063-253] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:54:42,562 [qtp1527668063-253] io.confluent.mds.request.logger log - 65 * Server has received a request on thread qtp1527668063-253null65 > GET https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull65 > User Principal: c3
65 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null65 > Cache-Control: no-cachenull65 > Connection: keep-alivenull65 > Content-Type: application/jsonnull65 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null65 > Pragma: no-cachenull65 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:54:42,565 [qtp1527668063-253] io.confluent.mds.request.logger log - 65 * Server responded with a response on thread qtp1527668063-253null65 < 200null65 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:54:42,567 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:54:42 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 25
[INFO] 2023-11-08 14:54:42,567 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:54:42 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 25
[INFO] 2023-11-08 14:54:42,567 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:54:42 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 25
[DEBUG] 2023-11-08 14:54:45,386 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:45,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:54:46,037 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:46 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:54:46,037 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:46 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:54:46,037 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:54:46 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[TRACE] 2023-11-08 14:54:46,987 [qtp1527668063-199] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 199]
[TRACE] 2023-11-08 14:54:46,988 [qtp1527668063-199] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 199]
[DEBUG] 2023-11-08 14:54:46,990 [qtp1527668063-199] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:54:46,991 [qtp1527668063-199] io.confluent.mds.request.logger log - 66 * Server has received a request on thread qtp1527668063-199null66 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull66 > User Principal: c3
66 > authorization: Basic YzM6YzMtc2VjcmV0null66 > host: kafka.confluent.svc.cluster.local:8090null66 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:54:46,991 [qtp1527668063-199] io.confluent.mds.request.logger log - 66 * Server responded with a response on thread qtp1527668063-199null66 < 200null66 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:54:46,993 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:54:46 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:54:46,993 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:54:46 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:54:46,993 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:54:46 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[DEBUG] 2023-11-08 14:54:48,386 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:48,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:54:48,880 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:54:48,880 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:54:48,881 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:54:48,894 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,894 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:54:48,895 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:54:48,895 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:54:48,895 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455288895
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,897 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,898 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:54:48,899 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455288899
[INFO] 2023-11-08 14:54:48,915 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:54:48,930 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:54:48,930 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:54:48,930 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:54:48,931 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:54:48,931 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:54:48,931 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:54:48,931 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:54:48,931 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:54:48,931 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:54:48,932 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:54:48,932 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:54:48,932 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:54:48,932 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:54:51,386 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:51,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:54:53,547 [qtp1527668063-242] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 242]
[TRACE] 2023-11-08 14:54:53,549 [qtp1527668063-242] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 242]
[DEBUG] 2023-11-08 14:54:53,551 [qtp1527668063-242] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:54:53,552 [qtp1527668063-242] io.confluent.mds.request.logger log - 67 * Server has received a request on thread qtp1527668063-242null67 > GET https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull67 > User Principal: c3
67 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null67 > Cache-Control: no-cachenull67 > Connection: keep-alivenull67 > Content-Type: application/jsonnull67 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null67 > Pragma: no-cachenull67 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:54:53,555 [qtp1527668063-242] io.confluent.mds.request.logger log - 67 * Server responded with a response on thread qtp1527668063-242null67 < 200null67 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:54:53,556 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:54:53 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:54:53,556 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:54:53 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:54:53,556 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:54:53 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[DEBUG] 2023-11-08 14:54:54,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:54,387 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:54:55,417 [qtp1527668063-249] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 249]
[TRACE] 2023-11-08 14:54:55,419 [qtp1527668063-249] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 249]
[DEBUG] 2023-11-08 14:54:55,421 [qtp1527668063-249] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:54:55,422 [qtp1527668063-249] io.confluent.mds.request.logger log - 68 * Server has received a request on thread qtp1527668063-249null68 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:testadmin/roles/ClusterAdminnull68 > User Principal: kafka
68 > Accept: application/jsonnull68 > Content-Length: 56null68 > Content-Type: application/jsonnull68 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null68 > User-Agent: confluent-operatornull68 > Via: 1.1 kafka-1null68 > X-Forwarded-For: 10.40.2.7null68 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null68 > X-Forwarded-Proto: httpsnull68 > X-Forwarded-Server: 10.40.1.7null{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}}


[DEBUG] 2023-11-08 14:54:55,425 [qtp1527668063-249] io.confluent.security.auth.store.kafka.KafkaAuthWriter addClusterRoleBinding - addClusterRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:testadmin role=ClusterAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') reason=null
[DEBUG] 2023-11-08 14:54:55,425 [qtp1527668063-249] io.confluent.security.auth.store.kafka.KafkaAuthWriter replaceResourceRoleBinding - replaceResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:testadmin role=ClusterAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}') resources=[] reason=null
[DEBUG] 2023-11-08 14:54:55,426 [qtp1527668063-249] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-1]Writing new record with key RoleBindingKey{principal=User:testadmin, role='ClusterAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} to partition _confluent-metadata-auth-1 generation id 1
[DEBUG] 2023-11-08 14:54:55,476 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-1]Pending write completed metadata=_confluent-metadata-auth-1@7 exception=null
[DEBUG] 2023-11-08 14:54:55,476 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Send callback for record with partition _confluent-metadata-auth-1 generationId 1 offset 7
[DEBUG] 2023-11-08 14:54:55,477 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:7 key RoleBindingKey{principal=User:testadmin, role='ClusterAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:54:55,477 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-1]Completing pending write since offset 7 has been consumed
[INFO] 2023-11-08 14:54:55,477 [qtp1527668063-249] io.confluent.mds.request.logger log - 68 * Server responded with a response on thread qtp1527668063-249null68 < 204null
[INFO] 2023-11-08 14:54:55,478 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/ClusterAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 61
[INFO] 2023-11-08 14:54:55,478 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/ClusterAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 61
[INFO] 2023-11-08 14:54:55,478 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/ClusterAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 61
[TRACE] 2023-11-08 14:54:55,570 [qtp1527668063-190] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 190]
[TRACE] 2023-11-08 14:54:55,572 [qtp1527668063-190] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 190]
[DEBUG] 2023-11-08 14:54:55,574 [qtp1527668063-190] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:54:55,574 [qtp1527668063-190] io.confluent.mds.request.logger log - 69 * Server has received a request on thread qtp1527668063-190null69 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:testadmin/roles/SystemAdminnull69 > User Principal: kafka
69 > Accept: application/jsonnull69 > Content-Length: 112null69 > Content-Type: application/jsonnull69 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null69 > User-Agent: confluent-operatornull69 > Via: 1.1 kafka-2null69 > X-Forwarded-For: 10.40.2.7null69 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null69 > X-Forwarded-Proto: httpsnull69 > X-Forwarded-Server: 10.40.0.15null{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","schema-registry-cluster":"id_schemaregistry_confluent"}}


[DEBUG] 2023-11-08 14:54:55,577 [qtp1527668063-190] io.confluent.security.auth.store.kafka.KafkaAuthWriter addClusterRoleBinding - addClusterRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:testadmin role=SystemAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, schema-registry-cluster=id_schemaregistry_confluent}') reason=null
[DEBUG] 2023-11-08 14:54:55,577 [qtp1527668063-190] io.confluent.security.auth.store.kafka.KafkaAuthWriter replaceResourceRoleBinding - replaceResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:testadmin role=SystemAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, schema-registry-cluster=id_schemaregistry_confluent}') resources=[] reason=null
[DEBUG] 2023-11-08 14:54:55,577 [qtp1527668063-190] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-5]Writing new record with key RoleBindingKey{principal=User:testadmin, role='SystemAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, schema-registry-cluster=id_schemaregistry_confluent}')'} to partition _confluent-metadata-auth-5 generation id 1
[DEBUG] 2023-11-08 14:54:55,580 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-5]Pending write completed metadata=_confluent-metadata-auth-5@10 exception=null
[DEBUG] 2023-11-08 14:54:55,580 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Send callback for record with partition _confluent-metadata-auth-5 generationId 1 offset 10
[DEBUG] 2023-11-08 14:54:55,581 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:10 key RoleBindingKey{principal=User:testadmin, role='SystemAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, schema-registry-cluster=id_schemaregistry_confluent}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:54:55,581 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-5]Completing pending write since offset 10 has been consumed
[INFO] 2023-11-08 14:54:55,581 [qtp1527668063-190] io.confluent.mds.request.logger log - 69 * Server responded with a response on thread qtp1527668063-190null69 < 204null
[INFO] 2023-11-08 14:54:55,582 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 12
[INFO] 2023-11-08 14:54:55,582 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 12
[INFO] 2023-11-08 14:54:55,582 [qtp1527668063-190] io.confluent.rest-utils.requests write - 10.40.0.15 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 12
[TRACE] 2023-11-08 14:54:55,659 [qtp1527668063-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 246]
[TRACE] 2023-11-08 14:54:55,660 [qtp1527668063-246] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 246]
[DEBUG] 2023-11-08 14:54:55,662 [qtp1527668063-246] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:54:55,663 [qtp1527668063-246] io.confluent.mds.request.logger log - 70 * Server has received a request on thread qtp1527668063-246null70 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:testadmin/roles/SystemAdminnull70 > User Principal: kafka
70 > Accept: application/jsonnull70 > Content-Length: 94null70 > Content-Type: application/jsonnull70 > Host: kafka.confluent.svc.cluster.local:8090null70 > User-Agent: confluent-operatornull{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-cluster":"confluent.connect"}}


[DEBUG] 2023-11-08 14:54:55,665 [qtp1527668063-246] io.confluent.security.auth.store.kafka.KafkaAuthWriter addClusterRoleBinding - addClusterRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:testadmin role=SystemAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, connect-cluster=confluent.connect}') reason=null
[DEBUG] 2023-11-08 14:54:55,665 [qtp1527668063-246] io.confluent.security.auth.store.kafka.KafkaAuthWriter replaceResourceRoleBinding - replaceResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:testadmin role=SystemAdmin scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, connect-cluster=confluent.connect}') resources=[] reason=null
[DEBUG] 2023-11-08 14:54:55,665 [qtp1527668063-246] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-1]Writing new record with key RoleBindingKey{principal=User:testadmin, role='SystemAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, connect-cluster=confluent.connect}')'} to partition _confluent-metadata-auth-1 generation id 1
[DEBUG] 2023-11-08 14:54:55,668 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-1]Pending write completed metadata=_confluent-metadata-auth-1@8 exception=null
[DEBUG] 2023-11-08 14:54:55,668 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Send callback for record with partition _confluent-metadata-auth-1 generationId 1 offset 8
[DEBUG] 2023-11-08 14:54:55,669 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:8 key RoleBindingKey{principal=User:testadmin, role='SystemAdmin', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, connect-cluster=confluent.connect}')'} newValue RoleBindingValue(resources=[]) oldValue null
[DEBUG] 2023-11-08 14:54:55,669 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-1]Completing pending write since offset 8 has been consumed
[INFO] 2023-11-08 14:54:55,669 [qtp1527668063-246] io.confluent.mds.request.logger log - 70 * Server responded with a response on thread qtp1527668063-246null70 < 204null
[INFO] 2023-11-08 14:54:55,670 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 12
[INFO] 2023-11-08 14:54:55,670 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 12
[INFO] 2023-11-08 14:54:55,670 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.2.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/SystemAdmin HTTP/1.1" 204 0 "-" "confluent-operator" 12
[TRACE] 2023-11-08 14:54:55,798 [qtp1527668063-185] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 185]
[TRACE] 2023-11-08 14:54:55,800 [qtp1527668063-185] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user kafka with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=kafka,dc=test,dc=com [Thread 185]
[DEBUG] 2023-11-08 14:54:55,802 [qtp1527668063-185] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for kafka
[INFO] 2023-11-08 14:54:55,803 [qtp1527668063-185] io.confluent.mds.request.logger log - 71 * Server has received a request on thread qtp1527668063-185null71 > POST https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/principals/User:testadmin/roles/ResourceOwner/bindingsnull71 > User Principal: kafka
71 > Accept: application/jsonnull71 > Content-Length: 199null71 > Content-Type: application/jsonnull71 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null71 > User-Agent: confluent-operatornull71 > Via: 1.1 kafka-1null71 > X-Forwarded-For: 10.40.2.7null71 > X-Forwarded-Host: kafka.confluent.svc.cluster.local:8090null71 > X-Forwarded-Proto: httpsnull71 > X-Forwarded-Server: 10.40.1.7null{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","ksql-cluster":"confluent.ksqldb_"}},"resourcePatterns":[{"resourceType":"KsqlCluster","name":"ksql-cluster","patternType":"LITERAL"}]}


[DEBUG] 2023-11-08 14:54:55,806 [qtp1527668063-185] io.confluent.security.auth.store.kafka.KafkaAuthWriter addResourceRoleBinding - addResourceRoleBinding requestorPrincipal=Optional[User:kafka] principal=User:testadmin role=ResourceOwner scope=Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, ksql-cluster=confluent.ksqldb_}') resources=[KsqlCluster:LITERAL:ksql-cluster] reason=null
[DEBUG] 2023-11-08 14:54:55,806 [qtp1527668063-185] io.confluent.security.auth.store.kafka.KafkaAuthWriter lambda$addResourceRoleBinding$6 - New binding User:testadmin ResourceOwner Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, ksql-cluster=confluent.ksqldb_}') [KsqlCluster:LITERAL:ksql-cluster]
[DEBUG] 2023-11-08 14:54:55,806 [qtp1527668063-185] io.confluent.security.store.kafka.clients.KafkaPartitionWriter write - [PartitionWriter _confluent-metadata-auth-1]Writing new record with key RoleBindingKey{principal=User:testadmin, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, ksql-cluster=confluent.ksqldb_}')'} to partition _confluent-metadata-auth-1 generation id 1
[DEBUG] 2023-11-08 14:54:55,808 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onCompletion - [PartitionWriter _confluent-metadata-auth-1]Pending write completed metadata=_confluent-metadata-auth-1@9 exception=null
[DEBUG] 2023-11-08 14:54:55,808 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Send callback for record with partition _confluent-metadata-auth-1 generationId 1 offset 9
[DEBUG] 2023-11-08 14:54:55,809 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:9 key RoleBindingKey{principal=User:testadmin, role='ResourceOwner', scope='Scope(path='[]', clusters='{kafka-cluster=9PWH12e6ROOpezLRDGV6Ag, ksql-cluster=confluent.ksqldb_}')'} newValue RoleBindingValue(resources=[KsqlCluster:LITERAL:ksql-cluster]) oldValue null
[DEBUG] 2023-11-08 14:54:55,809 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter maybeComplete - [PartitionWriter _confluent-metadata-auth-1]Completing pending write since offset 9 has been consumed
[INFO] 2023-11-08 14:54:55,809 [qtp1527668063-185] io.confluent.mds.request.logger log - 71 * Server responded with a response on thread qtp1527668063-185null71 < 204null
[INFO] 2023-11-08 14:54:55,810 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 13
[INFO] 2023-11-08 14:54:55,810 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 13
[INFO] 2023-11-08 14:54:55,810 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.1.7 - kafka [08/Nov/2023:14:54:55 +0000] "POST /security/1.0/principals/User:testadmin/roles/ResourceOwner/bindings HTTP/1.1" 204 0 "-" "confluent-operator" 13
[DEBUG] 2023-11-08 14:54:57,388 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:54:57,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:00,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:00,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:03,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:03,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:55:03,419 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 186]
[TRACE] 2023-11-08 14:55:03,421 [qtp1527668063-186] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 186]
[DEBUG] 2023-11-08 14:55:03,423 [qtp1527668063-186] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:55:03,423 [qtp1527668063-186] io.confluent.mds.request.logger log - 72 * Server has received a request on thread qtp1527668063-186null72 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull72 > User Principal: c3
72 > authorization: Basic YzM6YzMtc2VjcmV0null72 > host: kafka.confluent.svc.cluster.local:8090null72 > user-agent: armeria/1.13.4null
[INFO] 2023-11-08 14:55:03,424 [qtp1527668063-186] io.confluent.mds.request.logger log - 72 * Server responded with a response on thread qtp1527668063-186null72 < 200null72 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:55:03,425 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:55:03,425 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 7
[INFO] 2023-11-08 14:55:03,426 [qtp1527668063-186] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/metadataClusterId HTTP/2.0" 200 22 "-" "armeria/1.13.4" 8
[TRACE] 2023-11-08 14:55:03,655 [qtp1527668063-199] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user testadmin with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 199]
[TRACE] 2023-11-08 14:55:03,657 [qtp1527668063-199] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user testadmin with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=testadmin,dc=test,dc=com [Thread 199]
[DEBUG] 2023-11-08 14:55:03,659 [qtp1527668063-199] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for testadmin
[INFO] 2023-11-08 14:55:03,661 [qtp1527668063-199] io.confluent.mds.request.logger log - 73 * Server has received a request on thread qtp1527668063-199null73 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticate?renewer=kafkanull73 > User Principal: testadmin
73 > Accept: application/jsonnull73 > Accept-Encoding: gzip, deflate, brnull73 > Accept-Language: en-GB,en-US;q=0.9,en;q=0.8null73 > Cookie: grafana_session=8d570e5554a7e8bf399eeaee6cfa126b,grafana_session_expiry=1697558855null73 > Host: kafka.confluent.svc.cluster.local:8090null73 > Referer: https://localhost:9021/loginnull73 > sec-ch-ua: "Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"null73 > sec-ch-ua-mobile: ?0null73 > sec-ch-ua-platform: "macOS"null73 > sec-fetch-dest: emptynull73 > sec-fetch-mode: corsnull73 > sec-fetch-site: same-originnull73 > User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36null73 > Via: 2.0 controlcenter-0null73 > X-Forwarded-For: 127.0.0.1null73 > X-Forwarded-Host: localhost:9021null73 > X-Forwarded-Proto: httpsnull73 > X-Forwarded-Server: 127.0.0.1null73 > x-requested-by: 28ac9d86-2317-4b2f-bbc6-22c1316157danull73 > x-requested-with: undefinednull
[INFO] 2023-11-08 14:55:03,664 [qtp1527668063-199] io.confluent.mds.request.logger log - 73 * Server responded with a response on thread qtp1527668063-199null73 < 200null73 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:55:03,667 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/authenticate?renewer=kafka HTTP/1.1" 200 527 "https://localhost:9021/login" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36" 20
[INFO] 2023-11-08 14:55:03,667 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/authenticate?renewer=kafka HTTP/1.1" 200 527 "https://localhost:9021/login" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36" 20
[INFO] 2023-11-08 14:55:03,667 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/authenticate?renewer=kafka HTTP/1.1" 200 527 "https://localhost:9021/login" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36" 20
[TRACE] 2023-11-08 14:55:03,751 [qtp1527668063-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 192]
[TRACE] 2023-11-08 14:55:03,752 [qtp1527668063-192] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 192]
[DEBUG] 2023-11-08 14:55:03,754 [qtp1527668063-192] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:55:03,755 [qtp1527668063-192] io.confluent.mds.request.logger log - 74 * Server has received a request on thread qtp1527668063-192null74 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/metadataClusterIdnull74 > User Principal: c3
74 > Accept-Encoding: gzipnull74 > Host: kafka.confluent.svc.cluster.local:8090null74 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:03,756 [qtp1527668063-192] io.confluent.mds.request.logger log - 74 * Server responded with a response on thread qtp1527668063-192null74 < 200null74 < Content-Type: application/jsonnull9PWH12e6ROOpezLRDGV6Ag

[INFO] 2023-11-08 14:55:03,756 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/metadataClusterId HTTP/1.1" 200 22 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:03,756 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/metadataClusterId HTTP/1.1" 200 22 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:03,756 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.11 - c3 [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/metadataClusterId HTTP/1.1" 200 22 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:03,765 [qtp1527668063-185] io.confluent.mds.request.logger log - 75 * Server has received a request on thread qtp1527668063-185null75 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull75 > User Principal: 
75 > Accept-Encoding: gzipnull75 > Host: kafka.confluent.svc.cluster.local:8090null75 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:03,768 [qtp1527668063-185] io.confluent.mds.request.logger log - 75 * Server responded with a response on thread qtp1527668063-185null75 < 200null75 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:03,769 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:03,769 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:03,770 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 6
[DEBUG] 2023-11-08 14:55:03,788 [qtp1527668063-199] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:03,788 [qtp1527668063-199] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@51fb8bf2
[INFO] 2023-11-08 14:55:03,789 [qtp1527668063-199] io.confluent.mds.request.logger log - 76 * Server has received a request on thread qtp1527668063-199null76 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull76 > User Principal: testadmin
76 > Accept-Encoding: gzipnull76 > Connection: closenull76 > Content-Length: 115null76 > Content-Type: application/jsonnull76 > Host: kafka.confluent.svc.cluster.local:8090null76 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:03,805 [qtp1527668063-199] io.confluent.mds.request.logger log - 76 * Server responded with a response on thread qtp1527668063-199null76 < 200null76 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:03,806 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 19
[INFO] 2023-11-08 14:55:03,806 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 19
[INFO] 2023-11-08 14:55:03,806 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 19
[DEBUG] 2023-11-08 14:55:03,827 [qtp1527668063-182] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:03,827 [qtp1527668063-182] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@59412270
[INFO] 2023-11-08 14:55:03,828 [qtp1527668063-182] io.confluent.mds.request.logger log - 77 * Server has received a request on thread qtp1527668063-182null77 > PUT https://kafka.confluent.svc.cluster.local:8090/security/1.0/authorizenull77 > User Principal: testadmin
77 > Accept-Encoding: gzipnull77 > Content-Length: 515null77 > Content-Type: application/jsonnull77 > Host: kafka.confluent.svc.cluster.local:8090null77 > User-Agent: Jetty/9.4.44.v20210927null{"userPrincipal":"User:testadmin","host":"","actions":[{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourceName":"ControlCenterBrokerMetrics","resourceType":"ControlCenterBrokerMetrics","operation":"Read"},{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourceName":"ControlCenterAlerts","resourceType":"ControlCenterAlerts","operation":"Write"},{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourceName":"All","resourceType":"All","operation":"All"}]}

[INFO] 2023-11-08 14:55:03,849 [qtp1527668063-182] kafka.authorizer.logger logAuthorization - Principal = User:testadmin is Denied Operation = All from host = 127.0.0.1 on resource = All:LITERAL:All
[INFO] 2023-11-08 14:55:03,851 [qtp1527668063-182] io.confluent.mds.request.logger log - 77 * Server responded with a response on thread qtp1527668063-182null77 < 200null77 < Content-Type: application/jsonnull["ALLOWED","ALLOWED","DENIED"]

[INFO] 2023-11-08 14:55:03,852 [qtp1527668063-182] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "PUT /security/1.0/authorize HTTP/1.1" 200 30 "-" "Jetty/9.4.44.v20210927" 26
[INFO] 2023-11-08 14:55:03,852 [qtp1527668063-182] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "PUT /security/1.0/authorize HTTP/1.1" 200 30 "-" "Jetty/9.4.44.v20210927" 26
[INFO] 2023-11-08 14:55:03,852 [qtp1527668063-182] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "PUT /security/1.0/authorize HTTP/1.1" 200 30 "-" "Jetty/9.4.44.v20210927" 26
[DEBUG] 2023-11-08 14:55:03,877 [qtp1527668063-254] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:03,877 [qtp1527668063-255] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:03,877 [qtp1527668063-254] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@68da7e6f
[DEBUG] 2023-11-08 14:55:03,877 [qtp1527668063-255] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@2c021ac2
[INFO] 2023-11-08 14:55:03,878 [qtp1527668063-254] io.confluent.mds.request.logger log - 78 * Server has received a request on thread qtp1527668063-254null78 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull78 > User Principal: testadmin
78 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null78 > Cache-Control: no-cachenull78 > Connection: keep-alivenull78 > Content-Type: application/jsonnull78 > Host: kafka.confluent.svc.cluster.local:8090null78 > Pragma: no-cachenull78 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:03,878 [qtp1527668063-255] io.confluent.mds.request.logger log - 79 * Server has received a request on thread qtp1527668063-255null79 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull79 > User Principal: testadmin
79 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null79 > Cache-Control: no-cachenull79 > Connection: keep-alivenull79 > Content-Type: application/jsonnull79 > Host: kafka.confluent.svc.cluster.local:8090null79 > Pragma: no-cachenull79 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:03,879 [qtp1527668063-255] io.confluent.mds.request.logger log - 79 * Server responded with a response on thread qtp1527668063-255null79 < 200null79 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:55:03,880 [qtp1527668063-255] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 4
[INFO] 2023-11-08 14:55:03,880 [qtp1527668063-255] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 4
[INFO] 2023-11-08 14:55:03,880 [qtp1527668063-255] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 4
[INFO] 2023-11-08 14:55:03,881 [qtp1527668063-254] io.confluent.mds.request.logger log - 78 * Server responded with a response on thread qtp1527668063-254null78 < 200null78 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:55:03,882 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 650 "-" "Java/11.0.14.1" 6
[INFO] 2023-11-08 14:55:03,882 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 650 "-" "Java/11.0.14.1" 6
[INFO] 2023-11-08 14:55:03,882 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 650 "-" "Java/11.0.14.1" 6
[DEBUG] 2023-11-08 14:55:03,899 [qtp1527668063-183] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:03,899 [qtp1527668063-183] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@7d2c21e0
[INFO] 2023-11-08 14:55:03,900 [qtp1527668063-183] io.confluent.mds.request.logger log - 80 * Server has received a request on thread qtp1527668063-183null80 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/activenodes/httpsnull80 > User Principal: testadmin
80 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null80 > Cache-Control: no-cachenull80 > Connection: keep-alivenull80 > Content-Type: application/jsonnull80 > Host: kafka.confluent.svc.cluster.local:8090null80 > Pragma: no-cachenull80 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:03,901 [qtp1527668063-183] io.confluent.mds.request.logger log - 80 * Server responded with a response on thread qtp1527668063-183null80 < 200null80 < Content-Type: application/jsonnull["https://kafka-1.kafka.confluent.svc.cluster.local:8090","https://kafka-2.kafka.confluent.svc.cluster.local:8090","https://kafka-0.kafka.confluent.svc.cluster.local:8090"]

[INFO] 2023-11-08 14:55:03,902 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 4
[INFO] 2023-11-08 14:55:03,902 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 4
[INFO] 2023-11-08 14:55:03,902 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:03 +0000] "GET /security/1.0/activenodes/https HTTP/1.1" 200 172 "-" "Java/11.0.14.1" 4
[INFO] 2023-11-08 14:55:04,037 [qtp1527668063-189] io.confluent.mds.request.logger log - 81 * Server has received a request on thread qtp1527668063-189null81 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull81 > User Principal: 
81 > Accept-Encoding: gzipnull81 > Host: kafka.confluent.svc.cluster.local:8090null81 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,038 [qtp1527668063-189] io.confluent.mds.request.logger log - 81 * Server responded with a response on thread qtp1527668063-189null81 < 200null81 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,039 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,039 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,039 [qtp1527668063-189] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,042 [qtp1527668063-183] io.confluent.mds.request.logger log - 82 * Server has received a request on thread qtp1527668063-183null82 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull82 > User Principal: 
82 > Accept-Encoding: gzipnull82 > Host: kafka.confluent.svc.cluster.local:8090null82 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,043 [qtp1527668063-183] io.confluent.mds.request.logger log - 82 * Server responded with a response on thread qtp1527668063-183null82 < 200null82 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,044 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,044 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,044 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[DEBUG] 2023-11-08 14:55:04,051 [qtp1527668063-248] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,051 [qtp1527668063-248] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@646c1b13
[INFO] 2023-11-08 14:55:04,052 [qtp1527668063-248] io.confluent.mds.request.logger log - 83 * Server has received a request on thread qtp1527668063-248null83 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull83 > User Principal: testadmin
83 > Accept-Encoding: gzipnull83 > Connection: closenull83 > Content-Length: 115null83 > Content-Type: application/jsonnull83 > Host: kafka.confluent.svc.cluster.local:8090null83 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,054 [qtp1527668063-248] io.confluent.mds.request.logger log - 83 * Server responded with a response on thread qtp1527668063-248null83 < 200null83 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,054 [qtp1527668063-248] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,054 [qtp1527668063-248] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,055 [qtp1527668063-248] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:04,060 [qtp1527668063-241] io.confluent.mds.request.logger log - 84 * Server has received a request on thread qtp1527668063-241null84 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull84 > User Principal: 
84 > Accept-Encoding: gzipnull84 > Host: kafka.confluent.svc.cluster.local:8090null84 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,060 [qtp1527668063-246] io.confluent.mds.request.logger log - 85 * Server has received a request on thread qtp1527668063-246null85 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull85 > User Principal: 
85 > Accept-Encoding: gzipnull85 > Host: kafka.confluent.svc.cluster.local:8090null85 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,061 [qtp1527668063-259] io.confluent.mds.request.logger log - 86 * Server has received a request on thread qtp1527668063-259null86 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull86 > User Principal: 
86 > Accept-Encoding: gzipnull86 > Host: kafka.confluent.svc.cluster.local:8090null86 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,061 [qtp1527668063-246] io.confluent.mds.request.logger log - 85 * Server responded with a response on thread qtp1527668063-246null85 < 200null85 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,061 [qtp1527668063-241] io.confluent.mds.request.logger log - 84 * Server responded with a response on thread qtp1527668063-241null84 < 200null84 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,061 [qtp1527668063-259] io.confluent.mds.request.logger log - 86 * Server responded with a response on thread qtp1527668063-259null86 < 200null86 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,061 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,061 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,062 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,062 [qtp1527668063-241] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,062 [qtp1527668063-241] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,062 [qtp1527668063-259] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,062 [qtp1527668063-241] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,062 [qtp1527668063-259] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[DEBUG] 2023-11-08 14:55:04,062 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[INFO] 2023-11-08 14:55:04,062 [qtp1527668063-259] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[DEBUG] 2023-11-08 14:55:04,062 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@75ea03d0
[INFO] 2023-11-08 14:55:04,063 [qtp1527668063-194] io.confluent.mds.request.logger log - 87 * Server has received a request on thread qtp1527668063-194null87 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull87 > User Principal: testadmin
87 > Accept-Encoding: gzipnull87 > Connection: closenull87 > Content-Length: 115null87 > Content-Type: application/jsonnull87 > Host: kafka.confluent.svc.cluster.local:8090null87 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,064 [qtp1527668063-194] io.confluent.mds.request.logger log - 87 * Server responded with a response on thread qtp1527668063-194null87 < 200null87 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,065 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,065 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,065 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[DEBUG] 2023-11-08 14:55:04,082 [qtp1527668063-183] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,082 [qtp1527668063-199] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,083 [qtp1527668063-249] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,083 [qtp1527668063-183] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@e761ced
[DEBUG] 2023-11-08 14:55:04,083 [qtp1527668063-199] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@1f642047
[DEBUG] 2023-11-08 14:55:04,083 [qtp1527668063-249] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@4eefcbcf
[INFO] 2023-11-08 14:55:04,084 [qtp1527668063-183] io.confluent.mds.request.logger log - 88 * Server has received a request on thread qtp1527668063-183null88 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull88 > User Principal: testadmin
88 > Accept-Encoding: gzipnull88 > Connection: closenull88 > Content-Length: 115null88 > Content-Type: application/jsonnull88 > Host: kafka.confluent.svc.cluster.local:8090null88 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,084 [qtp1527668063-249] io.confluent.mds.request.logger log - 90 * Server has received a request on thread qtp1527668063-249null90 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull90 > User Principal: testadmin
90 > Accept-Encoding: gzipnull90 > Connection: closenull90 > Content-Length: 115null90 > Content-Type: application/jsonnull90 > Host: kafka.confluent.svc.cluster.local:8090null90 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,084 [qtp1527668063-199] io.confluent.mds.request.logger log - 89 * Server has received a request on thread qtp1527668063-199null89 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull89 > User Principal: testadmin
89 > Accept-Encoding: gzipnull89 > Connection: closenull89 > Content-Length: 115null89 > Content-Type: application/jsonnull89 > Host: kafka.confluent.svc.cluster.local:8090null89 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,086 [qtp1527668063-199] io.confluent.mds.request.logger log - 89 * Server responded with a response on thread qtp1527668063-199null89 < 200null89 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,086 [qtp1527668063-249] io.confluent.mds.request.logger log - 90 * Server responded with a response on thread qtp1527668063-249null90 < 200null90 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,086 [qtp1527668063-183] io.confluent.mds.request.logger log - 88 * Server responded with a response on thread qtp1527668063-183null88 < 200null88 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,087 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,087 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,087 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,087 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,087 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,087 [qtp1527668063-183] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,087 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,087 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,087 [qtp1527668063-249] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[DEBUG] 2023-11-08 14:55:04,099 [qtp1527668063-188] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,099 [qtp1527668063-188] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@2db7e38c
[INFO] 2023-11-08 14:55:04,100 [qtp1527668063-188] io.confluent.mds.request.logger log - 91 * Server has received a request on thread qtp1527668063-188null91 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/managed/clusters/principal/User:testadminnull91 > User Principal: testadmin
91 > Accept: */*null91 > Accept-Encoding: gzip, deflate, brnull91 > Accept-Language: en-GB,en-US;q=0.9,en;q=0.8null91 > Host: kafka.confluent.svc.cluster.local:8090null91 > Referer: https://localhost:9021/clustersnull91 > sec-ch-ua: "Google Chrome";v="119", "Chromium";v="119", "Not?A_Brand";v="24"null91 > sec-ch-ua-mobile: ?0null91 > sec-ch-ua-platform: "macOS"null91 > sec-fetch-dest: emptynull91 > sec-fetch-mode: corsnull91 > sec-fetch-site: same-originnull91 > User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36null91 > Via: 2.0 controlcenter-0null91 > X-Forwarded-For: 127.0.0.1null91 > X-Forwarded-Host: localhost:9021null91 > X-Forwarded-Proto: httpsnull91 > X-Forwarded-Server: 127.0.0.1null91 > x-requested-by: 28ac9d86-2317-4b2f-bbc6-22c1316157danull91 > x-requested-with: undefinednull
[INFO] 2023-11-08 14:55:04,106 [qtp1527668063-188] io.confluent.mds.request.logger log - 91 * Server responded with a response on thread qtp1527668063-188null91 < 200null91 < Content-Type: application/jsonnull[{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-cluster":"confluent.connect"}},{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","ksql-cluster":"confluent.ksqldb_"}},{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","schema-registry-cluster":"id_schemaregistry_confluent"}}]

[INFO] 2023-11-08 14:55:04,107 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/lookup/managed/clusters/principal/User:testadmin HTTP/1.1" 200 141 "https://localhost:9021/clusters" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36" 10
[INFO] 2023-11-08 14:55:04,107 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/lookup/managed/clusters/principal/User:testadmin HTTP/1.1" 200 141 "https://localhost:9021/clusters" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36" 10
[INFO] 2023-11-08 14:55:04,108 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/lookup/managed/clusters/principal/User:testadmin HTTP/1.1" 200 141 "https://localhost:9021/clusters" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36" 11
[INFO] 2023-11-08 14:55:04,140 [qtp1527668063-187] io.confluent.mds.request.logger log - 92 * Server has received a request on thread qtp1527668063-187null92 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull92 > User Principal: 
92 > Accept-Encoding: gzipnull92 > Host: kafka.confluent.svc.cluster.local:8090null92 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,140 [qtp1527668063-187] io.confluent.mds.request.logger log - 92 * Server responded with a response on thread qtp1527668063-187null92 < 200null92 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,141 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,141 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,141 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[DEBUG] 2023-11-08 14:55:04,155 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,156 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@25992f4f
[INFO] 2023-11-08 14:55:04,157 [qtp1527668063-194] io.confluent.mds.request.logger log - 93 * Server has received a request on thread qtp1527668063-194null93 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull93 > User Principal: testadmin
93 > Accept-Encoding: gzipnull93 > Connection: closenull93 > Content-Length: 115null93 > Content-Type: application/jsonnull93 > Host: kafka.confluent.svc.cluster.local:8090null93 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,159 [qtp1527668063-194] io.confluent.mds.request.logger log - 93 * Server responded with a response on thread qtp1527668063-194null93 < 200null93 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,163 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,163 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,163 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,178 [qtp1527668063-188] io.confluent.mds.request.logger log - 94 * Server has received a request on thread qtp1527668063-188null94 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull94 > User Principal: 
94 > Accept-Encoding: gzipnull94 > Host: kafka.confluent.svc.cluster.local:8090null94 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,178 [qtp1527668063-247] io.confluent.mds.request.logger log - 95 * Server has received a request on thread qtp1527668063-247null95 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull95 > User Principal: 
95 > Accept-Encoding: gzipnull95 > Host: kafka.confluent.svc.cluster.local:8090null95 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,179 [qtp1527668063-188] io.confluent.mds.request.logger log - 94 * Server responded with a response on thread qtp1527668063-188null94 < 200null94 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,179 [qtp1527668063-247] io.confluent.mds.request.logger log - 95 * Server responded with a response on thread qtp1527668063-247null95 < 200null95 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,180 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,180 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,180 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,180 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,180 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,180 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,183 [qtp1527668063-258] io.confluent.mds.request.logger log - 96 * Server has received a request on thread qtp1527668063-258null96 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull96 > User Principal: 
96 > Accept-Encoding: gzipnull96 > Host: kafka.confluent.svc.cluster.local:8090null96 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,183 [qtp1527668063-258] io.confluent.mds.request.logger log - 96 * Server responded with a response on thread qtp1527668063-258null96 < 200null96 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,184 [qtp1527668063-258] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,185 [qtp1527668063-258] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,185 [qtp1527668063-258] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,185 [qtp1527668063-253] io.confluent.mds.request.logger log - 97 * Server has received a request on thread qtp1527668063-253null97 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull97 > User Principal: 
97 > Accept-Encoding: gzipnull97 > Host: kafka.confluent.svc.cluster.local:8090null97 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,186 [qtp1527668063-253] io.confluent.mds.request.logger log - 97 * Server responded with a response on thread qtp1527668063-253null97 < 200null97 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,187 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,187 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,187 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,191 [qtp1527668063-239] io.confluent.mds.request.logger log - 98 * Server has received a request on thread qtp1527668063-239null98 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull98 > User Principal: 
98 > Accept-Encoding: gzipnull98 > Host: kafka.confluent.svc.cluster.local:8090null98 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,191 [qtp1527668063-239] io.confluent.mds.request.logger log - 98 * Server responded with a response on thread qtp1527668063-239null98 < 200null98 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,192 [qtp1527668063-239] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,192 [qtp1527668063-239] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,192 [qtp1527668063-239] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[DEBUG] 2023-11-08 14:55:04,200 [qtp1527668063-244] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,200 [qtp1527668063-244] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@63a2a265
[DEBUG] 2023-11-08 14:55:04,201 [qtp1527668063-198] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[INFO] 2023-11-08 14:55:04,201 [qtp1527668063-244] io.confluent.mds.request.logger log - 99 * Server has received a request on thread qtp1527668063-244null99 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull99 > User Principal: testadmin
99 > Accept-Encoding: gzipnull99 > Connection: closenull99 > Content-Length: 115null99 > Content-Type: application/jsonnull99 > Host: kafka.confluent.svc.cluster.local:8090null99 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[DEBUG] 2023-11-08 14:55:04,202 [qtp1527668063-198] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@2e29a0d2
[DEBUG] 2023-11-08 14:55:04,202 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[INFO] 2023-11-08 14:55:04,203 [qtp1527668063-198] io.confluent.mds.request.logger log - 100 * Server has received a request on thread qtp1527668063-198null100 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull100 > User Principal: testadmin
100 > Accept-Encoding: gzipnull100 > Connection: closenull100 > Content-Length: 144null100 > Content-Type: application/jsonnull100 > Host: kafka.confluent.svc.cluster.local:8090null100 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":["id_schemaregistry_confluent"],"ksql-clusters":[]}]

[DEBUG] 2023-11-08 14:55:04,203 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@75a338fd
[DEBUG] 2023-11-08 14:55:04,203 [qtp1527668063-247] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,206 [qtp1527668063-185] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,206 [qtp1527668063-247] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@6e430198
[INFO] 2023-11-08 14:55:04,206 [qtp1527668063-194] io.confluent.mds.request.logger log - 101 * Server has received a request on thread qtp1527668063-194null101 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull101 > User Principal: testadmin
101 > Accept-Encoding: gzipnull101 > Connection: closenull101 > Content-Length: 115null101 > Content-Type: application/jsonnull101 > Host: kafka.confluent.svc.cluster.local:8090null101 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[DEBUG] 2023-11-08 14:55:04,206 [qtp1527668063-185] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@791b7548
[INFO] 2023-11-08 14:55:04,209 [qtp1527668063-244] io.confluent.mds.request.logger log - 99 * Server responded with a response on thread qtp1527668063-244null99 < 200null99 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,209 [qtp1527668063-247] io.confluent.mds.request.logger log - 102 * Server has received a request on thread qtp1527668063-247null102 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull102 > User Principal: testadmin
102 > Accept-Encoding: gzipnull102 > Connection: closenull102 > Content-Length: 144null102 > Content-Type: application/jsonnull102 > Host: kafka.confluent.svc.cluster.local:8090null102 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":["id_schemaregistry_confluent"],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,209 [qtp1527668063-185] io.confluent.mds.request.logger log - 103 * Server has received a request on thread qtp1527668063-185null103 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull103 > User Principal: testadmin
103 > Accept-Encoding: gzipnull103 > Connection: closenull103 > Content-Length: 115null103 > Content-Type: application/jsonnull103 > Host: kafka.confluent.svc.cluster.local:8090null103 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,209 [qtp1527668063-198] io.confluent.mds.request.logger log - 100 * Server responded with a response on thread qtp1527668063-198null100 < 200null100 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[{"id":"id_schemaregistry_confluent","visible":true,"clusterName":null}],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,210 [qtp1527668063-244] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 11
[INFO] 2023-11-08 14:55:04,210 [qtp1527668063-244] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 11
[INFO] 2023-11-08 14:55:04,210 [qtp1527668063-244] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 11
[INFO] 2023-11-08 14:55:04,210 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 161 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,210 [qtp1527668063-194] io.confluent.mds.request.logger log - 101 * Server responded with a response on thread qtp1527668063-194null101 < 200null101 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,210 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 161 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,210 [qtp1527668063-185] io.confluent.mds.request.logger log - 103 * Server responded with a response on thread qtp1527668063-185null103 < 200null103 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,210 [qtp1527668063-247] io.confluent.mds.request.logger log - 102 * Server responded with a response on thread qtp1527668063-247null102 < 200null102 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[{"id":"id_schemaregistry_confluent","visible":true,"clusterName":null}],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,210 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 161 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,211 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,211 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,211 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 8
[INFO] 2023-11-08 14:55:04,211 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,211 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 8
[INFO] 2023-11-08 14:55:04,211 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 161 "-" "Jetty/9.4.44.v20210927" 8
[INFO] 2023-11-08 14:55:04,211 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 8
[INFO] 2023-11-08 14:55:04,212 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 161 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,212 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 161 "-" "Jetty/9.4.44.v20210927" 9
[INFO] 2023-11-08 14:55:04,228 [qtp1527668063-251] io.confluent.mds.request.logger log - 104 * Server has received a request on thread qtp1527668063-251null104 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull104 > User Principal: 
104 > Accept-Encoding: gzipnull104 > Host: kafka.confluent.svc.cluster.local:8090null104 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,228 [qtp1527668063-185] io.confluent.mds.request.logger log - 105 * Server has received a request on thread qtp1527668063-185null105 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull105 > User Principal: 
105 > Accept-Encoding: gzipnull105 > Host: kafka.confluent.svc.cluster.local:8090null105 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,228 [qtp1527668063-251] io.confluent.mds.request.logger log - 104 * Server responded with a response on thread qtp1527668063-251null104 < 200null104 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,229 [qtp1527668063-185] io.confluent.mds.request.logger log - 105 * Server responded with a response on thread qtp1527668063-185null105 < 200null105 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,229 [qtp1527668063-251] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,229 [qtp1527668063-251] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,229 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,229 [qtp1527668063-251] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,230 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,230 [qtp1527668063-185] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[DEBUG] 2023-11-08 14:55:04,235 [qtp1527668063-188] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,236 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,236 [qtp1527668063-188] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@579156f7
[DEBUG] 2023-11-08 14:55:04,236 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@5072411a
[INFO] 2023-11-08 14:55:04,237 [qtp1527668063-188] io.confluent.mds.request.logger log - 106 * Server has received a request on thread qtp1527668063-188null106 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull106 > User Principal: testadmin
106 > Accept-Encoding: gzipnull106 > Connection: closenull106 > Content-Length: 134null106 > Content-Type: application/jsonnull106 > Host: kafka.confluent.svc.cluster.local:8090null106 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":["confluent.connect"],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,237 [qtp1527668063-194] io.confluent.mds.request.logger log - 107 * Server has received a request on thread qtp1527668063-194null107 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull107 > User Principal: testadmin
107 > Accept-Encoding: gzipnull107 > Connection: closenull107 > Content-Length: 134null107 > Content-Type: application/jsonnull107 > Host: kafka.confluent.svc.cluster.local:8090null107 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":["confluent.connect"],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,238 [qtp1527668063-194] io.confluent.mds.request.logger log - 107 * Server responded with a response on thread qtp1527668063-194null107 < 200null107 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[{"id":"confluent.connect","visible":true,"clusterName":null}],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,238 [qtp1527668063-188] io.confluent.mds.request.logger log - 106 * Server responded with a response on thread qtp1527668063-188null106 < 200null106 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[{"id":"confluent.connect","visible":true,"clusterName":null}],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,239 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 156 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:04,239 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 156 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:04,239 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 156 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:04,239 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 156 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:04,239 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 156 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:04,239 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 156 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:04,260 [qtp1527668063-192] io.confluent.mds.request.logger log - 108 * Server has received a request on thread qtp1527668063-192null108 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull108 > User Principal: 
108 > Accept-Encoding: gzipnull108 > Host: kafka.confluent.svc.cluster.local:8090null108 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,260 [qtp1527668063-187] io.confluent.mds.request.logger log - 109 * Server has received a request on thread qtp1527668063-187null109 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull109 > User Principal: 
109 > Accept-Encoding: gzipnull109 > Host: kafka.confluent.svc.cluster.local:8090null109 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,261 [qtp1527668063-192] io.confluent.mds.request.logger log - 108 * Server responded with a response on thread qtp1527668063-192null108 < 200null108 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,262 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,262 [qtp1527668063-187] io.confluent.mds.request.logger log - 109 * Server responded with a response on thread qtp1527668063-187null109 < 200null109 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,262 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,262 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,262 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,263 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,263 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[DEBUG] 2023-11-08 14:55:04,267 [qtp1527668063-258] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,267 [qtp1527668063-199] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,267 [qtp1527668063-199] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@14288e87
[DEBUG] 2023-11-08 14:55:04,267 [qtp1527668063-258] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@62aa9966
[INFO] 2023-11-08 14:55:04,268 [qtp1527668063-199] io.confluent.mds.request.logger log - 110 * Server has received a request on thread qtp1527668063-199null110 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull110 > User Principal: testadmin
110 > Accept-Encoding: gzipnull110 > Connection: closenull110 > Content-Length: 134null110 > Content-Type: application/jsonnull110 > Host: kafka.confluent.svc.cluster.local:8090null110 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":["confluent.ksqldb_"]}]

[INFO] 2023-11-08 14:55:04,268 [qtp1527668063-258] io.confluent.mds.request.logger log - 111 * Server has received a request on thread qtp1527668063-258null111 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull111 > User Principal: testadmin
111 > Accept-Encoding: gzipnull111 > Connection: closenull111 > Content-Length: 134null111 > Content-Type: application/jsonnull111 > Host: kafka.confluent.svc.cluster.local:8090null111 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":["confluent.ksqldb_"]}]

[INFO] 2023-11-08 14:55:04,270 [qtp1527668063-199] io.confluent.mds.request.logger log - 110 * Server responded with a response on thread qtp1527668063-199null110 < 200null110 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[{"id":"confluent.ksqldb_","visible":true,"clusterName":null}]}]

[INFO] 2023-11-08 14:55:04,270 [qtp1527668063-258] io.confluent.mds.request.logger log - 111 * Server responded with a response on thread qtp1527668063-258null111 < 200null111 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[{"id":"confluent.ksqldb_","visible":true,"clusterName":null}]}]

[INFO] 2023-11-08 14:55:04,271 [qtp1527668063-258] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 159 "-" "Jetty/9.4.44.v20210927" 6
[INFO] 2023-11-08 14:55:04,271 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 159 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,272 [qtp1527668063-258] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 159 "-" "Jetty/9.4.44.v20210927" 7
[INFO] 2023-11-08 14:55:04,272 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 159 "-" "Jetty/9.4.44.v20210927" 6
[INFO] 2023-11-08 14:55:04,272 [qtp1527668063-258] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 159 "-" "Jetty/9.4.44.v20210927" 7
[INFO] 2023-11-08 14:55:04,272 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 159 "-" "Jetty/9.4.44.v20210927" 6
[INFO] 2023-11-08 14:55:04,282 [qtp1527668063-180] io.confluent.mds.request.logger log - 112 * Server has received a request on thread qtp1527668063-180null112 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull112 > User Principal: 
112 > Accept-Encoding: gzipnull112 > Host: kafka.confluent.svc.cluster.local:8090null112 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,283 [qtp1527668063-180] io.confluent.mds.request.logger log - 112 * Server responded with a response on thread qtp1527668063-180null112 < 200null112 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,284 [qtp1527668063-180] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,284 [qtp1527668063-180] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,284 [qtp1527668063-180] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[DEBUG] 2023-11-08 14:55:04,288 [qtp1527668063-247] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,288 [qtp1527668063-247] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@1694e95a
[INFO] 2023-11-08 14:55:04,289 [qtp1527668063-247] io.confluent.mds.request.logger log - 113 * Server has received a request on thread qtp1527668063-247null113 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull113 > User Principal: testadmin
113 > Accept-Encoding: gzipnull113 > Connection: closenull113 > Content-Length: 115null113 > Content-Type: application/jsonnull113 > Host: kafka.confluent.svc.cluster.local:8090null113 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,291 [qtp1527668063-247] io.confluent.mds.request.logger log - 113 * Server responded with a response on thread qtp1527668063-247null113 < 200null113 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,292 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,293 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 6
[INFO] 2023-11-08 14:55:04,293 [qtp1527668063-247] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 6
[INFO] 2023-11-08 14:55:04,397 [qtp1527668063-253] io.confluent.mds.request.logger log - 114 * Server has received a request on thread qtp1527668063-253null114 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull114 > User Principal: 
114 > Accept-Encoding: gzipnull114 > Host: kafka.confluent.svc.cluster.local:8090null114 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,398 [qtp1527668063-253] io.confluent.mds.request.logger log - 114 * Server responded with a response on thread qtp1527668063-253null114 < 200null114 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,399 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,399 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,399 [qtp1527668063-253] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[DEBUG] 2023-11-08 14:55:04,402 [qtp1527668063-242] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,403 [qtp1527668063-242] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@6bce245e
[INFO] 2023-11-08 14:55:04,403 [qtp1527668063-242] io.confluent.mds.request.logger log - 115 * Server has received a request on thread qtp1527668063-242null115 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull115 > User Principal: testadmin
115 > Accept-Encoding: gzipnull115 > Connection: closenull115 > Content-Length: 115null115 > Content-Type: application/jsonnull115 > Host: kafka.confluent.svc.cluster.local:8090null115 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,405 [qtp1527668063-242] io.confluent.mds.request.logger log - 115 * Server responded with a response on thread qtp1527668063-242null115 < 200null115 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,406 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:04,407 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,407 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,435 [qtp1527668063-246] io.confluent.mds.request.logger log - 116 * Server has received a request on thread qtp1527668063-246null116 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull116 > User Principal: 
116 > Accept-Encoding: gzipnull116 > Host: kafka.confluent.svc.cluster.local:8090null116 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,435 [qtp1527668063-248] io.confluent.mds.request.logger log - 117 * Server has received a request on thread qtp1527668063-248null117 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull117 > User Principal: 
117 > Accept-Encoding: gzipnull117 > Host: kafka.confluent.svc.cluster.local:8090null117 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:04,436 [qtp1527668063-246] io.confluent.mds.request.logger log - 116 * Server responded with a response on thread qtp1527668063-246null116 < 200null116 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,436 [qtp1527668063-248] io.confluent.mds.request.logger log - 117 * Server responded with a response on thread qtp1527668063-248null117 < 200null117 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:04,436 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,436 [qtp1527668063-248] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,436 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:04,437 [qtp1527668063-248] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,437 [qtp1527668063-246] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:04,437 [qtp1527668063-248] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:04 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[DEBUG] 2023-11-08 14:55:04,443 [qtp1527668063-244] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,443 [qtp1527668063-198] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:04,443 [qtp1527668063-244] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@430f70e8
[DEBUG] 2023-11-08 14:55:04,443 [qtp1527668063-198] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@a5c672d
[INFO] 2023-11-08 14:55:04,444 [qtp1527668063-198] io.confluent.mds.request.logger log - 119 * Server has received a request on thread qtp1527668063-198null119 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull119 > User Principal: testadmin
119 > Accept-Encoding: gzipnull119 > Connection: closenull119 > Content-Length: 115null119 > Content-Type: application/jsonnull119 > Host: kafka.confluent.svc.cluster.local:8090null119 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,444 [qtp1527668063-244] io.confluent.mds.request.logger log - 118 * Server has received a request on thread qtp1527668063-244null118 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull118 > User Principal: testadmin
118 > Accept-Encoding: gzipnull118 > Connection: closenull118 > Content-Length: 115null118 > Content-Type: application/jsonnull118 > Host: kafka.confluent.svc.cluster.local:8090null118 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,446 [qtp1527668063-198] io.confluent.mds.request.logger log - 119 * Server responded with a response on thread qtp1527668063-198null119 < 200null119 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,446 [qtp1527668063-244] io.confluent.mds.request.logger log - 118 * Server responded with a response on thread qtp1527668063-244null118 < 200null118 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:04,447 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,447 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,447 [qtp1527668063-244] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,447 [qtp1527668063-198] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,447 [qtp1527668063-244] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:04,447 [qtp1527668063-244] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:04 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 5
[INFO] 2023-11-08 14:55:06,305 [qtp1527668063-181] io.confluent.mds.request.logger log - 120 * Server has received a request on thread qtp1527668063-181null120 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull120 > User Principal: 
120 > Accept-Encoding: gzipnull120 > Host: kafka.confluent.svc.cluster.local:8090null120 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:06,306 [qtp1527668063-181] io.confluent.mds.request.logger log - 120 * Server responded with a response on thread qtp1527668063-181null120 < 200null120 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:06,307 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:06 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:06,307 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:06 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:06,307 [qtp1527668063-181] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:06 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 3
[DEBUG] 2023-11-08 14:55:06,310 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:06,310 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@a85871a
[INFO] 2023-11-08 14:55:06,311 [qtp1527668063-194] io.confluent.mds.request.logger log - 121 * Server has received a request on thread qtp1527668063-194null121 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull121 > User Principal: testadmin
121 > Accept-Encoding: gzipnull121 > Connection: closenull121 > Content-Length: 115null121 > Content-Type: application/jsonnull121 > Host: kafka.confluent.svc.cluster.local:8090null121 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:06,312 [qtp1527668063-194] io.confluent.mds.request.logger log - 121 * Server responded with a response on thread qtp1527668063-194null121 < 200null121 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:06,313 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:06 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:06,313 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:06 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:06,313 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:06 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 4
[DEBUG] 2023-11-08 14:55:06,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:06,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:55:06,439 [qtp1527668063-260] io.confluent.mds.request.logger log - 122 * Server has received a request on thread qtp1527668063-260null122 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull122 > User Principal: 
122 > Accept-Encoding: gzipnull122 > Host: kafka.confluent.svc.cluster.local:8090null122 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:06,440 [qtp1527668063-260] io.confluent.mds.request.logger log - 122 * Server responded with a response on thread qtp1527668063-260null122 < 200null122 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:06,440 [qtp1527668063-260] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:06 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:06,440 [qtp1527668063-260] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:06 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:06,440 [qtp1527668063-260] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:06 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[DEBUG] 2023-11-08 14:55:06,443 [qtp1527668063-188] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:06,444 [qtp1527668063-188] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@490be394
[INFO] 2023-11-08 14:55:06,445 [qtp1527668063-188] io.confluent.mds.request.logger log - 123 * Server has received a request on thread qtp1527668063-188null123 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull123 > User Principal: testadmin
123 > Accept-Encoding: gzipnull123 > Connection: closenull123 > Content-Length: 115null123 > Content-Type: application/jsonnull123 > Host: kafka.confluent.svc.cluster.local:8090null123 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:06,446 [qtp1527668063-188] io.confluent.mds.request.logger log - 123 * Server responded with a response on thread qtp1527668063-188null123 < 200null123 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:06,447 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:06 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:06,447 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:06 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:06,447 [qtp1527668063-188] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:06 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 4
[DEBUG] 2023-11-08 14:55:06,454 [qtp1527668063-243] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:06,454 [qtp1527668063-243] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@6cc4546d
[INFO] 2023-11-08 14:55:06,455 [qtp1527668063-243] io.confluent.mds.request.logger log - 124 * Server has received a request on thread qtp1527668063-243null124 > PUT https://kafka.confluent.svc.cluster.local:8090/security/1.0/authorizenull124 > User Principal: testadmin
124 > Accept-Encoding: gzipnull124 > Content-Length: 515null124 > Content-Type: application/jsonnull124 > Host: kafka.confluent.svc.cluster.local:8090null124 > User-Agent: Jetty/9.4.44.v20210927null{"userPrincipal":"User:testadmin","host":"","actions":[{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourceName":"ControlCenterBrokerMetrics","resourceType":"ControlCenterBrokerMetrics","operation":"Read"},{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourceName":"ControlCenterAlerts","resourceType":"ControlCenterAlerts","operation":"Write"},{"scope":{"clusters":{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag"}},"resourceName":"All","resourceType":"All","operation":"All"}]}

[INFO] 2023-11-08 14:55:06,459 [qtp1527668063-243] kafka.authorizer.logger logAuthorization - Principal = User:testadmin is Denied Operation = All from host = 127.0.0.1 on resource = All:LITERAL:All
[INFO] 2023-11-08 14:55:06,460 [qtp1527668063-243] io.confluent.mds.request.logger log - 124 * Server responded with a response on thread qtp1527668063-243null124 < 200null124 < Content-Type: application/jsonnull["ALLOWED","ALLOWED","DENIED"]

[INFO] 2023-11-08 14:55:06,461 [qtp1527668063-243] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:06 +0000] "PUT /security/1.0/authorize HTTP/1.1" 200 30 "-" "Jetty/9.4.44.v20210927" 8
[INFO] 2023-11-08 14:55:06,461 [qtp1527668063-243] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:06 +0000] "PUT /security/1.0/authorize HTTP/1.1" 200 30 "-" "Jetty/9.4.44.v20210927" 8
[INFO] 2023-11-08 14:55:06,461 [qtp1527668063-243] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:06 +0000] "PUT /security/1.0/authorize HTTP/1.1" 200 30 "-" "Jetty/9.4.44.v20210927" 8
[INFO] 2023-11-08 14:55:08,922 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:08 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 2
[INFO] 2023-11-08 14:55:08,923 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:08 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[INFO] 2023-11-08 14:55:08,923 [qtp1527668063-242] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:08 +0000] "GET /v1/metadata/id HTTP/2.0" 200 105 "-" "armeria/1.13.4" 3
[DEBUG] 2023-11-08 14:55:09,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:09,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:12,387 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:12,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:15,388 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:15,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:55:15,960 [ldap-group-manager] io.confluent.security.auth.provider.ldap.LdapGroupManager search - Searching groups with base dc=test,dc=com filter (objectClass=group): 
[DEBUG] 2023-11-08 14:55:15,961 [ldap-group-manager] io.confluent.security.auth.provider.ldap.LdapGroupManager searchAndProcessResults - Search completed, group cache is {}
[DEBUG] 2023-11-08 14:55:15,962 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-0]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,962 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-1]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,962 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-2]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,962 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-3]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,962 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-4]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,962 [auth-writer-mgmt-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter writeStatus - [PartitionWriter _confluent-metadata-auth-5]writeStatus generation 1 status StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,965 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-0]Status record of generation 1 for partition _confluent-metadata-auth-0 written at offset 6
[DEBUG] 2023-11-08 14:55:15,965 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=0) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-0:6 key StatusKey(partition=0) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,965 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-2]Status record of generation 1 for partition _confluent-metadata-auth-2 written at offset 7
[DEBUG] 2023-11-08 14:55:15,965 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-0]Received new generation id 1 for partition writer _confluent-metadata-auth-0 at offset 6 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:55:15,965 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-1]Status record of generation 1 for partition _confluent-metadata-auth-1 written at offset 10
[DEBUG] 2023-11-08 14:55:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=1) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-1:10 key StatusKey(partition=1) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-1]Received new generation id 1 for partition writer _confluent-metadata-auth-1 at offset 10 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:55:15,966 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=2) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-2:7 key StatusKey(partition=2) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,966 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-2]Received new generation id 1 for partition writer _confluent-metadata-auth-2 at offset 7 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:55:15,967 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-3]Status record of generation 1 for partition _confluent-metadata-auth-3 written at offset 9
[DEBUG] 2023-11-08 14:55:15,967 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=3) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-3:9 key StatusKey(partition=3) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,967 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-3]Received new generation id 1 for partition writer _confluent-metadata-auth-3 at offset 9 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:55:15,968 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-5]Status record of generation 1 for partition _confluent-metadata-auth-5 written at offset 11
[DEBUG] 2023-11-08 14:55:15,968 [kafka-producer-network-thread | _confluent-metadata-auth-producer-0] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusRecordWriteCompletion - [PartitionWriter _confluent-metadata-auth-4]Status record of generation 1 for partition _confluent-metadata-auth-4 written at offset 8
[DEBUG] 2023-11-08 14:55:15,968 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=5) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,968 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-5:11 key StatusKey(partition=5) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,968 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-5]Received new generation id 1 for partition writer _confluent-metadata-auth-5 at offset 11 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:55:15,969 [auth-reader-1] io.confluent.security.auth.store.cache.DefaultAuthCache put - Processing status with key StatusKey(partition=4) value StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,969 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaReader processConsumerRecord - Processing new record _confluent-metadata-auth-4:8 key StatusKey(partition=4) newValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null) oldValue StatusValue(status=INITIALIZED, generationId=1, writerBrokerId=0, errorMessage=null)
[DEBUG] 2023-11-08 14:55:15,969 [auth-reader-1] io.confluent.security.store.kafka.clients.KafkaPartitionWriter onStatusConsumed - [PartitionWriter _confluent-metadata-auth-4]Received new generation id 1 for partition writer _confluent-metadata-auth-4 at offset 8 status INITIALIZED generation 1
[DEBUG] 2023-11-08 14:55:18,388 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:18,388 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:55:18,982 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:55:18,983 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:55:18,983 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:55:18,997 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:18,997 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:55:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:55:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:55:19,000 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455319000
[WARN] 2023-11-08 14:55:19,002 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,002 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,002 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,002 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,002 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:55:19,003 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:55:19,004 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:55:19,004 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455319003
[INFO] 2023-11-08 14:55:19,013 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:55:19,025 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:55:19,026 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:55:19,026 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:55:19,026 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:55:19,026 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:55:19,026 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:55:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:55:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:55:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:55:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:55:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:55:19,027 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:55:19,027 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:55:21,388 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:21,389 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:55:24,356 [qtp1527668063-250] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 250]
[TRACE] 2023-11-08 14:55:24,358 [qtp1527668063-250] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 250]
[DEBUG] 2023-11-08 14:55:24,372 [qtp1527668063-250] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:55:24,373 [qtp1527668063-250] io.confluent.mds.request.logger log - 125 * Server has received a request on thread qtp1527668063-250null125 > GET https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull125 > User Principal: c3
125 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null125 > Cache-Control: no-cachenull125 > Connection: keep-alivenull125 > Content-Type: application/jsonnull125 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null125 > Pragma: no-cachenull125 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:24,377 [qtp1527668063-250] io.confluent.mds.request.logger log - 125 * Server responded with a response on thread qtp1527668063-250null125 < 200null125 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:55:24,378 [qtp1527668063-250] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:55:24 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 23
[INFO] 2023-11-08 14:55:24,378 [qtp1527668063-250] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:55:24 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 23
[INFO] 2023-11-08 14:55:24,378 [qtp1527668063-250] io.confluent.rest-utils.requests write - 10.40.0.16 - c3 [08/Nov/2023:14:55:24 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 23
[DEBUG] 2023-11-08 14:55:24,388 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:24,389 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:27,388 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:27,389 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:30,389 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:30,389 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:55:30,466 [qtp1527668063-241] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 241]
[TRACE] 2023-11-08 14:55:30,468 [qtp1527668063-241] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 241]
[DEBUG] 2023-11-08 14:55:30,470 [qtp1527668063-241] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:55:30,471 [qtp1527668063-241] io.confluent.mds.request.logger log - 126 * Server has received a request on thread qtp1527668063-241null126 > GET https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull126 > User Principal: c3
126 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null126 > Cache-Control: no-cachenull126 > Connection: keep-alivenull126 > Content-Type: application/jsonnull126 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null126 > Pragma: no-cachenull126 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:30,474 [qtp1527668063-241] io.confluent.mds.request.logger log - 126 * Server responded with a response on thread qtp1527668063-241null126 < 200null126 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:55:30,475 [qtp1527668063-241] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:55:30 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:55:30,475 [qtp1527668063-241] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:55:30 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:55:30,475 [qtp1527668063-241] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:55:30 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[DEBUG] 2023-11-08 14:55:33,389 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:33,390 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:55:34,131 [qtp1527668063-199] io.confluent.mds.request.logger log - 127 * Server has received a request on thread qtp1527668063-199null127 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull127 > User Principal: 
127 > Accept-Encoding: gzipnull127 > Host: kafka.confluent.svc.cluster.local:8090null127 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:34,132 [qtp1527668063-199] io.confluent.mds.request.logger log - 127 * Server responded with a response on thread qtp1527668063-199null127 < 200null127 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:34,133 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:34,133 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:34,133 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 4
[INFO] 2023-11-08 14:55:34,134 [qtp1527668063-194] io.confluent.mds.request.logger log - 128 * Server has received a request on thread qtp1527668063-194null128 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull128 > User Principal: 
128 > Accept-Encoding: gzipnull128 > Host: kafka.confluent.svc.cluster.local:8090null128 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:34,135 [qtp1527668063-194] io.confluent.mds.request.logger log - 128 * Server responded with a response on thread qtp1527668063-194null128 < 200null128 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:34,135 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 1
[INFO] 2023-11-08 14:55:34,135 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 1
[INFO] 2023-11-08 14:55:34,136 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:34,137 [qtp1527668063-199] io.confluent.mds.request.logger log - 129 * Server has received a request on thread qtp1527668063-199null129 > GET https://kafka.confluent.svc.cluster.local:8090/security/1.0/featuresnull129 > User Principal: 
129 > Accept-Encoding: gzipnull129 > Host: kafka.confluent.svc.cluster.local:8090null129 > User-Agent: Jetty/9.4.44.v20210927null
[INFO] 2023-11-08 14:55:34,138 [qtp1527668063-199] io.confluent.mds.request.logger log - 129 * Server responded with a response on thread qtp1527668063-199null129 < 200null129 < Content-Type: application/jsonnull{"features":{"basic.auth.1.enabled":true,"centralized.acls.api.1.enabled":true,"cluster.registry.crud.api.1.enabled":true,"cluster.registry.nice.names.1.enabled":true,"cluster.registry.nice.names.2.enabled":true,"mds.authorize.api.1.enabled":true,"rbac.cached.users.api.1.enabled":true,"rbac.crud.api.1.enabled":true,"rbac.management.api.1.enabled":true,"rbac.summaries.api.1.enabled":true,"rbac.visibility.api.1.enabled":true,"secrets.gcm.encryption.1.enabled":true,"token.generation.1.enabled":true,"token.validation.1.enabled":true},"legend":{"basic.auth.1.enabled":"Config controlled feature - HTTP basic auth is enabled","centralized.acls.api.1.enabled":"The centralized ACLs feature","cluster.registry.crud.api.1.enabled":"The v1 ClusterRegistry CRUD APIs","cluster.registry.nice.names.1.enabled":"Nice names from ClusterReg are used and returned in RBAC APIs","cluster.registry.nice.names.2.enabled":"Cluster name can contain only Unicode Letters, Numbers, Marker or .,&_+|[]/- special characters.","mds.authorize.api.1.enabled":"The v1 RBAC authorize endpoint, used by CP components","rbac.cached.users.api.1.enabled":"The v1 RBAC cached user info endpoint, used by C3","rbac.crud.api.1.enabled":"The v1 RBAC CRUD APIs","rbac.management.api.1.enabled":"The v1 RBAC my Rolebindings and Manage Rolebindings APIs, used by C3","rbac.summaries.api.1.enabled":"The v1 RBAC summaries APIs, mainly used by CLI","rbac.visibility.api.1.enabled":"The v1 RBAC visibility endpoint, used by C3","secrets.gcm.encryption.1.enabled":"AES GCM encryption and decryption support is dependent on the MDS version","token.generation.1.enabled":"Config controlled feature - MDS can generate tokens and has token generation endpoints enabled","token.validation.1.enabled":"Config controlled feature - MDS can accept/validate tokens"}}

[INFO] 2023-11-08 14:55:34,139 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:34,139 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[INFO] 2023-11-08 14:55:34,139 [qtp1527668063-199] io.confluent.rest-utils.requests write - 10.40.2.11 - - [08/Nov/2023:14:55:34 +0000] "GET /security/1.0/features HTTP/1.1" 200 579 "-" "Jetty/9.4.44.v20210927" 2
[DEBUG] 2023-11-08 14:55:34,140 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:34,141 [qtp1527668063-194] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@668d2267
[INFO] 2023-11-08 14:55:34,141 [qtp1527668063-194] io.confluent.mds.request.logger log - 130 * Server has received a request on thread qtp1527668063-194null130 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull130 > User Principal: testadmin
130 > Accept-Encoding: gzipnull130 > Connection: closenull130 > Content-Length: 115null130 > Content-Type: application/jsonnull130 > Host: kafka.confluent.svc.cluster.local:8090null130 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:34,143 [qtp1527668063-194] io.confluent.mds.request.logger log - 130 * Server responded with a response on thread qtp1527668063-194null130 < 200null130 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:34,143 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:34 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:34,143 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:34 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:34,143 [qtp1527668063-194] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:34 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[DEBUG] 2023-11-08 14:55:34,149 [qtp1527668063-192] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:34,149 [qtp1527668063-192] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@445beabe
[INFO] 2023-11-08 14:55:34,150 [qtp1527668063-192] io.confluent.mds.request.logger log - 131 * Server has received a request on thread qtp1527668063-192null131 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull131 > User Principal: testadmin
131 > Accept-Encoding: gzipnull131 > Connection: closenull131 > Content-Length: 115null131 > Content-Type: application/jsonnull131 > Host: kafka.confluent.svc.cluster.local:8090null131 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[DEBUG] 2023-11-08 14:55:34,151 [qtp1527668063-187] io.confluent.common.security.jetty.JwtLoginService login - Processing new Jwt login request.
[DEBUG] 2023-11-08 14:55:34,151 [qtp1527668063-187] io.confluent.common.security.jetty.JwtLoginService login - Processing login for JwtClaims : io.confluent.kafka.common.multitenant.oauth.OAuthBearerJwsToken@4246391f
[INFO] 2023-11-08 14:55:34,151 [qtp1527668063-187] io.confluent.mds.request.logger log - 132 * Server has received a request on thread qtp1527668063-187null132 > POST https://kafka.confluent.svc.cluster.local:8090/security/1.0/lookup/principals/User:testadmin/visibilitynull132 > User Principal: testadmin
132 > Accept-Encoding: gzipnull132 > Connection: closenull132 > Content-Length: 115null132 > Content-Type: application/jsonnull132 > Host: kafka.confluent.svc.cluster.local:8090null132 > User-Agent: Jetty/9.4.44.v20210927null[{"kafka-cluster":"9PWH12e6ROOpezLRDGV6Ag","connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:34,152 [qtp1527668063-192] io.confluent.mds.request.logger log - 131 * Server responded with a response on thread qtp1527668063-192null131 < 200null131 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:34,152 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:34 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:34,152 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:34 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:34,152 [qtp1527668063-192] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:34 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:34,153 [qtp1527668063-187] io.confluent.mds.request.logger log - 132 * Server responded with a response on thread qtp1527668063-187null132 < 200null132 < Content-Type: application/jsonnull[{"kafka-cluster":{"id":"9PWH12e6ROOpezLRDGV6Ag","visible":true,"clusterName":null},"connect-clusters":[],"schema-registry-clusters":[],"ksql-clusters":[]}]

[INFO] 2023-11-08 14:55:34,153 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:34 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:34,153 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:34 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[INFO] 2023-11-08 14:55:34,153 [qtp1527668063-187] io.confluent.rest-utils.requests write - 10.40.2.11 - testadmin [08/Nov/2023:14:55:34 +0000] "POST /security/1.0/lookup/principals/User:testadmin/visibility HTTP/1.1" 200 139 "-" "Jetty/9.4.44.v20210927" 3
[DEBUG] 2023-11-08 14:55:36,390 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:36,390 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:39,390 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:39,390 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:42,391 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:42,391 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:45,392 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:45,392 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:48,392 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:48,393 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[INFO] 2023-11-08 14:55:49,067 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logAll - AdminClientConfig values: 
	bootstrap.servers = [kafka-0.kafka.confluent.svc.cluster.local:9072]
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-license-admin
	connections.max.idle.ms = 300000
	default.api.timeout.ms = 60000
	host.resolver.class = class org.apache.kafka.clients.DefaultHostResolver
	metadata.max.age.ms = 300000
	metric.reporters = [io.confluent.metrics.reporter.ConfluentMetricsReporter]
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	receive.buffer.bytes = 65536
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = [hidden]
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = /mnt/sslcerts/keystore.jks
	ssl.keystore.password = [hidden]
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS

[INFO] 2023-11-08 14:55:49,068 [LicenseBackgroundFetcher RUNNING] io.confluent.metrics.reporter.ConfluentMetricsReporterConfig logAll - ConfluentMetricsReporterConfig values: 
	confluent.metrics.reporter.bootstrap.servers = kafka.confluent.svc.cluster.local:9071
	confluent.metrics.reporter.include = .*MaxLag.*|kafka.log:type=Log,name=Size.*|.*name=(ActiveControllerCount|BytesInPerSec|BytesOutPerSec|CaughtUpReplicasCount|FailedFetchRequestsPerSec|FailedProduceRequestsPerSec|InSyncReplicasCount|LeaderCount|LeaderElectionRateAndTimeMs|LocalTimeMs|LogEndOffset|LogStartOffset|NetworkProcessorAvgIdlePercent|NumLogSegments|NumPartitionsInError|OfflinePartitionsCount|ObserverReplicasCount|PartitionCount|RemoteTimeMs|ReplicasCount|RequestHandlerAvgIdlePercent|RequestQueueSize|RequestQueueTimeMs|RequestsPerSec|ResponseQueueSize|ResponseQueueTimeMs|ResponseSendTimeMs|Size|TierSize|TotalFetchRequestsPerSec|TotalLag|TotalProduceRequestsPerSec|TotalSize|TotalTimeMs|UncleanLeaderElectionsPerSec|UnderReplicated|UnderReplicatedPartitions|UnderMinIsrPartitionCount|ZooKeeperDisconnectsPerSec|ZooKeeperExpiresPerSec|TieredPartitionsUndergoingUncleanLeaderRecoveryCount|NonTieredPartitionsUndergoingUncleanLeaderRecoveryCount|TierTopicPartitionsUndergoingUncleanLeaderRecoveryCount|ContiguousUnhealthySamples|ContiguousSamplesEngineThreadGroupsStuck|ContiguousSamplesStorageThreadGroupsStuck|ContiguousSamplesNoStorageThreadMakingProgress).*|.*(BytesFetchedRate).*
	confluent.metrics.reporter.publish.ms = 30000
	confluent.metrics.reporter.topic = _confluent-metrics
	confluent.metrics.reporter.topic.create = true
	confluent.metrics.reporter.topic.max.message.bytes = 10485760
	confluent.metrics.reporter.topic.partitions = 12
	confluent.metrics.reporter.topic.replicas = 3
	confluent.metrics.reporter.topic.retention.bytes = -1
	confluent.metrics.reporter.topic.retention.ms = 259200000
	confluent.metrics.reporter.topic.roll.ms = 14400000
	confluent.metrics.reporter.volume.metrics.refresh.ms = 15000
	confluent.metrics.reporter.whitelist = null

[INFO] 2023-11-08 14:55:49,068 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logAll - ProducerConfig values: 
	acks = -1
	batch.size = 16384
	bootstrap.servers = [kafka.confluent.svc.cluster.local:9071]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = confluent-metrics-reporter
	compression.type = lz4
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 500
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 1
	max.request.size = 10485760
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 500
	sasl.client.callback.handler.class = null
	sasl.jaas.config = [hidden]
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.connect.timeout.ms = null
	sasl.login.read.timeout.ms = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.login.retry.backoff.max.ms = 10000
	sasl.login.retry.backoff.ms = 100
	sasl.mechanism = PLAIN
	sasl.oauthbearer.clock.skew.seconds = 30
	sasl.oauthbearer.expected.audience = null
	sasl.oauthbearer.expected.issuer = null
	sasl.oauthbearer.jwks.endpoint.refresh.ms = 3600000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.max.ms = 10000
	sasl.oauthbearer.jwks.endpoint.retry.backoff.ms = 100
	sasl.oauthbearer.jwks.endpoint.url = null
	sasl.oauthbearer.scope.claim.name = scope
	sasl.oauthbearer.sub.claim.name = sub
	sasl.oauthbearer.token.endpoint.url = null
	security.protocol = SASL_SSL
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = /mnt/sslcerts/truststore.jks
	ssl.truststore.password = [hidden]
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

[WARN] 2023-11-08 14:55:49,080 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,080 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.producer.ProducerConfig logUnused - The configuration 'topic.replicas' was supplied but isn't a known config.
[INFO] 2023-11-08 14:55:49,081 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:55:49,081 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:55:49,081 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455349080
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.name.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.version' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.enable.server.urls.refresh' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.type' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.commit.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'admin.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.security.protocol' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.member.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.bootstrap.server.urls' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'super.users' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'config.providers.file.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,084 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.authentication' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context.resource.cluster.id' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.endpoint.identification.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.provider.url' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.rest.servlet.initializor.classes' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.license.topic.replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.principal' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.reload' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.publish.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.group.object.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.client.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.basic.auth.user.info' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'replication.factor' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'metrics.context._namespace' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.bootstrap.servers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.advertised.listeners' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.topic.replicas' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.authentication.method' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.memberof.attribute.pattern' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.java.naming.security.credentials' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.search.base' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.auth.enable' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.max.lifetime.ms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.kafka.rest.resource.extension.class' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.mechanism' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,085 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.key.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'sasl.enabled.mechanisms' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.token.signature.algorithm' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.keystore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.http.auth.credentials.provider' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'kafka.rest.confluent.metadata.ssl.truststore.location' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.authorizer.access.rule.providers' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.ssl.truststore.password' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metrics.reporter.sasl.jaas.config' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'confluent.metadata.server.public.key.path' was supplied but isn't a known config.
[WARN] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.clients.admin.AdminClientConfig logUnused - The configuration 'ldap.user.name.attribute' was supplied but isn't a known config.
[INFO] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka version: 7.1.0-ce
[INFO] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka commitId: 818e9ed85c9442c9
[INFO] 2023-11-08 14:55:49,086 [LicenseBackgroundFetcher RUNNING] org.apache.kafka.common.utils.AppInfoParser <init> - Kafka startTimeMs: 1699455349086
[INFO] 2023-11-08 14:55:49,096 [kafka-producer-network-thread | confluent-metrics-reporter] org.apache.kafka.clients.Metadata update - [Producer clientId=confluent-metrics-reporter] Cluster ID: 9PWH12e6ROOpezLRDGV6Ag
[INFO] 2023-11-08 14:55:49,107 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.admin.client for confluent-license-admin unregistered
[INFO] 2023-11-08 14:55:49,107 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:55:49,107 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter io.confluent.metrics.reporter.ConfluentMetricsReporter
[INFO] 2023-11-08 14:55:49,107 [kafka-admin-client-thread | confluent-license-admin] io.confluent.metrics.reporter.ConfluentMetricsReporter close - Stopping Confluent metrics reporter
[INFO] 2023-11-08 14:55:49,108 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Closing the Kafka producer with timeoutMillis = 0 ms.
[INFO] 2023-11-08 14:55:49,108 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.clients.producer.KafkaProducer close - [Producer clientId=confluent-metrics-reporter] Proceeding to force close the producer since pending requests could not be completed within timeout 0 ms.
[INFO] 2023-11-08 14:55:49,109 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics scheduler closed
[INFO] 2023-11-08 14:55:49,109 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:55:49,109 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:55:49,109 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.utils.AppInfoParser unregisterAppInfo - App info kafka.producer for confluent-metrics-reporter unregistered
[INFO] 2023-11-08 14:55:49,109 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Closing reporter org.apache.kafka.common.metrics.JmxReporter
[INFO] 2023-11-08 14:55:49,109 [kafka-admin-client-thread | confluent-license-admin] org.apache.kafka.common.metrics.Metrics close - Metrics reporters closed
[INFO] 2023-11-08 14:55:49,109 [LicenseBackgroundFetcher RUNNING] io.confluent.license.LicenseManager checkLicense - Trial license for Confluent Enterprise expires in 29 days on 2023-12-08.
[DEBUG] 2023-11-08 14:55:51,393 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:51,393 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[TRACE] 2023-11-08 14:55:54,154 [qtp1527668063-254] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Searching for user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): [Thread 254]
[TRACE] 2023-11-08 14:55:54,156 [qtp1527668063-254] io.confluent.security.auth.provider.ldap.LdapAuthenticateCallbackHandler ldapSearch - Found user c3 with base dc=test,dc=com filter (&(objectClass=organizationalRole)(objectClass=organizationalRole)(cn={0})): cn=c3,dc=test,dc=com [Thread 254]
[DEBUG] 2023-11-08 14:55:54,158 [qtp1527668063-254] io.confluent.rbacapi.login.MdsLoginService login - Login succeeded for c3
[INFO] 2023-11-08 14:55:54,159 [qtp1527668063-254] io.confluent.mds.request.logger log - 133 * Server has received a request on thread qtp1527668063-254null133 > GET https://kafka-0.kafka.confluent.svc.cluster.local:8090/security/1.0/authenticatenull133 > User Principal: c3
133 > Accept: text/html, image/gif, image/jpeg, *; q=.2, */*; q=.2null133 > Cache-Control: no-cachenull133 > Connection: keep-alivenull133 > Content-Type: application/jsonnull133 > Host: kafka-0.kafka.confluent.svc.cluster.local:8090null133 > Pragma: no-cachenull133 > User-Agent: Java/11.0.14.1null
[INFO] 2023-11-08 14:55:54,162 [qtp1527668063-254] io.confluent.mds.request.logger log - 133 * Server responded with a response on thread qtp1527668063-254null133 < 200null133 < Content-Type: application/jsonnull
[INFO] 2023-11-08 14:55:54,163 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:55:54 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:55:54,163 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:55:54 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[INFO] 2023-11-08 14:55:54,163 [qtp1527668063-254] io.confluent.rest-utils.requests write - 10.40.2.9 - c3 [08/Nov/2023:14:55:54 +0000] "GET /security/1.0/authenticate HTTP/1.1" 200 631 "-" "Java/11.0.14.1" 9
[DEBUG] 2023-11-08 14:55:54,392 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:54,393 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
[DEBUG] 2023-11-08 14:55:57,392 [kafka-coordinator-heartbeat-thread | _confluent-metadata-coordinator-group] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator sendHeartbeatRequest - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Sending Heartbeat request with generation 1 and member id _confluent-metadata-coordinator-0-dc9ebf5f-0144-4977-aec8-bf8d775ae84d to coordinator kafka-0.kafka.confluent.svc.cluster.local:9072 (id: 2147483647 rack: null)
[DEBUG] 2023-11-08 14:55:57,393 [metadata-service-coordinator] io.confluent.security.store.kafka.coordinator.MetadataServiceCoordinator handle - [io.confluent.security.store.kafka.coordinator.MetadataNodeManager clientId=_confluent-metadata-coordinator-0, groupId=_confluent-metadata-coordinator-group]Received successful Heartbeat response
